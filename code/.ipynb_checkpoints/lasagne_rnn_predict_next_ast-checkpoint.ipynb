{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Knowledge Tracing\n",
    "Authors: Lisa Wang, Angela Sy\n",
    "\n",
    "Task: Predict what the student is going to code next.\n",
    "\n",
    "Input: For each of the N students, we have a time series of Abstract Syntax Trees (ASTs), which represent the student's code at that time step.\n",
    "- input shape (num_students, num_timesteps, num_asts)\n",
    "    - num_timesteps is the max sequence length of asts that we are taking into account.\n",
    "    - num_asts is the total number of asts for that problem.\n",
    "\n",
    "Output: At each timestep, we are predicting the next AST. \n",
    "- Output shape (num_students, num_timesteps). The values will be in the range (0, num_asts).\n",
    "\n",
    "The truth matrix contains the desired output for a given input, and is used to compute the loss as well as train/val/test accuracies.\n",
    "- Truth shape (num_students, num_timesteps)\n",
    "\n",
    "\n",
    "There are few ways to calculate the loss and perform the training:\n",
    "    1. We concatenate all trajectories (compare to comparing fragments of text to a bigger corpus) and use a sliding window, very similar to predicting the next character.\n",
    "    \n",
    "    2. We could train the model on each trajectory individually. We use a sliding window, e.g. of 3 ASTs.\n",
    "    \n",
    "Current Issues:\n",
    "    1. AST IDs are not consistent across different HOCs. Hence, we can only train and run this model on each HOC individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Python libraries\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as Tensor\n",
    "import lasagne\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# allows plots to show inline in ipython notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import our own modules\n",
    "import utils\n",
    "import model_predict_ast as model\n",
    "import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../processed_data/map_ast_row_7.pickle\n"
     ]
    }
   ],
   "source": [
    "# Each trajectory matrix corresponds to one hoc exercise and is\n",
    "# its own data set. Mixing data sets currently does not make much\n",
    "# sense since the AST IDs don't persist betweeen different hoc's.\n",
    "TRAJ_MAP = {\n",
    "    'hoc1': '../processed_data/traj_matrix_1.npy',\n",
    "    'hoc2': '../processed_data/traj_matrix_2.npy',\n",
    "    'hoc3': '../processed_data/traj_matrix_3.npy',\n",
    "    'hoc4': '../processed_data/traj_matrix_4.npy',\n",
    "    'hoc5': '../processed_data/traj_matrix_5.npy',\n",
    "    'hoc6': '../processed_data/traj_matrix_6.npy',\n",
    "    'hoc7': '../processed_data/traj_matrix_7.npy',\n",
    "    'hoc8': '../processed_data/traj_matrix_8.npy',\n",
    "    'hoc9': '../processed_data/traj_matrix_9.npy' \n",
    "}\n",
    "\n",
    "TRAJ_MAP_PREFIX = '../processed_data/traj_matrix_'\n",
    "TRAJ_MAP_SUFFIX = '.npy'\n",
    "\n",
    "HOC_NUM = str(7)\n",
    "DATA_SET = 'hoc' + HOC_NUM\n",
    "# if DATA_SZ = -1, use entire data set\n",
    "# For DATA_SZ, powers of 2 work best for performance.\n",
    "DATA_SZ = -1\n",
    "\n",
    "# DATA_SZ = -1\n",
    "\n",
    "AST_MAP_FILE = '../processed_data/map_ast_row_' + HOC_NUM + '.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4195, 7, 432)\n"
     ]
    }
   ],
   "source": [
    "# trajectories matrix for a single hoc exercise\n",
    "# shape (num_traj, max_traj_len, num_asts)\n",
    "# Note that ast_index = 0 corresponds to the <END> token,\n",
    "# marking that the student has already finished.\n",
    "# The <END> token does not correspond to an AST.\n",
    "traj_mat = np.load(TRAJ_MAP[DATA_SET])\n",
    "print traj_mat.shape\n",
    "# print traj_mat[:10, :, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4195, 7, 432)\n"
     ]
    }
   ],
   "source": [
    "# if DATA_SZ specified, reduce matrix. \n",
    "# Useful to create smaller data sets for testing purposes.\n",
    "if DATA_SZ != -1:\n",
    "    traj_mat = traj_mat[:DATA_SZ]\n",
    "print traj_mat.shape\n",
    "# print traj_mat_sm[:, :, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shuffle the first dimension of the matrix\n",
    "np.random.shuffle(traj_mat)\n",
    "# print traj_mat_sm[:, :, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_traj, max_traj_len, num_asts = traj_mat.shape\n",
    "# Split data into train, val, test\n",
    "# TODO: Replace with kfold validation in the future\n",
    "# perhaps we can use sklearn kfold?\n",
    "\n",
    "train_mat = traj_mat[0:7*num_traj/8,:]\n",
    "val_mat =  traj_mat[7*num_traj/8: 15*num_traj/16 ,:]\n",
    "test_mat = traj_mat[15*num_traj/16:num_traj,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing network inputs and targets...\n",
      "(3670, 6, 432)\n",
      "(3670, 6)\n",
      "(262, 6, 432)\n",
      "(263, 6, 432)\n",
      "[[   1.    7.    2.    7.    3.    0.]\n",
      " [   1.   37.   68.    3.    0.    0.]\n",
      " [   7.   51.    3.    0.    0.    0.]\n",
      " [  61.    0.    0.    0.    0.    0.]\n",
      " [  59.    2.  100.   27.    3.    0.]\n",
      " [ 274.    0.    0.    0.    0.    0.]\n",
      " [   1.   22.    7.    1.    0.    0.]\n",
      " [   1.   35.    2.    3.    0.    0.]\n",
      " [   1.    5.    4.    0.    0.    0.]\n",
      " [  45.   30.    0.    0.    0.    0.]]\n",
      "6\n",
      "Inputs and targets done!\n"
     ]
    }
   ],
   "source": [
    "print('Preparing network inputs and targets...')\n",
    "# X_train, y_train = utils.prepare_traj_data_for_rnn(train_data)\n",
    "# X_val, y_val = utils.prepare_traj_data_for_rnn(val_data)\n",
    "# X_test, y_test = utils.prepare_traj_data_for_rnn(test_data)\n",
    "\n",
    "train_data = utils.prepare_traj_data_for_rnn(train_mat)\n",
    "val_data = utils.prepare_traj_data_for_rnn(val_mat)\n",
    "test_data = utils.prepare_traj_data_for_rnn(test_mat)\n",
    "\n",
    "\n",
    "X_train, y_train = train_data\n",
    "X_val, y_val = val_data\n",
    "X_test, y_test = test_data\n",
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_val.shape\n",
    "print X_test.shape\n",
    "num_train, num_timesteps, num_asts = X_train.shape\n",
    "\n",
    "print y_train[:10]\n",
    "\n",
    "print num_timesteps\n",
    "print (\"Inputs and targets done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ast_id_to_row_map = pickle.load(open( AST_MAP_FILE, \"rb\" ))\n",
    "# print ast_id_to_row_map[-1]\n",
    "\n",
    "row_to_ast_id_map = {v: k for k, v in ast_id_to_row_map.items()}\n",
    "# print row_to_ast_id_map[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 200 # size of hidden layer of neurons\n",
    "learning_rate = 1e-2\n",
    "lr_decay = 1.0\n",
    "reg_strength = 1e-2\n",
    "grad_clip = 10\n",
    "batchsize = 32\n",
    "num_epochs = 2\n",
    "dropout_p = 0.2\n",
    "num_lstm_layers = 2\n",
    "theano.config.optimizer='fast_compile'\n",
    "theano.config.exception_verbosity='high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Compiling done!\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "train_loss_acc, compute_loss_acc, probs = model.create_model(num_timesteps, num_asts, hidden_size, learning_rate, grad_clip, dropout_p, num_lstm_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "  Epoch 0 \tbatch 1 \tloss 6.06933430048 \ttrain acc 0.52 \tval acc 69.60 \n",
      "  Epoch 0 \tbatch 2 \tloss 5.99079151975 \ttrain acc 71.88 \tval acc 52.67 \n",
      "  Epoch 0 \tbatch 3 \tloss 5.50406948927 \ttrain acc 49.48 \tval acc 47.40 \n",
      "  Epoch 0 \tbatch 4 \tloss 3.96635141318 \ttrain acc 49.48 \tval acc 47.40 \n",
      "  Epoch 0 \tbatch 5 \tloss 2.95648270026 \ttrain acc 45.83 \tval acc 47.40 \n",
      "  Epoch 0 \tbatch 6 \tloss 2.86701837836 \ttrain acc 47.92 \tval acc 47.40 \n",
      "  Epoch 0 \tbatch 7 \tloss 3.08346889098 \ttrain acc 47.40 \tval acc 47.40 \n",
      "  Epoch 0 \tbatch 8 \tloss 2.74874143149 \ttrain acc 47.40 \tval acc 48.31 \n",
      "  Epoch 0 \tbatch 9 \tloss 2.68879601785 \ttrain acc 48.96 \tval acc 51.69 \n",
      "  Epoch 0 \tbatch 10 \tloss 2.63875765232 \ttrain acc 56.77 \tval acc 51.76 \n",
      "  Epoch 0 \tbatch 11 \tloss 2.85956810964 \ttrain acc 47.40 \tval acc 51.82 \n",
      "  Epoch 0 \tbatch 12 \tloss 2.43769793988 \ttrain acc 54.17 \tval acc 52.08 \n",
      "  Epoch 0 \tbatch 13 \tloss 2.50181212347 \ttrain acc 52.08 \tval acc 52.21 \n",
      "  Epoch 0 \tbatch 14 \tloss 2.55067866671 \ttrain acc 49.48 \tval acc 52.02 \n",
      "  Epoch 0 \tbatch 15 \tloss 2.72162448159 \ttrain acc 53.12 \tval acc 52.08 \n",
      "  Epoch 0 \tbatch 16 \tloss 2.5284203396 \ttrain acc 51.04 \tval acc 52.02 \n",
      "  Epoch 0 \tbatch 17 \tloss 2.36876764523 \ttrain acc 56.77 \tval acc 52.15 \n",
      "  Epoch 0 \tbatch 18 \tloss 2.36954743791 \ttrain acc 50.00 \tval acc 51.63 \n",
      "  Epoch 0 \tbatch 19 \tloss 2.24355045214 \ttrain acc 53.12 \tval acc 53.39 \n",
      "  Epoch 0 \tbatch 20 \tloss 2.12175187882 \ttrain acc 52.60 \tval acc 53.45 \n",
      "  Epoch 0 \tbatch 21 \tloss 2.23273409447 \ttrain acc 51.04 \tval acc 53.97 \n",
      "  Epoch 0 \tbatch 22 \tloss 2.0961506002 \ttrain acc 61.98 \tval acc 53.84 \n",
      "  Epoch 0 \tbatch 23 \tloss 2.01146041354 \ttrain acc 57.29 \tval acc 55.86 \n",
      "  Epoch 0 \tbatch 24 \tloss 1.89988781219 \ttrain acc 62.50 \tval acc 52.21 \n",
      "  Epoch 0 \tbatch 25 \tloss 2.15609275585 \ttrain acc 58.33 \tval acc 54.62 \n",
      "  Epoch 0 \tbatch 26 \tloss 2.02813818349 \ttrain acc 56.25 \tval acc 54.49 \n",
      "  Epoch 0 \tbatch 27 \tloss 2.15047674537 \ttrain acc 49.48 \tval acc 55.73 \n",
      "  Epoch 0 \tbatch 28 \tloss 1.997138084 \ttrain acc 55.73 \tval acc 55.79 \n",
      "  Epoch 0 \tbatch 29 \tloss 1.89557795849 \ttrain acc 56.77 \tval acc 55.47 \n",
      "  Epoch 0 \tbatch 30 \tloss 2.23914708525 \ttrain acc 53.65 \tval acc 55.40 \n",
      "  Epoch 0 \tbatch 31 \tloss 2.11641139713 \ttrain acc 56.25 \tval acc 56.51 \n",
      "  Epoch 0 \tbatch 32 \tloss 2.19408762501 \ttrain acc 57.81 \tval acc 59.24 \n",
      "  Epoch 0 \tbatch 33 \tloss 2.17246129032 \ttrain acc 60.94 \tval acc 54.23 \n",
      "  Epoch 0 \tbatch 34 \tloss 2.09594385195 \ttrain acc 53.65 \tval acc 59.31 \n",
      "  Epoch 0 \tbatch 35 \tloss 1.6333823269 \ttrain acc 64.58 \tval acc 55.53 \n",
      "  Epoch 0 \tbatch 36 \tloss 1.90247933451 \ttrain acc 59.90 \tval acc 53.97 \n",
      "  Epoch 0 \tbatch 37 \tloss 2.17806946449 \ttrain acc 56.77 \tval acc 54.88 \n",
      "  Epoch 0 \tbatch 38 \tloss 2.07340324526 \ttrain acc 53.12 \tval acc 59.24 \n",
      "  Epoch 0 \tbatch 39 \tloss 1.74786098024 \ttrain acc 62.50 \tval acc 60.55 \n",
      "  Epoch 0 \tbatch 40 \tloss 2.00158552718 \ttrain acc 60.42 \tval acc 58.46 \n",
      "  Epoch 0 \tbatch 41 \tloss 1.97542143066 \ttrain acc 59.38 \tval acc 60.55 \n",
      "  Epoch 0 \tbatch 42 \tloss 1.82316407834 \ttrain acc 61.46 \tval acc 59.64 \n",
      "  Epoch 0 \tbatch 43 \tloss 1.67855481947 \ttrain acc 61.98 \tval acc 58.59 \n",
      "  Epoch 0 \tbatch 44 \tloss 2.10379472983 \ttrain acc 61.46 \tval acc 58.72 \n",
      "  Epoch 0 \tbatch 45 \tloss 1.8545440909 \ttrain acc 56.25 \tval acc 62.04 \n",
      "  Epoch 0 \tbatch 46 \tloss 1.77590034553 \ttrain acc 63.02 \tval acc 62.43 \n",
      "  Epoch 0 \tbatch 47 \tloss 1.81111345723 \ttrain acc 62.50 \tval acc 62.30 \n",
      "  Epoch 0 \tbatch 48 \tloss 1.74748334915 \ttrain acc 66.15 \tval acc 62.37 \n",
      "  Epoch 0 \tbatch 49 \tloss 1.60468344828 \ttrain acc 65.62 \tval acc 62.43 \n",
      "  Epoch 0 \tbatch 50 \tloss 1.79083195354 \ttrain acc 60.94 \tval acc 61.13 \n",
      "  Epoch 0 \tbatch 51 \tloss 1.43382285944 \ttrain acc 65.62 \tval acc 61.59 \n",
      "  Epoch 0 \tbatch 52 \tloss 1.68179578748 \ttrain acc 62.50 \tval acc 62.04 \n",
      "  Epoch 0 \tbatch 53 \tloss 1.31981389836 \ttrain acc 68.23 \tval acc 63.35 \n",
      "  Epoch 0 \tbatch 54 \tloss 1.6096548156 \ttrain acc 64.06 \tval acc 65.36 \n",
      "  Epoch 0 \tbatch 55 \tloss 1.42868952886 \ttrain acc 67.71 \tval acc 66.15 \n",
      "  Epoch 0 \tbatch 56 \tloss 1.57089327744 \ttrain acc 62.50 \tval acc 66.28 \n",
      "  Epoch 0 \tbatch 57 \tloss 1.42598411772 \ttrain acc 64.58 \tval acc 66.80 \n",
      "  Epoch 0 \tbatch 58 \tloss 1.55530379617 \ttrain acc 65.62 \tval acc 66.93 \n",
      "  Epoch 0 \tbatch 59 \tloss 1.50572381426 \ttrain acc 67.19 \tval acc 68.10 \n",
      "  Epoch 0 \tbatch 60 \tloss 1.31391863591 \ttrain acc 72.40 \tval acc 68.68 \n",
      "  Epoch 0 \tbatch 61 \tloss 1.40638808801 \ttrain acc 68.75 \tval acc 69.21 \n",
      "  Epoch 0 \tbatch 62 \tloss 1.18643984476 \ttrain acc 73.96 \tval acc 70.05 \n",
      "  Epoch 0 \tbatch 63 \tloss 1.25642782046 \ttrain acc 69.79 \tval acc 71.03 \n",
      "  Epoch 0 \tbatch 64 \tloss 1.47093508138 \ttrain acc 66.15 \tval acc 70.77 \n",
      "  Epoch 0 \tbatch 65 \tloss 1.25833750331 \ttrain acc 71.88 \tval acc 69.53 \n",
      "  Epoch 0 \tbatch 66 \tloss 1.54702515807 \ttrain acc 69.27 \tval acc 69.53 \n",
      "  Epoch 0 \tbatch 67 \tloss 1.26481881581 \ttrain acc 73.44 \tval acc 69.40 \n",
      "  Epoch 0 \tbatch 68 \tloss 1.3181311135 \ttrain acc 66.67 \tval acc 70.05 \n",
      "  Epoch 0 \tbatch 69 \tloss 1.35880434775 \ttrain acc 70.83 \tval acc 70.25 \n",
      "  Epoch 0 \tbatch 70 \tloss 1.30357282681 \ttrain acc 71.35 \tval acc 71.55 \n",
      "  Epoch 0 \tbatch 71 \tloss 1.2796725495 \ttrain acc 72.40 \tval acc 71.74 \n",
      "  Epoch 0 \tbatch 72 \tloss 1.1433417137 \ttrain acc 71.35 \tval acc 73.05 \n",
      "  Epoch 0 \tbatch 73 \tloss 1.59249830597 \ttrain acc 68.75 \tval acc 75.00 \n",
      "  Epoch 0 \tbatch 74 \tloss 1.05179169441 \ttrain acc 79.17 \tval acc 75.00 \n",
      "  Epoch 0 \tbatch 75 \tloss 1.14565515926 \ttrain acc 75.52 \tval acc 74.48 \n",
      "  Epoch 0 \tbatch 76 \tloss 0.946511574588 \ttrain acc 79.17 \tval acc 74.61 \n",
      "  Epoch 0 \tbatch 77 \tloss 0.941452385889 \ttrain acc 77.60 \tval acc 74.80 \n",
      "  Epoch 0 \tbatch 78 \tloss 1.2848127839 \ttrain acc 72.92 \tval acc 74.93 \n",
      "  Epoch 0 \tbatch 79 \tloss 1.20926414035 \ttrain acc 73.44 \tval acc 76.37 \n",
      "  Epoch 0 \tbatch 80 \tloss 1.25654147243 \ttrain acc 71.35 \tval acc 76.63 \n",
      "  Epoch 0 \tbatch 81 \tloss 0.987978143027 \ttrain acc 78.12 \tval acc 76.63 \n",
      "  Epoch 0 \tbatch 82 \tloss 1.00309010797 \ttrain acc 76.04 \tval acc 77.21 \n",
      "  Epoch 0 \tbatch 83 \tloss 1.17033885983 \ttrain acc 75.00 \tval acc 78.97 \n",
      "  Epoch 0 \tbatch 84 \tloss 1.00668744271 \ttrain acc 77.08 \tval acc 79.30 \n",
      "  Epoch 0 \tbatch 85 \tloss 1.15877011729 \ttrain acc 78.12 \tval acc 80.40 \n",
      "  Epoch 0 \tbatch 86 \tloss 0.981603898228 \ttrain acc 81.25 \tval acc 79.95 \n",
      "  Epoch 0 \tbatch 87 \tloss 1.06450350156 \ttrain acc 80.21 \tval acc 79.62 \n",
      "  Epoch 0 \tbatch 88 \tloss 1.00344602009 \ttrain acc 77.60 \tval acc 81.45 \n",
      "  Epoch 0 \tbatch 89 \tloss 0.924395980559 \ttrain acc 82.29 \tval acc 82.36 \n",
      "  Epoch 0 \tbatch 90 \tloss 0.946232655186 \ttrain acc 83.33 \tval acc 81.64 \n",
      "  Epoch 0 \tbatch 91 \tloss 0.872720761045 \ttrain acc 84.38 \tval acc 80.27 \n",
      "  Epoch 0 \tbatch 92 \tloss 0.942691071492 \ttrain acc 82.29 \tval acc 80.27 \n",
      "  Epoch 0 \tbatch 93 \tloss 0.958395929717 \ttrain acc 81.77 \tval acc 80.47 \n",
      "  Epoch 0 \tbatch 94 \tloss 0.95136727762 \ttrain acc 82.29 \tval acc 81.58 \n",
      "  Epoch 0 \tbatch 95 \tloss 0.897668233091 \ttrain acc 80.73 \tval acc 82.29 \n",
      "  Epoch 0 \tbatch 96 \tloss 0.886963964068 \ttrain acc 80.73 \tval acc 83.14 \n",
      "  Epoch 0 \tbatch 97 \tloss 1.00153177341 \ttrain acc 80.21 \tval acc 82.81 \n",
      "  Epoch 0 \tbatch 98 \tloss 0.823564132957 \ttrain acc 82.29 \tval acc 83.72 \n",
      "  Epoch 0 \tbatch 99 \tloss 0.961754488515 \ttrain acc 80.73 \tval acc 83.85 \n",
      "  Epoch 0 \tbatch 100 \tloss 0.883002917968 \ttrain acc 84.38 \tval acc 84.57 \n",
      "  Epoch 0 \tbatch 101 \tloss 1.03341131126 \ttrain acc 77.60 \tval acc 84.90 \n",
      "  Epoch 0 \tbatch 102 \tloss 0.68964318655 \ttrain acc 86.98 \tval acc 85.22 \n",
      "  Epoch 0 \tbatch 103 \tloss 0.683671445498 \ttrain acc 86.98 \tval acc 85.42 \n",
      "  Epoch 0 \tbatch 104 \tloss 0.760281874337 \ttrain acc 85.42 \tval acc 85.94 \n",
      "  Epoch 0 \tbatch 105 \tloss 0.487151667581 \ttrain acc 91.15 \tval acc 86.00 \n",
      "  Epoch 0 \tbatch 106 \tloss 0.716399915715 \ttrain acc 86.98 \tval acc 86.13 \n",
      "  Epoch 0 \tbatch 107 \tloss 0.607426515519 \ttrain acc 88.54 \tval acc 86.26 \n",
      "  Epoch 0 \tbatch 108 \tloss 0.706305175563 \ttrain acc 85.94 \tval acc 86.78 \n",
      "  Epoch 0 \tbatch 109 \tloss 0.602099975692 \ttrain acc 90.10 \tval acc 86.91 \n",
      "  Epoch 0 \tbatch 110 \tloss 0.558480963453 \ttrain acc 86.98 \tval acc 87.30 \n",
      "  Epoch 0 \tbatch 111 \tloss 0.709303341156 \ttrain acc 86.46 \tval acc 87.24 \n",
      "  Epoch 0 \tbatch 112 \tloss 0.602928628095 \ttrain acc 88.02 \tval acc 87.57 \n",
      "  Epoch 0 \tbatch 113 \tloss 0.488837978593 \ttrain acc 91.15 \tval acc 87.43 \n",
      "  Epoch 0 \tbatch 114 \tloss 0.794241084517 \ttrain acc 84.38 \tval acc 87.17 \n",
      "Epoch 1 of 2 took 55.609s\n",
      "  training loss:\t\t1.676668\n",
      "  training accuracy:\t\t67.39 %\n",
      "  validation loss:\t\t0.654716\n",
      "  validation accuracy:\t\t87.70 %\n",
      "  Epoch 1 \tbatch 1 \tloss 0.426069602577 \ttrain acc 91.15 \tval acc 87.89 \n",
      "  Epoch 1 \tbatch 2 \tloss 0.424276891959 \ttrain acc 91.15 \tval acc 88.41 \n",
      "  Epoch 1 \tbatch 3 \tloss 0.539737882405 \ttrain acc 91.15 \tval acc 88.35 \n",
      "  Epoch 1 \tbatch 4 \tloss 0.843516113775 \ttrain acc 84.90 \tval acc 88.80 \n",
      "  Epoch 1 \tbatch 5 \tloss 0.726991384874 \ttrain acc 87.50 \tval acc 88.41 \n",
      "  Epoch 1 \tbatch 6 \tloss 0.783194361441 \ttrain acc 85.42 \tval acc 89.06 \n",
      "  Epoch 1 \tbatch 7 \tloss 0.535767066987 \ttrain acc 89.06 \tval acc 89.32 \n",
      "  Epoch 1 \tbatch 8 \tloss 0.455385136894 \ttrain acc 91.67 \tval acc 89.13 \n",
      "  Epoch 1 \tbatch 9 \tloss 0.447660175002 \ttrain acc 92.19 \tval acc 88.93 \n",
      "  Epoch 1 \tbatch 10 \tloss 0.613203261425 \ttrain acc 89.58 \tval acc 89.13 \n",
      "  Epoch 1 \tbatch 11 \tloss 0.580809123309 \ttrain acc 90.10 \tval acc 89.91 \n",
      "  Epoch 1 \tbatch 12 \tloss 0.463345969202 \ttrain acc 91.15 \tval acc 89.71 \n",
      "  Epoch 1 \tbatch 13 \tloss 0.413895916174 \ttrain acc 94.27 \tval acc 89.32 \n",
      "  Epoch 1 \tbatch 14 \tloss 0.440941774214 \ttrain acc 92.71 \tval acc 90.10 \n",
      "  Epoch 1 \tbatch 15 \tloss 0.705217955281 \ttrain acc 86.98 \tval acc 89.71 \n",
      "  Epoch 1 \tbatch 16 \tloss 0.547676384344 \ttrain acc 88.02 \tval acc 89.71 \n",
      "  Epoch 1 \tbatch 17 \tloss 0.592688481298 \ttrain acc 89.06 \tval acc 90.10 \n",
      "  Epoch 1 \tbatch 18 \tloss 0.494691339453 \ttrain acc 91.15 \tval acc 90.36 \n",
      "  Epoch 1 \tbatch 19 \tloss 0.481031829594 \ttrain acc 91.15 \tval acc 90.36 \n",
      "  Epoch 1 \tbatch 20 \tloss 0.365946211174 \ttrain acc 93.23 \tval acc 90.89 \n",
      "  Epoch 1 \tbatch 21 \tloss 0.551140249111 \ttrain acc 89.58 \tval acc 90.76 \n",
      "  Epoch 1 \tbatch 22 \tloss 0.538036265462 \ttrain acc 89.06 \tval acc 91.47 \n",
      "  Epoch 1 \tbatch 23 \tloss 0.474115724212 \ttrain acc 90.62 \tval acc 91.34 \n",
      "  Epoch 1 \tbatch 24 \tloss 0.442070345845 \ttrain acc 92.71 \tval acc 91.47 \n",
      "  Epoch 1 \tbatch 25 \tloss 0.505216254191 \ttrain acc 92.71 \tval acc 91.60 \n",
      "  Epoch 1 \tbatch 26 \tloss 0.442879818397 \ttrain acc 92.19 \tval acc 91.41 \n",
      "  Epoch 1 \tbatch 27 \tloss 0.396171734556 \ttrain acc 92.71 \tval acc 91.67 \n",
      "  Epoch 1 \tbatch 28 \tloss 0.387916876979 \ttrain acc 92.71 \tval acc 91.47 \n",
      "  Epoch 1 \tbatch 29 \tloss 0.458983749473 \ttrain acc 92.19 \tval acc 91.86 \n",
      "  Epoch 1 \tbatch 30 \tloss 0.412477489883 \ttrain acc 91.67 \tval acc 92.12 \n",
      "  Epoch 1 \tbatch 31 \tloss 0.464715497836 \ttrain acc 92.71 \tval acc 92.58 \n",
      "  Epoch 1 \tbatch 32 \tloss 0.546752219608 \ttrain acc 90.62 \tval acc 92.51 \n",
      "  Epoch 1 \tbatch 33 \tloss 0.434781222497 \ttrain acc 92.19 \tval acc 92.19 \n",
      "  Epoch 1 \tbatch 34 \tloss 0.443622359113 \ttrain acc 92.19 \tval acc 92.19 \n",
      "  Epoch 1 \tbatch 35 \tloss 0.319648606801 \ttrain acc 94.79 \tval acc 92.38 \n",
      "  Epoch 1 \tbatch 36 \tloss 0.419353593458 \ttrain acc 92.19 \tval acc 92.32 \n",
      "  Epoch 1 \tbatch 37 \tloss 0.501876856394 \ttrain acc 91.67 \tval acc 91.86 \n",
      "  Epoch 1 \tbatch 38 \tloss 0.332556122114 \ttrain acc 94.27 \tval acc 92.71 \n",
      "  Epoch 1 \tbatch 39 \tloss 0.35512932064 \ttrain acc 93.23 \tval acc 92.32 \n",
      "  Epoch 1 \tbatch 40 \tloss 0.429697096533 \ttrain acc 93.23 \tval acc 92.71 \n",
      "  Epoch 1 \tbatch 41 \tloss 0.437054039377 \ttrain acc 93.75 \tval acc 92.97 \n",
      "  Epoch 1 \tbatch 42 \tloss 0.300373334501 \ttrain acc 95.31 \tval acc 93.03 \n",
      "  Epoch 1 \tbatch 43 \tloss 0.243501449532 \ttrain acc 96.35 \tval acc 93.29 \n",
      "  Epoch 1 \tbatch 44 \tloss 0.423101532284 \ttrain acc 92.19 \tval acc 92.97 \n",
      "  Epoch 1 \tbatch 45 \tloss 0.38116541756 \ttrain acc 92.19 \tval acc 93.16 \n",
      "  Epoch 1 \tbatch 46 \tloss 0.383521104388 \ttrain acc 93.75 \tval acc 93.29 \n",
      "  Epoch 1 \tbatch 47 \tloss 0.429161311458 \ttrain acc 92.71 \tval acc 93.16 \n",
      "  Epoch 1 \tbatch 48 \tloss 0.507888162448 \ttrain acc 89.58 \tval acc 93.23 \n",
      "  Epoch 1 \tbatch 49 \tloss 0.345518381369 \ttrain acc 91.67 \tval acc 93.55 \n",
      "  Epoch 1 \tbatch 50 \tloss 0.439552521218 \ttrain acc 91.67 \tval acc 93.75 \n",
      "  Epoch 1 \tbatch 51 \tloss 0.129028701778 \ttrain acc 97.92 \tval acc 93.55 \n",
      "  Epoch 1 \tbatch 52 \tloss 0.396094362711 \ttrain acc 92.19 \tval acc 94.08 \n",
      "  Epoch 1 \tbatch 53 \tloss 0.244973180474 \ttrain acc 95.31 \tval acc 93.95 \n",
      "  Epoch 1 \tbatch 54 \tloss 0.282161190468 \ttrain acc 94.27 \tval acc 94.21 \n",
      "  Epoch 1 \tbatch 55 \tloss 0.31633241455 \ttrain acc 94.79 \tval acc 94.21 \n",
      "  Epoch 1 \tbatch 56 \tloss 0.363047839619 \ttrain acc 92.71 \tval acc 94.34 \n",
      "  Epoch 1 \tbatch 57 \tloss 0.273871599513 \ttrain acc 96.35 \tval acc 94.21 \n",
      "  Epoch 1 \tbatch 58 \tloss 0.301723170044 \ttrain acc 93.75 \tval acc 94.27 \n",
      "  Epoch 1 \tbatch 59 \tloss 0.328171813458 \ttrain acc 94.79 \tval acc 94.79 \n",
      "  Epoch 1 \tbatch 60 \tloss 0.294946088312 \ttrain acc 95.83 \tval acc 94.86 \n",
      "  Epoch 1 \tbatch 61 \tloss 0.246051933231 \ttrain acc 95.83 \tval acc 94.73 \n",
      "  Epoch 1 \tbatch 62 \tloss 0.233682652595 \ttrain acc 95.31 \tval acc 94.73 \n",
      "  Epoch 1 \tbatch 63 \tloss 0.223614246724 \ttrain acc 95.31 \tval acc 94.92 \n",
      "  Epoch 1 \tbatch 64 \tloss 0.244371155691 \ttrain acc 95.83 \tval acc 95.31 \n",
      "  Epoch 1 \tbatch 65 \tloss 0.229802115206 \ttrain acc 95.83 \tval acc 95.57 \n",
      "  Epoch 1 \tbatch 66 \tloss 0.44927785048 \ttrain acc 93.23 \tval acc 95.57 \n",
      "  Epoch 1 \tbatch 67 \tloss 0.32481928872 \ttrain acc 94.79 \tval acc 95.38 \n",
      "  Epoch 1 \tbatch 68 \tloss 0.185101057433 \ttrain acc 98.44 \tval acc 95.05 \n",
      "  Epoch 1 \tbatch 69 \tloss 0.339962277132 \ttrain acc 95.83 \tval acc 94.92 \n",
      "  Epoch 1 \tbatch 70 \tloss 0.364557602892 \ttrain acc 93.75 \tval acc 95.18 \n",
      "  Epoch 1 \tbatch 71 \tloss 0.351722978869 \ttrain acc 94.79 \tval acc 95.25 \n",
      "  Epoch 1 \tbatch 72 \tloss 0.171963989814 \ttrain acc 97.40 \tval acc 94.92 \n",
      "  Epoch 1 \tbatch 73 \tloss 0.537734598189 \ttrain acc 91.67 \tval acc 95.18 \n",
      "  Epoch 1 \tbatch 74 \tloss 0.194900323345 \ttrain acc 96.88 \tval acc 95.12 \n",
      "  Epoch 1 \tbatch 75 \tloss 0.313854822269 \ttrain acc 93.23 \tval acc 94.99 \n",
      "  Epoch 1 \tbatch 76 \tloss 0.206103736869 \ttrain acc 95.83 \tval acc 95.57 \n",
      "  Epoch 1 \tbatch 77 \tloss 0.139076679081 \ttrain acc 97.92 \tval acc 95.57 \n",
      "  Epoch 1 \tbatch 78 \tloss 0.307016074975 \ttrain acc 94.79 \tval acc 95.70 \n",
      "  Epoch 1 \tbatch 79 \tloss 0.239575605152 \ttrain acc 96.35 \tval acc 95.51 \n",
      "  Epoch 1 \tbatch 80 \tloss 0.212957925738 \ttrain acc 97.40 \tval acc 95.57 \n",
      "  Epoch 1 \tbatch 81 \tloss 0.203390802599 \ttrain acc 95.83 \tval acc 95.77 \n",
      "  Epoch 1 \tbatch 82 \tloss 0.0778817531634 \ttrain acc 99.48 \tval acc 95.90 \n",
      "  Epoch 1 \tbatch 83 \tloss 0.296980470668 \ttrain acc 96.35 \tval acc 95.83 \n",
      "  Epoch 1 \tbatch 84 \tloss 0.181195996339 \ttrain acc 97.40 \tval acc 95.77 \n",
      "  Epoch 1 \tbatch 85 \tloss 0.289473596876 \ttrain acc 95.31 \tval acc 95.96 \n",
      "  Epoch 1 \tbatch 86 \tloss 0.231224045249 \ttrain acc 95.83 \tval acc 95.90 \n",
      "  Epoch 1 \tbatch 87 \tloss 0.298601137663 \ttrain acc 95.31 \tval acc 96.03 \n",
      "  Epoch 1 \tbatch 88 \tloss 0.261459656439 \ttrain acc 94.79 \tval acc 95.77 \n",
      "  Epoch 1 \tbatch 89 \tloss 0.19764819773 \ttrain acc 97.40 \tval acc 96.42 \n",
      "  Epoch 1 \tbatch 90 \tloss 0.227297663458 \ttrain acc 96.35 \tval acc 96.22 \n",
      "  Epoch 1 \tbatch 91 \tloss 0.216318369552 \ttrain acc 96.35 \tval acc 96.03 \n",
      "  Epoch 1 \tbatch 92 \tloss 0.223961977537 \ttrain acc 96.35 \tval acc 96.03 \n",
      "  Epoch 1 \tbatch 93 \tloss 0.287221778083 \ttrain acc 95.83 \tval acc 95.96 \n",
      "  Epoch 1 \tbatch 94 \tloss 0.330435312913 \ttrain acc 93.75 \tval acc 96.03 \n",
      "  Epoch 1 \tbatch 95 \tloss 0.20808558399 \ttrain acc 96.88 \tval acc 96.16 \n",
      "  Epoch 1 \tbatch 96 \tloss 0.137718366627 \ttrain acc 97.92 \tval acc 95.96 \n",
      "  Epoch 1 \tbatch 97 \tloss 0.184466597732 \ttrain acc 97.92 \tval acc 96.03 \n",
      "  Epoch 1 \tbatch 98 \tloss 0.160256358495 \ttrain acc 97.40 \tval acc 96.29 \n",
      "  Epoch 1 \tbatch 99 \tloss 0.265732337568 \ttrain acc 94.27 \tval acc 96.29 \n",
      "  Epoch 1 \tbatch 100 \tloss 0.171546218522 \ttrain acc 97.40 \tval acc 96.22 \n",
      "  Epoch 1 \tbatch 101 \tloss 0.184833048581 \ttrain acc 96.35 \tval acc 96.16 \n",
      "  Epoch 1 \tbatch 102 \tloss 0.201460206837 \ttrain acc 95.83 \tval acc 96.16 \n",
      "  Epoch 1 \tbatch 103 \tloss 0.132196069147 \ttrain acc 97.92 \tval acc 96.16 \n",
      "  Epoch 1 \tbatch 104 \tloss 0.168836000954 \ttrain acc 97.40 \tval acc 96.16 \n",
      "  Epoch 1 \tbatch 105 \tloss 0.0438857850355 \ttrain acc 98.96 \tval acc 96.09 \n",
      "  Epoch 1 \tbatch 106 \tloss 0.20391407587 \ttrain acc 97.40 \tval acc 96.29 \n",
      "  Epoch 1 \tbatch 107 \tloss 0.194569123221 \ttrain acc 96.88 \tval acc 96.22 \n",
      "  Epoch 1 \tbatch 108 \tloss 0.178643437099 \ttrain acc 96.88 \tval acc 96.35 \n",
      "  Epoch 1 \tbatch 109 \tloss 0.127297243011 \ttrain acc 97.92 \tval acc 96.29 \n",
      "  Epoch 1 \tbatch 110 \tloss 0.116501179304 \ttrain acc 98.44 \tval acc 96.35 \n",
      "  Epoch 1 \tbatch 111 \tloss 0.225995284836 \ttrain acc 95.83 \tval acc 96.16 \n",
      "  Epoch 1 \tbatch 112 \tloss 0.136943066086 \ttrain acc 98.44 \tval acc 96.48 \n",
      "  Epoch 1 \tbatch 113 \tloss 0.0853198393834 \ttrain acc 98.96 \tval acc 96.29 \n",
      "  Epoch 1 \tbatch 114 \tloss 0.239315780315 \ttrain acc 95.83 \tval acc 96.48 \n",
      "Epoch 2 of 2 took 58.510s\n",
      "  training loss:\t\t0.340063\n",
      "  training accuracy:\t\t94.07 %\n",
      "  validation loss:\t\t0.238954\n",
      "  validation accuracy:\t\t96.61 %\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Training!!!\n",
    "train_losses, train_accuracies, val_accuracies = model.train(train_data, val_data, train_loss_acc, compute_loss_acc, num_epochs=num_epochs, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Predicted AST IDs\n",
      "[[   1.    7.    3.    0.   -1.   -1.]\n",
      " [ 102.    0.   -1.   -1.   -1.   -1.]\n",
      " [  11.   31.   -1.   -1.   -1.   -1.]\n",
      " [   5.    1.    0.   -1.   -1.   -1.]\n",
      " [  84.    1.    2.    0.   -1.   -1.]\n",
      " [  86.   25.   -1.   -1.   -1.   -1.]\n",
      " [   1.   24.    7.   -1.   -1.   -1.]\n",
      " [   5.    0.   -1.   -1.   -1.   -1.]\n",
      " [  10.    6.   -1.   -1.   -1.   -1.]\n",
      " [   1.    4.   25.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[   1.    7.    3.    0.   -1.   -1.]\n",
      " [ 102.    0.   -1.   -1.   -1.   -1.]\n",
      " [  11.   31.   -1.   -1.   -1.   -1.]\n",
      " [   5.    1.    0.   -1.   -1.   -1.]\n",
      " [  84.    1.    2.    0.   -1.   -1.]\n",
      " [  86.   25.   -1.   -1.   -1.   -1.]\n",
      " [   1.   24.    7.   -1.   -1.   -1.]\n",
      " [   5.    0.   -1.   -1.   -1.   -1.]\n",
      " [  10.    6.   -1.   -1.   -1.   -1.]\n",
      " [   1.    4.   25.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[ 119.   -1.   -1.   -1.   -1.   -1.]\n",
      " [  61.   76.   25.   -1.   -1.   -1.]\n",
      " [   6.    5.   -1.   -1.   -1.   -1.]\n",
      " [  13.  113.   27.   -1.   -1.   -1.]\n",
      " [ 291.    0.   -1.   -1.   -1.   -1.]\n",
      " [   8.    3.   22.   32.   20.    9.]\n",
      " [   1.   17.    6.    5.    0.   -1.]\n",
      " [ 168.    2.    0.   -1.   -1.   -1.]\n",
      " [   6.   17.    0.   -1.   -1.   -1.]\n",
      " [ 119.  123.   -1.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[ 589.   -1.   -1.   -1.   -1.   -1.]\n",
      " [  61.  204.   25.   -1.   -1.   -1.]\n",
      " [   6.    5.   -1.   -1.   -1.   -1.]\n",
      " [  13.  121.   27.   -1.   -1.   -1.]\n",
      " [ 248.    0.   -1.   -1.   -1.   -1.]\n",
      " [   8.    3.   22.   32.   20.    9.]\n",
      " [   1.   17.    6.    5.    0.   -1.]\n",
      " [ 168.    2.    0.   -1.   -1.   -1.]\n",
      " [   6.   17.    0.   -1.   -1.   -1.]\n",
      " [ 414.  358.   -1.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[  65.   38.    9.   -1.   -1.   -1.]\n",
      " [  16.  113.    0.   -1.   -1.   -1.]\n",
      " [   2.   41.    0.   -1.   -1.   -1.]\n",
      " [   1.    3.   28.   14.   -1.   -1.]\n",
      " [   3.   16.    0.   -1.   -1.   -1.]\n",
      " [  21.    1.    8.    3.    0.   -1.]\n",
      " [   1.    6.    4.   -1.   -1.   -1.]\n",
      " [  32.    0.   -1.   -1.   -1.   -1.]\n",
      " [  10.    1.    3.    4.    0.   -1.]\n",
      " [   1.    4.   18.    3.    0.   -1.]]\n",
      "Truth AST IDs\n",
      "[[  65.   38.    9.   -1.   -1.   -1.]\n",
      " [  16.  113.    0.   -1.   -1.   -1.]\n",
      " [   2.   41.    0.   -1.   -1.   -1.]\n",
      " [   1.    3.   28.   14.   -1.   -1.]\n",
      " [   3.   16.    0.   -1.   -1.   -1.]\n",
      " [  21.    1.    8.    3.    0.   -1.]\n",
      " [   1.    6.    4.   -1.   -1.   -1.]\n",
      " [  32.    0.   -1.   -1.   -1.   -1.]\n",
      " [  10.    1.    3.    4.    0.   -1.]\n",
      " [   1.    4.   18.    3.    0.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[   1.   10.    0.   -1.   -1.   -1.]\n",
      " [  84.  211.   89.    0.   -1.   -1.]\n",
      " [   4.   15.    2.    0.   -1.   -1.]\n",
      " [  38.   65.   52.   -1.   -1.   -1.]\n",
      " [ 135.  112.   -1.   -1.   -1.   -1.]\n",
      " [  18.    6.    3.    0.   -1.   -1.]\n",
      " [  61.  100.   -1.   -1.   -1.   -1.]\n",
      " [  19.   17.    0.   -1.   -1.   -1.]\n",
      " [  64.    2.    0.   -1.   -1.   -1.]\n",
      " [   8.    3.    0.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[   1.   10.    0.   -1.   -1.   -1.]\n",
      " [  84.  243.   89.    0.   -1.   -1.]\n",
      " [   4.   15.    2.    0.   -1.   -1.]\n",
      " [  38.  111.   52.   -1.   -1.   -1.]\n",
      " [ 135.  184.   -1.   -1.   -1.   -1.]\n",
      " [  18.    6.    3.    0.   -1.   -1.]\n",
      " [  61.  100.   -1.   -1.   -1.   -1.]\n",
      " [  19.   17.    0.   -1.   -1.   -1.]\n",
      " [  64.    2.    0.   -1.   -1.   -1.]\n",
      " [   8.    3.    0.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[  1.   3.  33.  29.  -1.  -1.]\n",
      " [ 24.   7.   0.  -1.  -1.  -1.]\n",
      " [ 18.   3.   0.  -1.  -1.  -1.]\n",
      " [  4.  45.   0.  -1.  -1.  -1.]\n",
      " [  4.  43.  26.  -1.  -1.  -1.]\n",
      " [  1.  12.  75.  -1.  -1.  -1.]\n",
      " [  1.   4.   3.  32.   9.  -1.]\n",
      " [  4.   1.   3.   0.  -1.  -1.]\n",
      " [ 13.  23.   2.   0.  -1.  -1.]\n",
      " [  4.  52.  -1.  -1.  -1.  -1.]]\n",
      "Truth AST IDs\n",
      "[[   1.    3.   93.   29.   -1.   -1.]\n",
      " [  24.    7.    0.   -1.   -1.   -1.]\n",
      " [  18.    3.    0.   -1.   -1.   -1.]\n",
      " [   4.   45.    0.   -1.   -1.   -1.]\n",
      " [   4.   43.   26.   -1.   -1.   -1.]\n",
      " [   1.   12.  178.   -1.   -1.   -1.]\n",
      " [   1.    4.    3.   32.    9.   -1.]\n",
      " [   4.    1.    3.    0.   -1.   -1.]\n",
      " [  13.   23.    2.    0.   -1.   -1.]\n",
      " [   4.   52.   -1.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[  75.    0.   -1.   -1.   -1.   -1.]\n",
      " [   8.    2.    8.    0.   -1.   -1.]\n",
      " [   6.   73.   19.    0.   -1.   -1.]\n",
      " [   1.    4.    1.    0.   -1.   -1.]\n",
      " [  44.    3.    0.   -1.   -1.   -1.]\n",
      " [ 211.   14.   -1.   -1.   -1.   -1.]\n",
      " [  40.   19.   -1.   -1.   -1.   -1.]\n",
      " [   2.  108.    0.   -1.   -1.   -1.]\n",
      " [ 162.    0.   -1.   -1.   -1.   -1.]\n",
      " [  16.   28.   14.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[ 220.    0.   -1.   -1.   -1.   -1.]\n",
      " [   8.    2.    8.    0.   -1.   -1.]\n",
      " [   6.   73.   19.    0.   -1.   -1.]\n",
      " [   1.    4.    1.    0.   -1.   -1.]\n",
      " [  44.    3.    0.   -1.   -1.   -1.]\n",
      " [ 113.   14.   -1.   -1.   -1.   -1.]\n",
      " [  40.   19.   -1.   -1.   -1.   -1.]\n",
      " [   2.  262.    0.   -1.   -1.   -1.]\n",
      " [ 162.    0.   -1.   -1.   -1.   -1.]\n",
      " [  16.   28.   14.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[  39.   12.   26.   -1.   -1.   -1.]\n",
      " [   4.   10.    1.   12.    0.   -1.]\n",
      " [  85.   20.    9.   -1.   -1.   -1.]\n",
      " [  17.   34.    0.   -1.   -1.   -1.]\n",
      " [  63.    3.   -1.   -1.   -1.   -1.]\n",
      " [  13.    2.    1.    0.   -1.   -1.]\n",
      " [  13.   39.   12.   26.   -1.   -1.]\n",
      " [ 133.    0.   -1.   -1.   -1.   -1.]\n",
      " [  66.   -1.   -1.   -1.   -1.   -1.]\n",
      " [ 139.    8.    3.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[  39.   12.   26.   -1.   -1.   -1.]\n",
      " [   4.   10.    1.   12.    0.   -1.]\n",
      " [  85.   20.    9.   -1.   -1.   -1.]\n",
      " [  17.   34.    0.   -1.   -1.   -1.]\n",
      " [  63.    3.   -1.   -1.   -1.   -1.]\n",
      " [  13.    2.    1.    0.   -1.   -1.]\n",
      " [  13.   39.   12.   26.   -1.   -1.]\n",
      " [ 133.    0.   -1.   -1.   -1.   -1.]\n",
      " [  66.   -1.   -1.   -1.   -1.   -1.]\n",
      " [ 175.    8.    3.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[  34.   14.   -1.   -1.   -1.   -1.]\n",
      " [   3.   12.  178.   -1.   -1.   -1.]\n",
      " [   8.    3.   24.    7.   42.   -1.]\n",
      " [  81.    0.   -1.   -1.   -1.   -1.]\n",
      " [ 117.    9.   -1.   -1.   -1.   -1.]\n",
      " [   4.    2.    0.   -1.   -1.   -1.]\n",
      " [   4.   10.    1.    4.    1.    0.]\n",
      " [  57.    0.   -1.   -1.   -1.   -1.]\n",
      " [  44.   73.    0.   -1.   -1.   -1.]\n",
      " [   1.   52.   -1.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[  34.   14.   -1.   -1.   -1.   -1.]\n",
      " [   3.   12.  178.   -1.   -1.   -1.]\n",
      " [   8.    3.   24.    7.   42.   -1.]\n",
      " [  81.    0.   -1.   -1.   -1.   -1.]\n",
      " [ 117.    9.   -1.   -1.   -1.   -1.]\n",
      " [   4.    2.    0.   -1.   -1.   -1.]\n",
      " [   4.   10.    1.    4.    1.    0.]\n",
      " [  57.    0.   -1.   -1.   -1.   -1.]\n",
      " [  44.   73.    0.   -1.   -1.   -1.]\n",
      " [   1.   52.   -1.   -1.   -1.   -1.]]\n",
      "Final results:\n",
      "  test loss:\t\t\t0.229070\n",
      "  test accuracy:\t\t96.48 %\n"
     ]
    }
   ],
   "source": [
    "model.check_accuracy(test_data, compute_loss_acc, row_to_ast_id_map, dataset_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Final results:\n",
      "  test loss:\t\t\t0.012642\n",
      "  test accuracy:\t\t99.93 %\n",
      "Predicted AST IDs\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "model.check_accuracy(train_data, compute_loss_acc, row_to_ast_id_map, dataset_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Final results:\n",
      "  test loss:\t\t\t21.436427\n",
      "  test accuracy:\t\t33.27 %\n",
      "Predicted AST IDs\n",
      "[[  89.  270.   -1.   -1.   -1.   -1.]\n",
      " [  89.  270.   -1.   -1.   -1.   -1.]\n",
      " [  89.  270.   -1.   -1.   -1.   -1.]\n",
      " [  89.  270.   -1.   -1.   -1.   -1.]\n",
      " [  89.  270.   -1.   -1.   -1.   -1.]\n",
      " [  89.  270.   -1.   -1.   -1.   -1.]\n",
      " [  89.  270.   -1.   -1.   -1.   -1.]\n",
      " [  89.  270.   -1.   -1.   -1.   -1.]\n",
      " [  89.  270.   -1.   -1.   -1.   -1.]\n",
      " [  89.  270.   -1.   -1.   -1.   -1.]]\n"
     ]
    }
   ],
   "source": [
    "model.check_accuracy(val_data, compute_loss_acc, row_to_ast_id_map, dataset_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAawAAAE+CAYAAADRZiTkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJCkoYYmISFL4f1sI13DoJJNYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c27ced0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss and the accuracies for both training and validation sets for each epoch\n",
    "visualize.plot_loss_acc(DATA_SET + '_train', train_losses, train_accuracies, val_accuracies, learning_rate, reg_strength, num_epochs, num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432\n",
      "0\n",
      "-1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
