{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Knowledge Tracing\n",
    "Authors: Lisa Wang, Angela Sy, Larry Liu\n",
    "\n",
    "### Task: Predict which block is coming next in an Abstract Syntax Tree (AST), and use the learned hidden representation as an embedding for this AST. This embedding can then be used for predicting the next AST.\n",
    "\n",
    "Input: For each of the N students, we have a time series of blocks from an AST. \n",
    "\n",
    "- input shape (num_asts, num_timesteps, num_blocks)\n",
    "    - num_timesteps is the max sequence length of blocks that we are taking into account.\n",
    "    - num_blocks is the total number of blocks that appear in an HOC problem.\n",
    "\n",
    "Output: At each timestep, we are predicting the next block.\n",
    "- Output shape (num_asts, num_timesteps, num_blocks). (one-hot encoding)\n",
    "\n",
    "The truth matrix contains the desired output for a given input, and is used to compute the loss as well as train/val/test accuracies.\n",
    "- Truth shape (num_asts, num_timesteps) Values are in range (0, num_blocks)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "An AST can look like this:\n",
    "\n",
    "{\n",
    "    \"id\": \"5\",\n",
    "    \"children\": [\n",
    "        {\n",
    "            \"id\": \"0\",\n",
    "            \"type\": \"maze_moveForward\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"1\",\n",
    "            \"type\": \"maze_moveForward\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"3\",\n",
    "            \"type\": \"maze_turnRight\"\n",
    "        }\n",
    "    ],\n",
    "    \"type\": \"program\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Python libraries\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as Tensor\n",
    "import lasagne\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.utils import shuffle\n",
    "# allows plots to show inline in ipython notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import our own modules\n",
    "import utils\n",
    "import model_predict_block as model\n",
    "import visualize\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 128 # size of hidden layer of neurons\n",
    "learning_rate = 1e-2\n",
    "lr_decay = 0.995\n",
    "reg_strength = 2e-2\n",
    "grad_clip = 10\n",
    "batchsize = 32\n",
    "num_epochs = 8\n",
    "dropout_p = 0.5\n",
    "num_lstm_layers = 1\n",
    "theano.config.optimizer='fast_compile'\n",
    "theano.config.exception_verbosity='high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing network inputs and targets, and the block maps...\n",
      "Trajectory matrix shape (5926, 20, 15)\n",
      "Inputs and targets done!\n",
      "num_timesteps 19\n",
      "X_train shape (5185, 19, 15)\n",
      "mask_train shape (5185, 19)\n",
      "y_train shape (5185, 19)\n",
      "X_val shape (370, 19, 15)\n",
      "X_test shape (371, 19, 15)\n",
      "[[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  1. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " ..., \n",
      " [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]]]\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      " [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 0 0]\n",
      " [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]]\n",
      "[[ 4.  2.  6.  0.  2.  2.  7.  6.  0.  2.  7.  2.  2.  2.  3.  0.  0.  0.\n",
      "   0.]\n",
      " [ 6.  0.  4.  2.  2.  2.  4.  2.  7.  3.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.]\n",
      " [ 6.  0.  4.  2.  2.  2.  2.  7.  6.  0.  2.  2.  7.  3.  0.  0.  0.  0.\n",
      "   0.]\n",
      " [ 4.  4.  2.  2.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.]\n",
      " [ 6.  0.  5.  5.  5.  2.  2.  2.  2.  2.  2.  7.  3.  0.  0.  0.  0.  0.\n",
      "   0.]\n",
      " [ 5.  4.  6.  0.  2.  7.  2.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.]\n",
      " [ 4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  3.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.]\n",
      " [ 2.  6.  0.  5.  7.  4.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.]\n",
      " [ 6.  0.  4.  4.  4.  5.  4.  5.  7.  6.  0.  4.  5.  4.  5.  2.  7.  3.\n",
      "   0.]\n",
      " [ 4.  4.  5.  2.  2.  2.  2.  2.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.]]\n",
      "[['maze_turnRight' 'maze_moveForward' 'controls_repeat' 'no_block'\n",
      "  'maze_moveForward' 'maze_moveForward' 'end_loop' 'controls_repeat'\n",
      "  'no_block' 'maze_moveForward' 'end_loop' 'maze_moveForward'\n",
      "  'maze_moveForward' 'maze_moveForward' 'end_program' 'no_block' 'no_block'\n",
      "  'no_block' 'no_block']\n",
      " ['controls_repeat' 'no_block' 'maze_turnRight' 'maze_moveForward'\n",
      "  'maze_moveForward' 'maze_moveForward' 'maze_turnRight' 'maze_moveForward'\n",
      "  'end_loop' 'end_program' 'no_block' 'no_block' 'no_block' 'no_block'\n",
      "  'no_block' 'no_block' 'no_block' 'no_block' 'no_block']\n",
      " ['controls_repeat' 'no_block' 'maze_turnRight' 'maze_moveForward'\n",
      "  'maze_moveForward' 'maze_moveForward' 'maze_moveForward' 'end_loop'\n",
      "  'controls_repeat' 'no_block' 'maze_moveForward' 'maze_moveForward'\n",
      "  'end_loop' 'end_program' 'no_block' 'no_block' 'no_block' 'no_block'\n",
      "  'no_block']\n",
      " ['maze_turnRight' 'maze_turnRight' 'maze_moveForward' 'maze_moveForward'\n",
      "  'end_program' 'no_block' 'no_block' 'no_block' 'no_block' 'no_block'\n",
      "  'no_block' 'no_block' 'no_block' 'no_block' 'no_block' 'no_block'\n",
      "  'no_block' 'no_block' 'no_block']\n",
      " ['controls_repeat' 'no_block' 'maze_turnLeft' 'maze_turnLeft'\n",
      "  'maze_turnLeft' 'maze_moveForward' 'maze_moveForward' 'maze_moveForward'\n",
      "  'maze_moveForward' 'maze_moveForward' 'maze_moveForward' 'end_loop'\n",
      "  'end_program' 'no_block' 'no_block' 'no_block' 'no_block' 'no_block'\n",
      "  'no_block']\n",
      " ['maze_turnLeft' 'maze_turnRight' 'controls_repeat' 'no_block'\n",
      "  'maze_moveForward' 'end_loop' 'maze_moveForward' 'end_program' 'no_block'\n",
      "  'no_block' 'no_block' 'no_block' 'no_block' 'no_block' 'no_block'\n",
      "  'no_block' 'no_block' 'no_block' 'no_block']\n",
      " ['maze_turnRight' 'maze_turnRight' 'maze_turnRight' 'maze_turnRight'\n",
      "  'maze_turnRight' 'maze_turnRight' 'maze_turnRight' 'maze_turnRight'\n",
      "  'maze_turnRight' 'maze_turnRight' 'end_program' 'no_block' 'no_block'\n",
      "  'no_block' 'no_block' 'no_block' 'no_block' 'no_block' 'no_block']\n",
      " ['maze_moveForward' 'controls_repeat' 'no_block' 'maze_turnLeft'\n",
      "  'end_loop' 'maze_turnRight' 'end_program' 'no_block' 'no_block'\n",
      "  'no_block' 'no_block' 'no_block' 'no_block' 'no_block' 'no_block'\n",
      "  'no_block' 'no_block' 'no_block' 'no_block']\n",
      " ['controls_repeat' 'no_block' 'maze_turnRight' 'maze_turnRight'\n",
      "  'maze_turnRight' 'maze_turnLeft' 'maze_turnRight' 'maze_turnLeft'\n",
      "  'end_loop' 'controls_repeat' 'no_block' 'maze_turnRight' 'maze_turnLeft'\n",
      "  'maze_turnRight' 'maze_turnLeft' 'maze_moveForward' 'end_loop'\n",
      "  'end_program' 'no_block']\n",
      " ['maze_turnRight' 'maze_turnRight' 'maze_turnLeft' 'maze_moveForward'\n",
      "  'maze_moveForward' 'maze_moveForward' 'maze_moveForward'\n",
      "  'maze_moveForward' 'end_program' 'no_block' 'no_block' 'no_block'\n",
      "  'no_block' 'no_block' 'no_block' 'no_block' 'no_block' 'no_block'\n",
      "  'no_block']]\n"
     ]
    }
   ],
   "source": [
    "HOC_NUM = 7\n",
    "DATA_SZ = -1\n",
    "train_data, val_data, test_data, all_data, num_timesteps, num_blocks  =\\\n",
    "utils.load_dataset_predict_block(hoc_num=HOC_NUM, data_sz=DATA_SZ)\n",
    "\n",
    "print 'num_timesteps {}'.format(num_timesteps)\n",
    "\n",
    "X_train, mask_train, y_train = train_data\n",
    "X_val, mask_val, y_val = val_data\n",
    "X_test, mask_test, y_test = test_data\n",
    "print 'X_train shape {}'.format(X_train.shape)\n",
    "print 'mask_train shape {}'.format(mask_train.shape)\n",
    "print 'y_train shape {}'.format(y_train.shape)\n",
    "print 'X_val shape {}'.format(X_val.shape)\n",
    "print 'X_test shape {}'.format(X_test.shape)\n",
    "print X_train[:10,:15:]\n",
    "print mask_train[:10,:15]\n",
    "print y_train[:10]\n",
    "\n",
    "print utils.convert_to_block_strings(y_train[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Compiling done!\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "print num_blocks\n",
    "train_loss_acc, compute_loss_acc, probs, generate_hidden_reps, compute_pred = model.create_model(num_timesteps, num_blocks, hidden_size, learning_rate, \\\n",
    "                                                             grad_clip, dropout_p, num_lstm_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training :)...\n",
      "Total training iterations: 504\n",
      "Ep 0 \titer 1  \tloss 1.79028, train acc 20.72, train corr acc 16.72, val acc 49.26, val corr acc 1.20\n",
      "Ep 0 \titer 2  \tloss 1.74787, train acc 51.48, train corr acc 0.67, val acc 48.68, val corr acc 0.07\n",
      "Ep 0 \titer 3  \tloss 1.62221, train acc 48.36, train corr acc 0.00, val acc 48.64, val corr acc 0.00\n",
      "Ep 0 \titer 4  \tloss 1.41561, train acc 48.52, train corr acc 0.00, val acc 48.64, val corr acc 0.00\n",
      "Ep 0 \titer 5  \tloss 1.64868, train acc 44.57, train corr acc 0.00, val acc 48.64, val corr acc 0.00\n",
      "Ep 0 \titer 6  \tloss 1.40385, train acc 47.53, train corr acc 0.00, val acc 50.34, val corr acc 3.32\n",
      "Ep 0 \titer 7  \tloss 1.26705, train acc 56.74, train corr acc 2.59, val acc 52.23, val corr acc 7.00\n",
      "Ep 0 \titer 8  \tloss 1.29537, train acc 58.06, train corr acc 8.60, val acc 52.60, val corr acc 7.70\n",
      "Ep 0 \titer 9  \tloss 1.39847, train acc 50.99, train corr acc 10.24, val acc 52.89, val corr acc 8.27\n",
      "Ep 0 \titer 10  \tloss 1.33388, train acc 53.45, train corr acc 9.00, val acc 52.99, val corr acc 8.48\n",
      "Ep 0 \titer 11  \tloss 1.30975, train acc 52.47, train corr acc 9.12, val acc 53.28, val corr acc 9.05\n",
      "Ep 0 \titer 12  \tloss 1.34475, train acc 49.34, train corr acc 9.38, val acc 53.54, val corr acc 9.54\n",
      "Ep 0 \titer 13  \tloss 1.35028, train acc 50.16, train corr acc 9.28, val acc 54.05, val corr acc 10.53\n",
      "Ep 0 \titer 14  \tloss 1.26078, train acc 57.40, train corr acc 12.24, val acc 54.45, val corr acc 12.23\n",
      "Ep 0 \titer 15  \tloss 1.35297, train acc 50.33, train corr acc 11.95, val acc 55.39, val corr acc 15.62\n",
      "Ep 0 \titer 16  \tloss 1.25924, train acc 55.43, train corr acc 15.28, val acc 57.31, val corr acc 20.14\n",
      "Ep 0 \titer 17  \tloss 1.27577, train acc 54.11, train corr acc 21.74, val acc 55.21, val corr acc 14.28\n",
      "Ep 0 \titer 18  \tloss 1.19659, train acc 57.40, train corr acc 15.08, val acc 54.88, val corr acc 13.14\n",
      "Ep 0 \titer 19  \tloss 1.12592, train acc 62.99, train corr acc 18.48, val acc 54.85, val corr acc 12.72\n",
      "Ep 0 \titer 20  \tloss 1.27542, train acc 53.78, train corr acc 11.36, val acc 54.59, val corr acc 12.58\n",
      "Ep 0 \titer 21  \tloss 1.25512, train acc 55.26, train corr acc 13.38, val acc 54.63, val corr acc 12.72\n",
      "Ep 0 \titer 22  \tloss 1.28952, train acc 52.30, train corr acc 14.06, val acc 55.17, val corr acc 14.13\n",
      "Ep 0 \titer 23  \tloss 1.21001, train acc 55.76, train corr acc 11.80, val acc 55.83, val corr acc 16.68\n",
      "Ep 0 \titer 24  \tloss 1.13199, train acc 62.66, train corr acc 19.50, val acc 56.99, val corr acc 19.51\n",
      "Ep 0 \titer 25  \tloss 1.28980, train acc 49.18, train corr acc 16.72, val acc 56.44, val corr acc 19.22\n",
      "Ep 0 \titer 26  \tloss 1.26072, train acc 52.80, train corr acc 22.51, val acc 56.01, val corr acc 21.48\n",
      "Ep 0 \titer 27  \tloss 1.31247, train acc 48.52, train corr acc 18.58, val acc 56.33, val corr acc 22.33\n",
      "Ep 0 \titer 28  \tloss 1.27689, train acc 51.48, train corr acc 25.00, val acc 56.62, val corr acc 19.43\n",
      "Ep 0 \titer 29  \tloss 1.26977, train acc 50.66, train corr acc 20.24, val acc 55.90, val corr acc 17.17\n",
      "Ep 0 \titer 30  \tloss 1.25364, train acc 53.29, train corr acc 19.05, val acc 55.90, val corr acc 15.48\n",
      "Ep 0 \titer 31  \tloss 1.22685, train acc 54.44, train corr acc 14.86, val acc 55.14, val corr acc 13.78\n",
      "Ep 0 \titer 32  \tloss 1.22800, train acc 54.61, train corr acc 14.29, val acc 55.10, val corr acc 13.71\n",
      "Ep 0 \titer 33  \tloss 1.22508, train acc 55.59, train corr acc 14.83, val acc 55.28, val corr acc 14.06\n",
      "Ep 0 \titer 34  \tloss 1.15535, train acc 59.54, train corr acc 16.61, val acc 55.32, val corr acc 14.13\n",
      "Ep 0 \titer 35  \tloss 1.23293, train acc 53.78, train corr acc 14.24, val acc 55.46, val corr acc 14.42\n",
      "Ep 0 \titer 36  \tloss 1.25210, train acc 53.12, train corr acc 15.68, val acc 55.50, val corr acc 14.77\n",
      "Ep 0 \titer 37  \tloss 1.21865, train acc 53.95, train corr acc 14.11, val acc 56.44, val corr acc 17.67\n",
      "Ep 0 \titer 38  \tloss 1.22664, train acc 55.76, train corr acc 19.46, val acc 56.59, val corr acc 18.80\n",
      "Ep 0 \titer 39  \tloss 1.22785, train acc 54.28, train corr acc 19.57, val acc 57.06, val corr acc 19.93\n",
      "Ep 0 \titer 40  \tloss 1.17501, train acc 55.59, train corr acc 16.88, val acc 56.99, val corr acc 20.42\n",
      "Ep 0 \titer 41  \tloss 1.28215, train acc 48.68, train corr acc 20.87, val acc 57.28, val corr acc 20.35\n",
      "Ep 0 \titer 42  \tloss 1.20460, train acc 53.78, train corr acc 18.81, val acc 57.06, val corr acc 20.21\n",
      "Ep 0 \titer 43  \tloss 1.19912, train acc 54.77, train corr acc 17.47, val acc 56.95, val corr acc 19.51\n",
      "Ep 0 \titer 44  \tloss 1.24868, train acc 53.29, train corr acc 17.35, val acc 56.73, val corr acc 19.58\n",
      "Ep 0 \titer 45  \tloss 1.09194, train acc 60.53, train corr acc 20.33, val acc 56.95, val corr acc 19.86\n",
      "Ep 0 \titer 46  \tloss 1.33638, train acc 49.51, train corr acc 19.22, val acc 56.73, val corr acc 19.43\n",
      "Ep 0 \titer 47  \tloss 1.19437, train acc 55.76, train corr acc 20.13, val acc 56.33, val corr acc 18.37\n",
      "Ep 0 \titer 48  \tloss 1.09228, train acc 59.54, train corr acc 20.39, val acc 56.66, val corr acc 18.94\n",
      "Ep 0 \titer 49  \tloss 1.20953, train acc 52.47, train corr acc 17.74, val acc 56.62, val corr acc 18.73\n",
      "Ep 0 \titer 50  \tloss 1.12110, train acc 59.87, train corr acc 21.64, val acc 56.84, val corr acc 18.94\n",
      "Ep 0 \titer 51  \tloss 1.21959, train acc 54.44, train corr acc 15.26, val acc 56.99, val corr acc 18.87\n",
      "Ep 0 \titer 52  \tloss 1.17356, train acc 55.10, train corr acc 18.87, val acc 57.02, val corr acc 18.94\n",
      "Ep 0 \titer 53  \tloss 1.30251, train acc 49.67, train corr acc 14.71, val acc 56.48, val corr acc 17.88\n",
      "Ep 0 \titer 54  \tloss 1.17473, train acc 55.26, train corr acc 16.61, val acc 57.06, val corr acc 19.29\n",
      "Ep 0 \titer 55  \tloss 1.26288, train acc 55.76, train corr acc 21.00, val acc 57.28, val corr acc 20.00\n",
      "Ep 0 \titer 56  \tloss 1.19380, train acc 54.77, train corr acc 18.26, val acc 57.39, val corr acc 20.35\n",
      "Ep 0 \titer 57  \tloss 1.09063, train acc 61.51, train corr acc 22.45, val acc 57.53, val corr acc 20.71\n",
      "Ep 0 \titer 58  \tloss 1.07321, train acc 60.53, train corr acc 19.57, val acc 56.84, val corr acc 19.36\n",
      "Ep 0 \titer 59  \tloss 1.11022, train acc 59.54, train corr acc 20.39, val acc 57.39, val corr acc 20.35\n",
      "Ep 0 \titer 60  \tloss 1.20602, train acc 56.58, train corr acc 22.47, val acc 56.52, val corr acc 18.66\n",
      "Ep 0 \titer 61  \tloss 1.17882, train acc 58.55, train corr acc 23.58, val acc 56.37, val corr acc 18.23\n",
      "Ep 0 \titer 62  \tloss 1.07547, train acc 61.84, train corr acc 23.68, val acc 57.02, val corr acc 19.58\n",
      "Ep 0 \titer 63  \tloss 1.23437, train acc 52.63, train corr acc 17.12, val acc 56.81, val corr acc 19.08\n",
      "\n",
      "Epoch 1 of 8 took 22.480s\n",
      "  training loss:\t\t1.165082\n",
      "  training accuracy:\t\t55.32 %\n",
      "  training corrected acc:\t\t20.42 %\n",
      "  validation loss:\t\t1.132417\n",
      "  validation accuracy:\t\t56.81 %\n",
      "  validation corrected acc:\t\t19.08 % \n",
      "\n",
      "Ep 1 \titer 64  \tloss 1.14693, train acc 57.40, train corr acc 21.95, val acc 57.02, val corr acc 19.65\n",
      "Ep 1 \titer 65  \tloss 1.14532, train acc 55.76, train corr acc 22.56, val acc 57.46, val corr acc 20.42\n",
      "Ep 1 \titer 66  \tloss 1.12205, train acc 58.22, train corr acc 20.70, val acc 57.53, val corr acc 20.57\n",
      "Ep 1 \titer 67  \tloss 1.21402, train acc 55.92, train corr acc 25.88, val acc 56.99, val corr acc 19.51\n",
      "Ep 1 \titer 68  \tloss 1.14458, train acc 55.59, train corr acc 19.88, val acc 56.66, val corr acc 18.73\n",
      "Ep 1 \titer 69  \tloss 1.16406, train acc 57.57, train corr acc 21.00, val acc 56.23, val corr acc 17.31\n",
      "Ep 1 \titer 70  \tloss 1.08144, train acc 61.84, train corr acc 17.78, val acc 56.73, val corr acc 17.74\n",
      "Ep 1 \titer 71  \tloss 1.08010, train acc 60.03, train corr acc 18.64, val acc 56.55, val corr acc 16.82\n",
      "Ep 1 \titer 72  \tloss 1.21177, train acc 55.59, train corr acc 18.98, val acc 56.04, val corr acc 15.97\n",
      "Ep 1 \titer 73  \tloss 1.16309, train acc 58.06, train corr acc 18.01, val acc 56.55, val corr acc 17.31\n",
      "Ep 1 \titer 74  \tloss 1.14127, train acc 56.91, train corr acc 17.61, val acc 57.02, val corr acc 18.87\n",
      "Ep 1 \titer 75  \tloss 1.24963, train acc 55.43, train corr acc 20.94, val acc 57.71, val corr acc 20.71\n",
      "Ep 1 \titer 76  \tloss 1.18952, train acc 55.43, train corr acc 18.86, val acc 57.64, val corr acc 20.99\n",
      "Ep 1 \titer 77  \tloss 1.19344, train acc 57.24, train corr acc 22.11, val acc 57.21, val corr acc 20.92\n",
      "Ep 1 \titer 78  \tloss 1.19265, train acc 54.77, train corr acc 20.12, val acc 57.71, val corr acc 21.84\n",
      "Ep 1 \titer 79  \tloss 1.13925, train acc 59.05, train corr acc 22.59, val acc 57.53, val corr acc 21.06\n",
      "Ep 1 \titer 80  \tloss 1.15613, train acc 57.89, train corr acc 22.46, val acc 57.89, val corr acc 21.13\n",
      "Ep 1 \titer 81  \tloss 1.09686, train acc 60.53, train corr acc 21.31, val acc 56.77, val corr acc 18.87\n",
      "Ep 1 \titer 82  \tloss 1.04556, train acc 64.64, train corr acc 23.19, val acc 56.73, val corr acc 18.52\n",
      "Ep 1 \titer 83  \tloss 1.18117, train acc 57.24, train corr acc 18.30, val acc 56.91, val corr acc 18.30\n",
      "Ep 1 \titer 84  \tloss 1.10441, train acc 59.87, train corr acc 22.29, val acc 57.21, val corr acc 19.29\n",
      "Ep 1 \titer 85  \tloss 1.16954, train acc 56.25, train corr acc 21.88, val acc 57.57, val corr acc 20.49\n",
      "Ep 1 \titer 86  \tloss 1.09388, train acc 60.20, train corr acc 21.31, val acc 58.40, val corr acc 22.69\n",
      "Ep 1 \titer 87  \tloss 1.06609, train acc 64.14, train corr acc 26.24, val acc 58.62, val corr acc 23.11\n",
      "Ep 1 \titer 88  \tloss 1.24484, train acc 50.99, train corr acc 22.69, val acc 58.69, val corr acc 23.04\n",
      "Ep 1 \titer 89  \tloss 1.24638, train acc 53.62, train corr acc 24.50, val acc 59.13, val corr acc 24.24\n",
      "Ep 1 \titer 90  \tloss 1.27770, train acc 50.00, train corr acc 21.86, val acc 58.48, val corr acc 24.17\n",
      "Ep 1 \titer 91  \tloss 1.17421, train acc 57.24, train corr acc 24.07, val acc 58.48, val corr acc 23.25\n",
      "Ep 1 \titer 92  \tloss 1.13569, train acc 56.25, train corr acc 30.06, val acc 58.44, val corr acc 22.83\n",
      "Ep 1 \titer 93  \tloss 1.24405, train acc 53.12, train corr acc 26.03, val acc 58.29, val corr acc 22.26\n",
      "Ep 1 \titer 94  \tloss 1.13643, train acc 55.59, train corr acc 21.05, val acc 57.71, val corr acc 21.27\n",
      "Ep 1 \titer 95  \tloss 1.13016, train acc 58.72, train corr acc 25.47, val acc 57.42, val corr acc 19.79\n",
      "Ep 1 \titer 96  \tloss 1.10350, train acc 59.54, train corr acc 22.71, val acc 58.29, val corr acc 21.13\n",
      "Ep 1 \titer 97  \tloss 1.01240, train acc 63.49, train corr acc 24.75, val acc 58.51, val corr acc 21.84\n",
      "Ep 1 \titer 98  \tloss 1.14045, train acc 56.58, train corr acc 19.74, val acc 58.19, val corr acc 22.47\n",
      "Ep 1 \titer 99  \tloss 1.18362, train acc 55.26, train corr acc 26.92, val acc 57.89, val corr acc 21.20\n",
      "Ep 1 \titer 100  \tloss 1.08599, train acc 58.06, train corr acc 25.46, val acc 58.33, val corr acc 22.61\n",
      "Ep 1 \titer 101  \tloss 1.15991, train acc 58.39, train corr acc 24.85, val acc 58.58, val corr acc 23.96\n",
      "Ep 1 \titer 102  \tloss 1.15076, train acc 58.55, train corr acc 29.05, val acc 58.22, val corr acc 24.24\n",
      "Ep 1 \titer 103  \tloss 1.10582, train acc 57.89, train corr acc 21.97, val acc 58.51, val corr acc 25.37\n",
      "Ep 1 \titer 104  \tloss 1.23377, train acc 48.68, train corr acc 24.30, val acc 59.02, val corr acc 26.01\n",
      "Ep 1 \titer 105  \tloss 1.11964, train acc 59.54, train corr acc 30.15, val acc 58.51, val corr acc 24.03\n",
      "Ep 1 \titer 106  \tloss 1.11670, train acc 57.24, train corr acc 24.40, val acc 58.62, val corr acc 24.45\n",
      "Ep 1 \titer 107  \tloss 1.19320, train acc 56.74, train corr acc 23.53, val acc 58.48, val corr acc 24.24\n",
      "Ep 1 \titer 108  \tloss 1.03086, train acc 62.99, train corr acc 27.33, val acc 58.66, val corr acc 24.59\n",
      "Ep 1 \titer 109  \tloss 1.26265, train acc 54.11, train corr acc 27.58, val acc 58.40, val corr acc 23.82\n",
      "Ep 1 \titer 110  \tloss 1.13910, train acc 58.72, train corr acc 26.52, val acc 58.44, val corr acc 23.82\n",
      "Ep 1 \titer 111  \tloss 1.08263, train acc 58.55, train corr acc 19.09, val acc 58.62, val corr acc 24.10\n",
      "Ep 1 \titer 112  \tloss 1.16778, train acc 54.11, train corr acc 21.29, val acc 59.02, val corr acc 24.03\n",
      "Ep 1 \titer 113  \tloss 1.03949, train acc 61.84, train corr acc 29.18, val acc 58.80, val corr acc 23.60\n",
      "Ep 1 \titer 114  \tloss 1.16863, train acc 57.24, train corr acc 21.43, val acc 58.87, val corr acc 23.67\n",
      "Ep 1 \titer 115  \tloss 1.13788, train acc 55.92, train corr acc 21.70, val acc 58.37, val corr acc 23.25\n",
      "Ep 1 \titer 116  \tloss 1.25745, train acc 52.80, train corr acc 20.59, val acc 58.44, val corr acc 24.10\n",
      "Ep 1 \titer 117  \tloss 1.11996, train acc 55.92, train corr acc 22.68, val acc 58.95, val corr acc 25.09\n",
      "Ep 1 \titer 118  \tloss 1.24429, train acc 56.91, train corr acc 25.39, val acc 58.91, val corr acc 25.58\n",
      "Ep 1 \titer 119  \tloss 1.15149, train acc 57.24, train corr acc 26.95, val acc 58.91, val corr acc 26.08\n",
      "Ep 1 \titer 120  \tloss 1.04427, train acc 61.51, train corr acc 27.89, val acc 58.80, val corr acc 25.65\n",
      "Ep 1 \titer 121  \tloss 1.02067, train acc 64.14, train corr acc 31.32, val acc 58.37, val corr acc 23.46\n",
      "Ep 1 \titer 122  \tloss 1.07243, train acc 60.53, train corr acc 22.98, val acc 58.44, val corr acc 22.26\n",
      "Ep 1 \titer 123  \tloss 1.11960, train acc 59.05, train corr acc 25.32, val acc 58.37, val corr acc 22.47\n",
      "Ep 1 \titer 124  \tloss 1.14338, train acc 57.73, train corr acc 26.42, val acc 58.22, val corr acc 22.12\n",
      "Ep 1 \titer 125  \tloss 0.98661, train acc 64.31, train corr acc 28.62, val acc 58.62, val corr acc 23.11\n",
      "Ep 1 \titer 126  \tloss 1.17224, train acc 54.77, train corr acc 20.72, val acc 58.37, val corr acc 24.81\n",
      "\n",
      "Epoch 2 of 8 took 22.788s\n",
      "  training loss:\t\t1.152080\n",
      "  training accuracy:\t\t55.86 %\n",
      "  training corrected acc:\t\t24.24 %\n",
      "  validation loss:\t\t1.114219\n",
      "  validation accuracy:\t\t58.37 %\n",
      "  validation corrected acc:\t\t24.81 % \n",
      "\n",
      "Ep 2 \titer 127  \tloss 1.13294, train acc 57.57, train corr acc 24.04, val acc 58.58, val corr acc 25.65\n",
      "Ep 2 \titer 128  \tloss 1.15885, train acc 57.07, train corr acc 27.61, val acc 59.35, val corr acc 25.16\n",
      "Ep 2 \titer 129  \tloss 1.11442, train acc 60.36, train corr acc 27.39, val acc 58.98, val corr acc 23.75\n",
      "Ep 2 \titer 130  \tloss 1.16085, train acc 54.93, train corr acc 25.56, val acc 58.19, val corr acc 22.05\n",
      "Ep 2 \titer 131  \tloss 1.14773, train acc 55.10, train corr acc 20.47, val acc 58.11, val corr acc 22.33\n",
      "Ep 2 \titer 132  \tloss 1.10048, train acc 58.88, train corr acc 23.82, val acc 58.00, val corr acc 21.84\n",
      "Ep 2 \titer 133  \tloss 1.06547, train acc 62.66, train corr acc 21.85, val acc 57.64, val corr acc 20.99\n",
      "Ep 2 \titer 134  \tloss 1.09306, train acc 61.51, train corr acc 22.58, val acc 57.86, val corr acc 20.85\n",
      "Ep 2 \titer 135  \tloss 1.21041, train acc 55.76, train corr acc 24.70, val acc 58.11, val corr acc 20.92\n",
      "Ep 2 \titer 136  \tloss 1.14398, train acc 58.72, train corr acc 20.90, val acc 57.97, val corr acc 20.49\n",
      "Ep 2 \titer 137  \tloss 1.10156, train acc 60.20, train corr acc 23.90, val acc 58.08, val corr acc 21.27\n",
      "Ep 2 \titer 138  \tloss 1.21806, train acc 59.38, train corr acc 29.69, val acc 58.58, val corr acc 23.53\n",
      "Ep 2 \titer 139  \tloss 1.16747, train acc 58.22, train corr acc 25.15, val acc 58.22, val corr acc 24.10\n",
      "Ep 2 \titer 140  \tloss 1.16951, train acc 58.22, train corr acc 23.81, val acc 58.37, val corr acc 24.17\n",
      "Ep 2 \titer 141  \tloss 1.15851, train acc 57.07, train corr acc 25.36, val acc 58.51, val corr acc 24.95\n",
      "Ep 2 \titer 142  \tloss 1.09463, train acc 59.87, train corr acc 24.25, val acc 58.66, val corr acc 25.44\n",
      "Ep 2 \titer 143  \tloss 1.12487, train acc 60.20, train corr acc 25.36, val acc 58.77, val corr acc 24.45\n",
      "Ep 2 \titer 144  \tloss 1.07121, train acc 62.34, train corr acc 26.23, val acc 58.98, val corr acc 23.96\n",
      "Ep 2 \titer 145  \tloss 1.04627, train acc 62.83, train corr acc 23.91, val acc 58.29, val corr acc 20.85\n",
      "Ep 2 \titer 146  \tloss 1.13158, train acc 57.40, train corr acc 18.93, val acc 58.58, val corr acc 20.85\n",
      "Ep 2 \titer 147  \tloss 1.04221, train acc 61.68, train corr acc 27.07, val acc 58.44, val corr acc 20.57\n",
      "Ep 2 \titer 148  \tloss 1.15274, train acc 57.07, train corr acc 23.44, val acc 58.26, val corr acc 20.57\n",
      "Ep 2 \titer 149  \tloss 1.07443, train acc 61.35, train corr acc 24.59, val acc 59.78, val corr acc 23.53\n",
      "Ep 2 \titer 150  \tloss 1.01538, train acc 65.46, train corr acc 25.89, val acc 59.89, val corr acc 24.81\n",
      "Ep 2 \titer 151  \tloss 1.18730, train acc 56.41, train corr acc 23.88, val acc 59.67, val corr acc 26.15\n",
      "Ep 2 \titer 152  \tloss 1.18717, train acc 55.92, train corr acc 28.77, val acc 59.53, val corr acc 29.12\n",
      "Ep 2 \titer 153  \tloss 1.25534, train acc 52.30, train corr acc 26.23, val acc 57.57, val corr acc 29.82\n",
      "Ep 2 \titer 154  \tloss 1.19761, train acc 53.62, train corr acc 28.40, val acc 58.55, val corr acc 30.60\n",
      "Ep 2 \titer 155  \tloss 1.08133, train acc 61.18, train corr acc 43.45, val acc 59.67, val corr acc 30.32\n",
      "Ep 2 \titer 156  \tloss 1.27457, train acc 53.78, train corr acc 33.33, val acc 60.47, val corr acc 27.63\n",
      "Ep 2 \titer 157  \tloss 1.09782, train acc 56.58, train corr acc 23.53, val acc 60.33, val corr acc 25.65\n",
      "Ep 2 \titer 158  \tloss 1.07001, train acc 62.01, train corr acc 29.81, val acc 60.40, val corr acc 24.81\n",
      "Ep 2 \titer 159  \tloss 1.05387, train acc 62.01, train corr acc 28.08, val acc 59.89, val corr acc 23.82\n",
      "Ep 2 \titer 160  \tloss 0.99852, train acc 63.49, train corr acc 24.75, val acc 60.07, val corr acc 24.17\n",
      "Ep 2 \titer 161  \tloss 1.11114, train acc 57.89, train corr acc 22.33, val acc 60.22, val corr acc 24.88\n",
      "Ep 2 \titer 162  \tloss 1.14722, train acc 58.22, train corr acc 27.51, val acc 59.35, val corr acc 23.67\n",
      "Ep 2 \titer 163  \tloss 1.06856, train acc 61.51, train corr acc 30.37, val acc 58.98, val corr acc 24.38\n",
      "Ep 2 \titer 164  \tloss 1.16301, train acc 58.39, train corr acc 25.15, val acc 59.06, val corr acc 25.72\n",
      "Ep 2 \titer 165  \tloss 1.13450, train acc 59.70, train corr acc 31.50, val acc 58.80, val corr acc 27.56\n",
      "Ep 2 \titer 166  \tloss 1.10954, train acc 57.89, train corr acc 21.97, val acc 58.80, val corr acc 29.05\n",
      "Ep 2 \titer 167  \tloss 1.22000, train acc 49.84, train corr acc 26.17, val acc 58.40, val corr acc 28.41\n",
      "Ep 2 \titer 168  \tloss 1.12349, train acc 56.58, train corr acc 29.55, val acc 59.06, val corr acc 28.90\n",
      "Ep 2 \titer 169  \tloss 1.07409, train acc 59.38, train corr acc 31.63, val acc 59.13, val corr acc 27.70\n",
      "Ep 2 \titer 170  \tloss 1.17177, train acc 59.05, train corr acc 27.65, val acc 59.35, val corr acc 27.00\n",
      "Ep 2 \titer 171  \tloss 1.02044, train acc 63.65, train corr acc 29.67, val acc 59.02, val corr acc 25.65\n",
      "Ep 2 \titer 172  \tloss 1.23115, train acc 54.28, train corr acc 29.81, val acc 58.77, val corr acc 24.81\n",
      "Ep 2 \titer 173  \tloss 1.13669, train acc 57.89, train corr acc 24.60, val acc 59.17, val corr acc 24.88\n",
      "Ep 2 \titer 174  \tloss 1.07653, train acc 58.39, train corr acc 19.42, val acc 58.80, val corr acc 24.31\n",
      "Ep 2 \titer 175  \tloss 1.14345, train acc 57.89, train corr acc 25.48, val acc 59.02, val corr acc 24.24\n",
      "Ep 2 \titer 176  \tloss 1.05246, train acc 62.17, train corr acc 29.18, val acc 59.09, val corr acc 24.10\n",
      "Ep 2 \titer 177  \tloss 1.18056, train acc 55.76, train corr acc 18.51, val acc 59.35, val corr acc 25.09\n",
      "Ep 2 \titer 178  \tloss 1.09984, train acc 58.06, train corr acc 25.47, val acc 59.56, val corr acc 26.08\n",
      "Ep 2 \titer 179  \tloss 1.24284, train acc 54.77, train corr acc 25.88, val acc 60.22, val corr acc 28.20\n",
      "Ep 2 \titer 180  \tloss 1.11048, train acc 56.09, train corr acc 23.32, val acc 60.36, val corr acc 28.69\n",
      "Ep 2 \titer 181  \tloss 1.17929, train acc 58.22, train corr acc 31.66, val acc 60.36, val corr acc 29.33\n",
      "Ep 2 \titer 182  \tloss 1.11661, train acc 58.72, train corr acc 29.04, val acc 59.71, val corr acc 29.05\n",
      "Ep 2 \titer 183  \tloss 1.01091, train acc 63.65, train corr acc 30.61, val acc 59.02, val corr acc 28.06\n",
      "Ep 2 \titer 184  \tloss 1.01638, train acc 65.30, train corr acc 32.03, val acc 58.77, val corr acc 27.14\n",
      "Ep 2 \titer 185  \tloss 1.07394, train acc 61.84, train corr acc 31.39, val acc 59.56, val corr acc 25.87\n",
      "Ep 2 \titer 186  \tloss 1.05214, train acc 62.50, train corr acc 32.59, val acc 58.69, val corr acc 22.83\n",
      "Ep 2 \titer 187  \tloss 1.14333, train acc 58.55, train corr acc 27.67, val acc 58.33, val corr acc 21.27\n",
      "Ep 2 \titer 188  \tloss 1.00466, train acc 64.14, train corr acc 28.29, val acc 58.58, val corr acc 22.40\n",
      "Ep 2 \titer 189  \tloss 1.19682, train acc 54.77, train corr acc 18.92, val acc 59.02, val corr acc 23.39\n",
      "\n",
      "Epoch 3 of 8 took 24.574s\n",
      "  training loss:\t\t1.144662\n",
      "  training accuracy:\t\t56.91 %\n",
      "  training corrected acc:\t\t22.76 %\n",
      "  validation loss:\t\t1.120784\n",
      "  validation accuracy:\t\t59.02 %\n",
      "  validation corrected acc:\t\t23.39 % \n",
      "\n",
      "Ep 3 \titer 190  \tloss 1.12955, train acc 57.89, train corr acc 23.34, val acc 59.31, val corr acc 25.16\n",
      "Ep 3 \titer 191  \tloss 1.10867, train acc 59.21, train corr acc 26.26, val acc 58.08, val corr acc 24.38\n",
      "Ep 3 \titer 192  \tloss 1.09658, train acc 59.38, train corr acc 25.80, val acc 58.22, val corr acc 25.51\n",
      "Ep 3 \titer 193  \tloss 1.17249, train acc 56.41, train corr acc 27.48, val acc 58.73, val corr acc 25.58\n",
      "Ep 3 \titer 194  \tloss 1.13683, train acc 56.58, train corr acc 25.82, val acc 58.44, val corr acc 25.58\n",
      "Ep 3 \titer 195  \tloss 1.13909, train acc 58.39, train corr acc 28.84, val acc 58.11, val corr acc 23.89\n",
      "Ep 3 \titer 196  \tloss 1.05114, train acc 63.49, train corr acc 27.04, val acc 58.73, val corr acc 23.82\n",
      "Ep 3 \titer 197  \tloss 1.07299, train acc 60.69, train corr acc 22.58, val acc 57.60, val corr acc 20.78\n",
      "Ep 3 \titer 198  \tloss 1.16424, train acc 55.43, train corr acc 24.10, val acc 57.86, val corr acc 20.85\n",
      "Ep 3 \titer 199  \tloss 1.13390, train acc 57.73, train corr acc 20.26, val acc 57.31, val corr acc 19.51\n",
      "Ep 3 \titer 200  \tloss 1.07891, train acc 60.20, train corr acc 23.90, val acc 57.60, val corr acc 20.21\n",
      "Ep 3 \titer 201  \tloss 1.26038, train acc 57.73, train corr acc 25.31, val acc 58.19, val corr acc 21.34\n",
      "Ep 3 \titer 202  \tloss 1.15766, train acc 56.74, train corr acc 23.35, val acc 58.04, val corr acc 21.91\n",
      "Ep 3 \titer 203  \tloss 1.14312, train acc 58.06, train corr acc 22.79, val acc 58.44, val corr acc 23.18\n",
      "Ep 3 \titer 204  \tloss 1.15931, train acc 55.59, train corr acc 23.03, val acc 58.44, val corr acc 24.03\n",
      "Ep 3 \titer 205  \tloss 1.08500, train acc 59.70, train corr acc 23.92, val acc 58.48, val corr acc 24.52\n",
      "Ep 3 \titer 206  \tloss 1.09572, train acc 61.18, train corr acc 28.99, val acc 58.48, val corr acc 25.37\n",
      "Ep 3 \titer 207  \tloss 1.09011, train acc 61.18, train corr acc 26.56, val acc 59.13, val corr acc 26.08\n",
      "Ep 3 \titer 208  \tloss 1.07511, train acc 60.20, train corr acc 22.83, val acc 59.27, val corr acc 24.88\n",
      "Ep 3 \titer 209  \tloss 1.09970, train acc 59.21, train corr acc 25.87, val acc 59.13, val corr acc 23.82\n",
      "Ep 3 \titer 210  \tloss 1.02180, train acc 65.46, train corr acc 33.44, val acc 59.20, val corr acc 22.83\n",
      "Ep 3 \titer 211  \tloss 1.14394, train acc 57.07, train corr acc 23.44, val acc 58.98, val corr acc 22.40\n",
      "Ep 3 \titer 212  \tloss 1.07658, train acc 58.88, train corr acc 26.56, val acc 59.56, val corr acc 23.25\n",
      "Ep 3 \titer 213  \tloss 0.98490, train acc 66.45, train corr acc 28.01, val acc 58.98, val corr acc 22.05\n",
      "Ep 3 \titer 214  \tloss 1.20919, train acc 56.58, train corr acc 21.19, val acc 59.82, val corr acc 24.38\n",
      "Ep 3 \titer 215  \tloss 1.22686, train acc 54.11, train corr acc 24.79, val acc 59.67, val corr acc 25.23\n",
      "Ep 3 \titer 216  \tloss 1.27165, train acc 49.51, train corr acc 21.58, val acc 59.24, val corr acc 26.57\n",
      "Ep 3 \titer 217  \tloss 1.13596, train acc 59.05, train corr acc 28.09, val acc 58.40, val corr acc 27.21\n",
      "Ep 3 \titer 218  \tloss 1.10985, train acc 55.92, train corr acc 34.23, val acc 59.02, val corr acc 30.25\n",
      "Ep 3 \titer 219  \tloss 1.30076, train acc 53.45, train corr acc 33.97, val acc 58.19, val corr acc 29.47\n",
      "Ep 3 \titer 220  \tloss 1.08387, train acc 58.72, train corr acc 30.65, val acc 58.84, val corr acc 30.39\n",
      "Ep 3 \titer 221  \tloss 1.07058, train acc 61.68, train corr acc 35.09, val acc 59.20, val corr acc 29.19\n",
      "Ep 3 \titer 222  \tloss 1.08131, train acc 62.50, train corr acc 34.70, val acc 60.00, val corr acc 27.99\n",
      "Ep 3 \titer 223  \tloss 1.00141, train acc 64.80, train corr acc 28.47, val acc 58.69, val corr acc 25.30\n",
      "Ep 3 \titer 224  \tloss 1.11219, train acc 58.55, train corr acc 24.27, val acc 59.46, val corr acc 25.16\n",
      "Ep 3 \titer 225  \tloss 1.11954, train acc 60.20, train corr acc 29.88, val acc 59.09, val corr acc 24.10\n",
      "Ep 3 \titer 226  \tloss 1.05756, train acc 61.18, train corr acc 30.37, val acc 58.91, val corr acc 23.60\n",
      "Ep 3 \titer 227  \tloss 1.16503, train acc 59.05, train corr acc 26.05, val acc 58.40, val corr acc 22.97\n",
      "Ep 3 \titer 228  \tloss 1.13292, train acc 59.54, train corr acc 30.89, val acc 59.02, val corr acc 24.38\n",
      "Ep 3 \titer 229  \tloss 1.12022, train acc 57.73, train corr acc 18.15, val acc 59.31, val corr acc 25.65\n",
      "Ep 3 \titer 230  \tloss 1.22658, train acc 49.84, train corr acc 24.92, val acc 59.85, val corr acc 27.14\n",
      "Ep 3 \titer 231  \tloss 1.11619, train acc 56.25, train corr acc 28.36, val acc 59.60, val corr acc 27.21\n",
      "Ep 3 \titer 232  \tloss 1.07873, train acc 59.70, train corr acc 29.22, val acc 60.00, val corr acc 28.62\n",
      "Ep 3 \titer 233  \tloss 1.18150, train acc 57.07, train corr acc 27.06, val acc 60.58, val corr acc 29.47\n",
      "Ep 3 \titer 234  \tloss 1.01354, train acc 66.28, train corr acc 33.67, val acc 60.25, val corr acc 28.55\n",
      "Ep 3 \titer 235  \tloss 1.18866, train acc 54.11, train corr acc 33.70, val acc 60.29, val corr acc 28.41\n",
      "Ep 3 \titer 236  \tloss 1.11886, train acc 56.74, train corr acc 27.16, val acc 60.80, val corr acc 28.90\n",
      "Ep 3 \titer 237  \tloss 1.05934, train acc 59.05, train corr acc 20.71, val acc 59.89, val corr acc 27.21\n",
      "Ep 3 \titer 238  \tloss 1.15492, train acc 58.06, train corr acc 28.71, val acc 60.22, val corr acc 27.21\n",
      "Ep 3 \titer 239  \tloss 1.02344, train acc 63.49, train corr acc 35.41, val acc 60.07, val corr acc 26.43\n",
      "Ep 3 \titer 240  \tloss 1.18225, train acc 55.76, train corr acc 20.13, val acc 59.96, val corr acc 25.23\n",
      "Ep 3 \titer 241  \tloss 1.12244, train acc 58.55, train corr acc 25.47, val acc 59.49, val corr acc 25.09\n",
      "Ep 3 \titer 242  \tloss 1.21748, train acc 53.78, train corr acc 24.12, val acc 60.33, val corr acc 25.94\n",
      "Ep 3 \titer 243  \tloss 1.08544, train acc 56.58, train corr acc 22.68, val acc 60.15, val corr acc 26.43\n",
      "Ep 3 \titer 244  \tloss 1.19172, train acc 58.06, train corr acc 29.15, val acc 60.83, val corr acc 28.06\n",
      "Ep 3 \titer 245  \tloss 1.13383, train acc 59.87, train corr acc 28.44, val acc 60.29, val corr acc 27.70\n",
      "Ep 3 \titer 246  \tloss 1.01431, train acc 63.82, train corr acc 29.25, val acc 60.25, val corr acc 28.41\n",
      "Ep 3 \titer 247  \tloss 1.00396, train acc 66.45, train corr acc 34.88, val acc 59.53, val corr acc 27.70\n",
      "Ep 3 \titer 248  \tloss 1.07881, train acc 60.03, train corr acc 27.18, val acc 60.04, val corr acc 27.70\n",
      "Ep 3 \titer 249  \tloss 1.05091, train acc 64.80, train corr acc 36.08, val acc 59.71, val corr acc 26.22\n",
      "Ep 3 \titer 250  \tloss 1.13289, train acc 57.40, train corr acc 31.45, val acc 60.00, val corr acc 26.15\n",
      "Ep 3 \titer 251  \tloss 0.97148, train acc 65.62, train corr acc 32.24, val acc 59.46, val corr acc 24.52\n",
      "Ep 3 \titer 252  \tloss 1.14197, train acc 56.25, train corr acc 21.92, val acc 59.46, val corr acc 24.03\n",
      "\n",
      "Epoch 4 of 8 took 25.776s\n",
      "  training loss:\t\t1.137689\n",
      "  training accuracy:\t\t56.23 %\n",
      "  training corrected acc:\t\t24.03 %\n",
      "  validation loss:\t\t1.086524\n",
      "  validation accuracy:\t\t59.46 %\n",
      "  validation corrected acc:\t\t24.03 % \n",
      "\n",
      "Ep 4 \titer 253  \tloss 1.11688, train acc 57.40, train corr acc 25.09, val acc 58.98, val corr acc 22.97\n",
      "Ep 4 \titer 254  \tloss 1.10199, train acc 58.39, train corr acc 25.93, val acc 59.64, val corr acc 24.31\n",
      "Ep 4 \titer 255  \tloss 1.08770, train acc 58.88, train corr acc 23.57, val acc 59.35, val corr acc 24.24\n",
      "Ep 4 \titer 256  \tloss 1.15803, train acc 56.41, train corr acc 28.12, val acc 59.20, val corr acc 24.88\n",
      "Ep 4 \titer 257  \tloss 1.13705, train acc 57.07, train corr acc 26.11, val acc 59.09, val corr acc 25.16\n",
      "Ep 4 \titer 258  \tloss 1.10685, train acc 60.03, train corr acc 29.47, val acc 59.20, val corr acc 25.80\n",
      "Ep 4 \titer 259  \tloss 1.04936, train acc 62.66, train corr acc 25.93, val acc 59.56, val corr acc 25.72\n",
      "Ep 4 \titer 260  \tloss 1.06042, train acc 60.20, train corr acc 22.94, val acc 59.09, val corr acc 25.09\n",
      "Ep 4 \titer 261  \tloss 1.16920, train acc 54.61, train corr acc 26.81, val acc 59.46, val corr acc 24.59\n",
      "Ep 4 \titer 262  \tloss 1.12131, train acc 57.89, train corr acc 25.40, val acc 59.20, val corr acc 23.60\n",
      "Ep 4 \titer 263  \tloss 1.06998, train acc 62.34, train corr acc 27.99, val acc 59.49, val corr acc 24.59\n",
      "Ep 4 \titer 264  \tloss 1.23699, train acc 59.70, train corr acc 29.06, val acc 58.77, val corr acc 23.32\n",
      "Ep 4 \titer 265  \tloss 1.13830, train acc 59.54, train corr acc 29.94, val acc 59.13, val corr acc 24.10\n",
      "Ep 4 \titer 266  \tloss 1.13303, train acc 57.89, train corr acc 22.79, val acc 59.24, val corr acc 24.38\n",
      "Ep 4 \titer 267  \tloss 1.14822, train acc 54.77, train corr acc 24.20, val acc 59.42, val corr acc 25.23\n",
      "Ep 4 \titer 268  \tloss 1.05933, train acc 58.22, train corr acc 23.26, val acc 59.20, val corr acc 25.65\n",
      "Ep 4 \titer 269  \tloss 1.09110, train acc 61.84, train corr acc 28.99, val acc 59.20, val corr acc 26.57\n",
      "Ep 4 \titer 270  \tloss 1.07091, train acc 62.01, train corr acc 28.20, val acc 59.53, val corr acc 27.21\n",
      "Ep 4 \titer 271  \tloss 1.08277, train acc 60.20, train corr acc 26.09, val acc 59.53, val corr acc 25.65\n",
      "Ep 4 \titer 272  \tloss 1.09049, train acc 58.22, train corr acc 23.97, val acc 59.56, val corr acc 24.81\n",
      "Ep 4 \titer 273  \tloss 1.02773, train acc 64.14, train corr acc 33.76, val acc 58.84, val corr acc 23.11\n",
      "Ep 4 \titer 274  \tloss 1.12380, train acc 58.06, train corr acc 25.31, val acc 59.53, val corr acc 24.10\n",
      "Ep 4 \titer 275  \tloss 1.07336, train acc 59.54, train corr acc 26.56, val acc 59.67, val corr acc 24.52\n",
      "Ep 4 \titer 276  \tloss 0.97491, train acc 66.61, train corr acc 28.72, val acc 59.49, val corr acc 24.03\n",
      "Ep 4 \titer 277  \tloss 1.18809, train acc 56.74, train corr acc 22.09, val acc 59.78, val corr acc 24.73\n",
      "Ep 4 \titer 278  \tloss 1.22186, train acc 55.10, train corr acc 28.21, val acc 59.64, val corr acc 24.88\n",
      "Ep 4 \titer 279  \tloss 1.27769, train acc 48.85, train corr acc 21.04, val acc 60.00, val corr acc 26.78\n",
      "Ep 4 \titer 280  \tloss 1.11816, train acc 58.39, train corr acc 29.32, val acc 60.18, val corr acc 28.55\n",
      "Ep 4 \titer 281  \tloss 1.08291, train acc 60.03, train corr acc 36.90, val acc 59.02, val corr acc 28.62\n",
      "Ep 4 \titer 282  \tloss 1.26064, train acc 53.29, train corr acc 31.43, val acc 59.35, val corr acc 29.47\n",
      "Ep 4 \titer 283  \tloss 1.08123, train acc 58.22, train corr acc 28.48, val acc 60.04, val corr acc 30.25\n",
      "Ep 4 \titer 284  \tloss 1.07077, train acc 61.02, train corr acc 32.30, val acc 59.56, val corr acc 29.26\n",
      "Ep 4 \titer 285  \tloss 1.06168, train acc 63.82, train corr acc 36.91, val acc 58.87, val corr acc 27.49\n",
      "Ep 4 \titer 286  \tloss 0.99704, train acc 66.12, train corr acc 30.51, val acc 58.84, val corr acc 26.15\n",
      "Ep 4 \titer 287  \tloss 1.09907, train acc 59.87, train corr acc 26.54, val acc 59.06, val corr acc 25.94\n",
      "Ep 4 \titer 288  \tloss 1.11990, train acc 60.36, train corr acc 29.88, val acc 59.38, val corr acc 25.44\n",
      "Ep 4 \titer 289  \tloss 1.04754, train acc 61.68, train corr acc 30.67, val acc 58.95, val corr acc 24.59\n",
      "Ep 4 \titer 290  \tloss 1.14896, train acc 59.05, train corr acc 26.35, val acc 58.87, val corr acc 24.45\n",
      "Ep 4 \titer 291  \tloss 1.13006, train acc 59.05, train corr acc 29.97, val acc 58.98, val corr acc 25.23\n",
      "Ep 4 \titer 292  \tloss 1.10952, train acc 59.70, train corr acc 21.97, val acc 59.78, val corr acc 26.93\n",
      "Ep 4 \titer 293  \tloss 1.19027, train acc 52.30, train corr acc 25.86, val acc 59.71, val corr acc 27.21\n",
      "Ep 4 \titer 294  \tloss 1.08205, train acc 57.89, train corr acc 28.96, val acc 59.75, val corr acc 27.49\n",
      "Ep 4 \titer 295  \tloss 1.03671, train acc 62.50, train corr acc 33.13, val acc 60.65, val corr acc 29.61\n",
      "Ep 4 \titer 296  \tloss 1.17991, train acc 57.40, train corr acc 27.06, val acc 60.36, val corr acc 29.40\n",
      "Ep 4 \titer 297  \tloss 0.99573, train acc 66.78, train corr acc 34.33, val acc 60.98, val corr acc 30.39\n",
      "Ep 4 \titer 298  \tloss 1.19431, train acc 56.58, train corr acc 37.33, val acc 60.54, val corr acc 29.19\n",
      "Ep 4 \titer 299  \tloss 1.12912, train acc 58.88, train corr acc 31.63, val acc 60.51, val corr acc 29.19\n",
      "Ep 4 \titer 300  \tloss 1.05318, train acc 59.70, train corr acc 22.01, val acc 60.33, val corr acc 28.55\n",
      "Ep 4 \titer 301  \tloss 1.14985, train acc 56.58, train corr acc 30.00, val acc 60.98, val corr acc 28.69\n",
      "Ep 4 \titer 302  \tloss 0.99304, train acc 64.31, train corr acc 37.38, val acc 60.62, val corr acc 27.49\n",
      "Ep 4 \titer 303  \tloss 1.16355, train acc 57.57, train corr acc 23.70, val acc 59.78, val corr acc 26.01\n",
      "Ep 4 \titer 304  \tloss 1.10707, train acc 58.06, train corr acc 24.53, val acc 60.15, val corr acc 26.36\n",
      "Ep 4 \titer 305  \tloss 1.24049, train acc 54.93, train corr acc 25.59, val acc 60.25, val corr acc 26.50\n",
      "Ep 4 \titer 306  \tloss 1.08100, train acc 58.39, train corr acc 25.88, val acc 59.85, val corr acc 25.80\n",
      "Ep 4 \titer 307  \tloss 1.20916, train acc 58.72, train corr acc 30.72, val acc 59.93, val corr acc 26.15\n",
      "Ep 4 \titer 308  \tloss 1.12208, train acc 59.21, train corr acc 26.65, val acc 60.62, val corr acc 27.92\n",
      "Ep 4 \titer 309  \tloss 0.99141, train acc 61.68, train corr acc 25.17, val acc 60.15, val corr acc 27.49\n",
      "Ep 4 \titer 310  \tloss 0.98888, train acc 66.45, train corr acc 33.45, val acc 60.07, val corr acc 27.14\n",
      "Ep 4 \titer 311  \tloss 1.06707, train acc 59.54, train corr acc 28.16, val acc 60.22, val corr acc 27.63\n",
      "Ep 4 \titer 312  \tloss 1.04178, train acc 65.13, train corr acc 34.49, val acc 60.07, val corr acc 26.57\n",
      "Ep 4 \titer 313  \tloss 1.13952, train acc 57.57, train corr acc 28.93, val acc 60.15, val corr acc 26.36\n",
      "Ep 4 \titer 314  \tloss 0.98068, train acc 65.30, train corr acc 31.91, val acc 60.44, val corr acc 26.86\n",
      "Ep 4 \titer 315  \tloss 1.15435, train acc 55.92, train corr acc 22.22, val acc 60.11, val corr acc 26.08\n",
      "\n",
      "Epoch 5 of 8 took 27.523s\n",
      "  training loss:\t\t1.125696\n",
      "  training accuracy:\t\t56.55 %\n",
      "  training corrected acc:\t\t25.16 %\n",
      "  validation loss:\t\t1.073711\n",
      "  validation accuracy:\t\t60.11 %\n",
      "  validation corrected acc:\t\t26.08 % \n",
      "\n",
      "Ep 5 \titer 316  \tloss 1.12271, train acc 56.91, train corr acc 24.39, val acc 59.60, val corr acc 24.59\n",
      "Ep 5 \titer 317  \tloss 1.09476, train acc 57.40, train corr acc 24.24, val acc 59.17, val corr acc 23.82\n",
      "Ep 5 \titer 318  \tloss 1.07718, train acc 59.87, train corr acc 25.80, val acc 59.56, val corr acc 24.95\n",
      "Ep 5 \titer 319  \tloss 1.13770, train acc 57.40, train corr acc 26.84, val acc 59.60, val corr acc 25.37\n",
      "Ep 5 \titer 320  \tloss 1.14496, train acc 56.25, train corr acc 24.93, val acc 59.20, val corr acc 24.88\n",
      "Ep 5 \titer 321  \tloss 1.11105, train acc 58.39, train corr acc 29.15, val acc 59.93, val corr acc 25.87\n",
      "Ep 5 \titer 322  \tloss 1.05168, train acc 64.31, train corr acc 26.67, val acc 59.82, val corr acc 25.80\n",
      "Ep 5 \titer 323  \tloss 1.05465, train acc 60.53, train corr acc 22.22, val acc 59.85, val corr acc 25.37\n",
      "Ep 5 \titer 324  \tloss 1.16203, train acc 56.74, train corr acc 31.93, val acc 59.67, val corr acc 25.30\n",
      "Ep 5 \titer 325  \tloss 1.10309, train acc 57.73, train corr acc 25.40, val acc 59.60, val corr acc 24.95\n",
      "Ep 5 \titer 326  \tloss 1.06292, train acc 61.68, train corr acc 27.36, val acc 59.53, val corr acc 24.88\n",
      "Ep 5 \titer 327  \tloss 1.22531, train acc 60.69, train corr acc 30.94, val acc 59.42, val corr acc 24.66\n",
      "Ep 5 \titer 328  \tloss 1.14742, train acc 61.02, train corr acc 31.74, val acc 59.49, val corr acc 25.37\n",
      "Ep 5 \titer 329  \tloss 1.13749, train acc 58.39, train corr acc 24.83, val acc 58.91, val corr acc 24.59\n",
      "Ep 5 \titer 330  \tloss 1.12799, train acc 56.25, train corr acc 26.24, val acc 59.20, val corr acc 25.23\n",
      "Ep 5 \titer 331  \tloss 1.04830, train acc 59.05, train corr acc 25.58, val acc 59.64, val corr acc 26.08\n",
      "Ep 5 \titer 332  \tloss 1.08432, train acc 60.36, train corr acc 26.09, val acc 59.13, val corr acc 25.23\n",
      "Ep 5 \titer 333  \tloss 1.08687, train acc 59.54, train corr acc 25.25, val acc 59.71, val corr acc 26.22\n",
      "Ep 5 \titer 334  \tloss 1.04294, train acc 62.34, train corr acc 26.45, val acc 59.42, val corr acc 25.23\n",
      "Ep 5 \titer 335  \tloss 1.09339, train acc 59.05, train corr acc 26.18, val acc 59.42, val corr acc 24.95\n",
      "Ep 5 \titer 336  \tloss 1.01288, train acc 65.62, train corr acc 35.03, val acc 59.13, val corr acc 24.17\n",
      "Ep 5 \titer 337  \tloss 1.11626, train acc 57.57, train corr acc 24.38, val acc 59.24, val corr acc 24.17\n",
      "Ep 5 \titer 338  \tloss 1.06296, train acc 60.69, train corr acc 31.15, val acc 59.42, val corr acc 24.03\n",
      "Ep 5 \titer 339  \tloss 0.98416, train acc 67.93, train corr acc 32.62, val acc 59.60, val corr acc 24.81\n",
      "Ep 5 \titer 340  \tloss 1.17448, train acc 57.07, train corr acc 22.69, val acc 60.00, val corr acc 25.51\n",
      "Ep 5 \titer 341  \tloss 1.17450, train acc 54.77, train corr acc 27.35, val acc 60.07, val corr acc 25.87\n",
      "Ep 5 \titer 342  \tloss 1.27583, train acc 50.66, train corr acc 23.77, val acc 60.04, val corr acc 26.36\n",
      "Ep 5 \titer 343  \tloss 1.11801, train acc 57.89, train corr acc 27.47, val acc 59.67, val corr acc 27.21\n",
      "Ep 5 \titer 344  \tloss 1.08000, train acc 60.20, train corr acc 37.20, val acc 60.15, val corr acc 28.34\n",
      "Ep 5 \titer 345  \tloss 1.24216, train acc 55.26, train corr acc 33.02, val acc 60.15, val corr acc 29.19\n",
      "Ep 5 \titer 346  \tloss 1.06318, train acc 58.22, train corr acc 28.17, val acc 59.93, val corr acc 29.05\n",
      "Ep 5 \titer 347  \tloss 1.05160, train acc 61.02, train corr acc 32.30, val acc 60.51, val corr acc 30.11\n",
      "Ep 5 \titer 348  \tloss 1.04898, train acc 63.82, train corr acc 34.07, val acc 59.60, val corr acc 28.55\n",
      "Ep 5 \titer 349  \tloss 0.98117, train acc 66.78, train corr acc 32.88, val acc 59.78, val corr acc 28.76\n",
      "Ep 5 \titer 350  \tloss 1.09410, train acc 59.38, train corr acc 26.54, val acc 59.64, val corr acc 27.77\n",
      "Ep 5 \titer 351  \tloss 1.11409, train acc 61.02, train corr acc 31.95, val acc 59.56, val corr acc 27.07\n",
      "Ep 5 \titer 352  \tloss 1.03366, train acc 61.84, train corr acc 31.60, val acc 59.49, val corr acc 26.86\n",
      "Ep 5 \titer 353  \tloss 1.12185, train acc 58.72, train corr acc 26.95, val acc 59.31, val corr acc 26.29\n",
      "Ep 5 \titer 354  \tloss 1.12122, train acc 61.84, train corr acc 35.47, val acc 59.82, val corr acc 26.93\n",
      "Ep 5 \titer 355  \tloss 1.09899, train acc 59.21, train corr acc 21.66, val acc 59.89, val corr acc 27.35\n",
      "Ep 5 \titer 356  \tloss 1.18108, train acc 51.48, train corr acc 24.92, val acc 60.04, val corr acc 27.63\n",
      "Ep 5 \titer 357  \tloss 1.07624, train acc 57.24, train corr acc 26.57, val acc 60.36, val corr acc 27.99\n",
      "Ep 5 \titer 358  \tloss 1.01359, train acc 62.99, train corr acc 34.04, val acc 60.91, val corr acc 29.40\n",
      "Ep 5 \titer 359  \tloss 1.16079, train acc 57.40, train corr acc 26.76, val acc 60.22, val corr acc 28.48\n",
      "Ep 5 \titer 360  \tloss 0.99724, train acc 65.95, train corr acc 34.00, val acc 60.51, val corr acc 29.12\n",
      "Ep 5 \titer 361  \tloss 1.18716, train acc 55.92, train corr acc 37.33, val acc 60.91, val corr acc 29.75\n",
      "Ep 5 \titer 362  \tloss 1.10524, train acc 59.38, train corr acc 32.91, val acc 60.83, val corr acc 29.54\n",
      "Ep 5 \titer 363  \tloss 1.06645, train acc 60.03, train corr acc 22.98, val acc 60.83, val corr acc 29.47\n",
      "Ep 5 \titer 364  \tloss 1.14136, train acc 57.24, train corr acc 34.84, val acc 61.27, val corr acc 29.75\n",
      "Ep 5 \titer 365  \tloss 0.99804, train acc 64.64, train corr acc 38.69, val acc 61.05, val corr acc 28.90\n",
      "Ep 5 \titer 366  \tloss 1.16235, train acc 57.24, train corr acc 23.70, val acc 60.76, val corr acc 27.56\n",
      "Ep 5 \titer 367  \tloss 1.11149, train acc 58.88, train corr acc 26.10, val acc 60.83, val corr acc 27.99\n",
      "Ep 5 \titer 368  \tloss 1.20960, train acc 55.59, train corr acc 28.53, val acc 61.23, val corr acc 28.34\n",
      "Ep 5 \titer 369  \tloss 1.05435, train acc 60.53, train corr acc 29.39, val acc 60.18, val corr acc 26.43\n",
      "Ep 5 \titer 370  \tloss 1.19595, train acc 58.72, train corr acc 29.78, val acc 61.09, val corr acc 28.27\n",
      "Ep 5 \titer 371  \tloss 1.13598, train acc 59.05, train corr acc 29.94, val acc 60.36, val corr acc 26.78\n",
      "Ep 5 \titer 372  \tloss 0.99383, train acc 63.16, train corr acc 27.89, val acc 60.80, val corr acc 27.92\n",
      "Ep 5 \titer 373  \tloss 0.97185, train acc 66.12, train corr acc 33.45, val acc 59.71, val corr acc 26.29\n",
      "Ep 5 \titer 374  \tloss 1.06196, train acc 61.51, train corr acc 26.86, val acc 59.96, val corr acc 27.42\n",
      "Ep 5 \titer 375  \tloss 1.02375, train acc 66.61, train corr acc 36.71, val acc 60.00, val corr acc 27.07\n",
      "Ep 5 \titer 376  \tloss 1.14056, train acc 57.73, train corr acc 31.76, val acc 59.82, val corr acc 26.29\n",
      "Ep 5 \titer 377  \tloss 0.98577, train acc 66.28, train corr acc 36.84, val acc 60.11, val corr acc 26.43\n",
      "Ep 5 \titer 378  \tloss 1.13609, train acc 56.58, train corr acc 23.72, val acc 60.18, val corr acc 26.22\n",
      "\n",
      "Epoch 6 of 8 took 21.961s\n",
      "  training loss:\t\t1.128381\n",
      "  training accuracy:\t\t55.93 %\n",
      "  training corrected acc:\t\t25.16 %\n",
      "  validation loss:\t\t1.059388\n",
      "  validation accuracy:\t\t60.18 %\n",
      "  validation corrected acc:\t\t26.22 % \n",
      "\n",
      "Ep 6 \titer 379  \tloss 1.10531, train acc 57.73, train corr acc 26.13, val acc 60.47, val corr acc 26.43\n",
      "Ep 6 \titer 380  \tloss 1.09895, train acc 57.40, train corr acc 25.25, val acc 60.76, val corr acc 26.93\n",
      "Ep 6 \titer 381  \tloss 1.07413, train acc 60.20, train corr acc 26.75, val acc 60.29, val corr acc 26.78\n",
      "Ep 6 \titer 382  \tloss 1.14070, train acc 57.24, train corr acc 26.84, val acc 60.18, val corr acc 26.50\n",
      "Ep 6 \titer 383  \tloss 1.11056, train acc 57.24, train corr acc 26.41, val acc 59.85, val corr acc 25.80\n",
      "Ep 6 \titer 384  \tloss 1.05590, train acc 61.68, train corr acc 33.23, val acc 60.25, val corr acc 26.64\n",
      "Ep 6 \titer 385  \tloss 1.03153, train acc 64.64, train corr acc 27.41, val acc 60.40, val corr acc 26.93\n",
      "Ep 6 \titer 386  \tloss 1.03924, train acc 60.86, train corr acc 23.66, val acc 60.07, val corr acc 26.43\n",
      "Ep 6 \titer 387  \tloss 1.14818, train acc 55.43, train corr acc 30.72, val acc 60.22, val corr acc 26.50\n",
      "Ep 6 \titer 388  \tloss 1.10927, train acc 58.55, train corr acc 26.69, val acc 60.04, val corr acc 26.36\n",
      "Ep 6 \titer 389  \tloss 1.05249, train acc 59.70, train corr acc 25.47, val acc 60.11, val corr acc 26.57\n",
      "Ep 6 \titer 390  \tloss 1.20644, train acc 62.34, train corr acc 35.00, val acc 60.11, val corr acc 26.93\n",
      "Ep 6 \titer 391  \tloss 1.13898, train acc 60.86, train corr acc 32.93, val acc 59.56, val corr acc 26.64\n",
      "Ep 6 \titer 392  \tloss 1.11845, train acc 58.88, train corr acc 26.87, val acc 59.56, val corr acc 26.36\n",
      "Ep 6 \titer 393  \tloss 1.11534, train acc 57.40, train corr acc 30.61, val acc 59.60, val corr acc 26.43\n",
      "Ep 6 \titer 394  \tloss 1.02991, train acc 59.87, train corr acc 28.24, val acc 60.07, val corr acc 27.07\n",
      "Ep 6 \titer 395  \tloss 1.05125, train acc 62.50, train corr acc 30.43, val acc 60.18, val corr acc 27.35\n",
      "Ep 6 \titer 396  \tloss 1.08117, train acc 60.69, train corr acc 27.87, val acc 59.93, val corr acc 26.93\n",
      "Ep 6 \titer 397  \tloss 1.03131, train acc 62.66, train corr acc 30.07, val acc 59.67, val corr acc 25.65\n",
      "Ep 6 \titer 398  \tloss 1.08116, train acc 58.88, train corr acc 26.18, val acc 59.96, val corr acc 25.72\n",
      "Ep 6 \titer 399  \tloss 0.99861, train acc 66.12, train corr acc 35.99, val acc 59.85, val corr acc 25.72\n",
      "Ep 6 \titer 400  \tloss 1.10750, train acc 58.06, train corr acc 25.31, val acc 59.67, val corr acc 25.02\n",
      "Ep 6 \titer 401  \tloss 1.05271, train acc 61.02, train corr acc 32.13, val acc 59.42, val corr acc 24.66\n",
      "Ep 6 \titer 402  \tloss 0.96037, train acc 67.43, train corr acc 32.27, val acc 59.67, val corr acc 24.66\n",
      "Ep 6 \titer 403  \tloss 1.17858, train acc 57.40, train corr acc 23.88, val acc 59.60, val corr acc 24.88\n",
      "Ep 6 \titer 404  \tloss 1.19578, train acc 55.10, train corr acc 27.35, val acc 59.89, val corr acc 25.58\n",
      "Ep 6 \titer 405  \tloss 1.27344, train acc 50.82, train corr acc 23.50, val acc 60.62, val corr acc 27.35\n",
      "Ep 6 \titer 406  \tloss 1.12144, train acc 57.07, train corr acc 27.78, val acc 60.47, val corr acc 27.70\n",
      "Ep 6 \titer 407  \tloss 1.04135, train acc 62.01, train corr acc 38.99, val acc 61.38, val corr acc 29.68\n",
      "Ep 6 \titer 408  \tloss 1.23666, train acc 56.09, train corr acc 33.97, val acc 60.51, val corr acc 29.33\n",
      "Ep 6 \titer 409  \tloss 1.07807, train acc 59.05, train corr acc 29.10, val acc 61.05, val corr acc 30.67\n",
      "Ep 6 \titer 410  \tloss 1.03175, train acc 65.30, train corr acc 36.96, val acc 60.07, val corr acc 29.47\n",
      "Ep 6 \titer 411  \tloss 1.03975, train acc 65.46, train corr acc 37.85, val acc 60.22, val corr acc 29.54\n",
      "Ep 6 \titer 412  \tloss 0.98396, train acc 66.45, train corr acc 33.90, val acc 60.36, val corr acc 28.69\n",
      "Ep 6 \titer 413  \tloss 1.08390, train acc 59.54, train corr acc 29.45, val acc 60.22, val corr acc 28.83\n",
      "Ep 6 \titer 414  \tloss 1.10248, train acc 60.36, train corr acc 32.25, val acc 59.71, val corr acc 27.42\n",
      "Ep 6 \titer 415  \tloss 1.02195, train acc 64.47, train corr acc 37.42, val acc 59.96, val corr acc 27.35\n",
      "Ep 6 \titer 416  \tloss 1.13828, train acc 58.06, train corr acc 28.14, val acc 59.56, val corr acc 26.64\n",
      "Ep 6 \titer 417  \tloss 1.09991, train acc 62.83, train corr acc 37.31, val acc 59.82, val corr acc 26.93\n",
      "Ep 6 \titer 418  \tloss 1.09596, train acc 59.05, train corr acc 23.25, val acc 59.64, val corr acc 26.71\n",
      "Ep 6 \titer 419  \tloss 1.16446, train acc 53.12, train corr acc 26.17, val acc 60.58, val corr acc 28.55\n",
      "Ep 6 \titer 420  \tloss 1.05301, train acc 61.51, train corr acc 34.63, val acc 60.65, val corr acc 29.05\n",
      "Ep 6 \titer 421  \tloss 0.99203, train acc 64.47, train corr acc 37.05, val acc 60.94, val corr acc 29.61\n",
      "Ep 6 \titer 422  \tloss 1.13563, train acc 57.73, train corr acc 28.24, val acc 60.73, val corr acc 29.82\n",
      "Ep 6 \titer 423  \tloss 0.97633, train acc 66.45, train corr acc 33.67, val acc 60.54, val corr acc 29.68\n",
      "Ep 6 \titer 424  \tloss 1.18906, train acc 55.76, train corr acc 36.49, val acc 61.20, val corr acc 31.10\n",
      "Ep 6 \titer 425  \tloss 1.12439, train acc 59.87, train corr acc 34.19, val acc 61.13, val corr acc 31.02\n",
      "Ep 6 \titer 426  \tloss 1.04050, train acc 61.68, train corr acc 26.21, val acc 61.63, val corr acc 31.52\n",
      "Ep 6 \titer 427  \tloss 1.14089, train acc 54.93, train corr acc 32.90, val acc 61.34, val corr acc 30.25\n",
      "Ep 6 \titer 428  \tloss 0.98537, train acc 63.98, train corr acc 39.34, val acc 61.02, val corr acc 28.83\n",
      "Ep 6 \titer 429  \tloss 1.13258, train acc 58.88, train corr acc 26.62, val acc 61.38, val corr acc 29.47\n",
      "Ep 6 \titer 430  \tloss 1.09135, train acc 60.69, train corr acc 29.56, val acc 60.40, val corr acc 27.35\n",
      "Ep 6 \titer 431  \tloss 1.19495, train acc 56.25, train corr acc 28.82, val acc 61.23, val corr acc 28.55\n",
      "Ep 6 \titer 432  \tloss 1.06680, train acc 58.72, train corr acc 28.12, val acc 60.76, val corr acc 27.77\n",
      "Ep 6 \titer 433  \tloss 1.18223, train acc 60.20, train corr acc 32.29, val acc 60.65, val corr acc 27.63\n",
      "Ep 6 \titer 434  \tloss 1.13404, train acc 58.72, train corr acc 28.14, val acc 60.91, val corr acc 27.28\n",
      "Ep 6 \titer 435  \tloss 0.98896, train acc 62.83, train corr acc 27.89, val acc 60.91, val corr acc 27.56\n",
      "Ep 6 \titer 436  \tloss 0.98718, train acc 65.79, train corr acc 33.81, val acc 60.98, val corr acc 28.34\n",
      "Ep 6 \titer 437  \tloss 1.05299, train acc 62.50, train corr acc 29.77, val acc 60.33, val corr acc 27.42\n",
      "Ep 6 \titer 438  \tloss 1.01350, train acc 66.12, train corr acc 37.03, val acc 60.29, val corr acc 26.71\n",
      "Ep 6 \titer 439  \tloss 1.15330, train acc 55.92, train corr acc 30.50, val acc 60.94, val corr acc 27.28\n",
      "Ep 6 \titer 440  \tloss 0.96703, train acc 66.78, train corr acc 34.87, val acc 61.20, val corr acc 27.07\n",
      "Ep 6 \titer 441  \tloss 1.12714, train acc 58.39, train corr acc 26.43, val acc 61.23, val corr acc 27.21\n",
      "\n",
      "Epoch 7 of 8 took 26.533s\n",
      "  training loss:\t\t1.126605\n",
      "  training accuracy:\t\t56.41 %\n",
      "  training corrected acc:\t\t26.08 %\n",
      "  validation loss:\t\t1.053000\n",
      "  validation accuracy:\t\t61.23 %\n",
      "  validation corrected acc:\t\t27.21 % \n",
      "\n",
      "Ep 7 \titer 442  \tloss 1.09901, train acc 58.39, train corr acc 27.53, val acc 61.23, val corr acc 26.50\n",
      "Ep 7 \titer 443  \tloss 1.10660, train acc 56.74, train corr acc 23.91, val acc 61.56, val corr acc 26.86\n",
      "Ep 7 \titer 444  \tloss 1.08131, train acc 59.87, train corr acc 25.48, val acc 61.31, val corr acc 26.93\n",
      "Ep 7 \titer 445  \tloss 1.15212, train acc 55.92, train corr acc 27.80, val acc 61.56, val corr acc 27.28\n",
      "Ep 7 \titer 446  \tloss 1.11236, train acc 57.73, train corr acc 28.49, val acc 60.83, val corr acc 26.71\n",
      "Ep 7 \titer 447  \tloss 1.01198, train acc 65.13, train corr acc 38.56, val acc 60.36, val corr acc 26.50\n",
      "Ep 7 \titer 448  \tloss 1.02373, train acc 63.82, train corr acc 25.56, val acc 61.20, val corr acc 27.49\n",
      "Ep 7 \titer 449  \tloss 1.02568, train acc 62.50, train corr acc 25.81, val acc 60.58, val corr acc 27.00\n",
      "Ep 7 \titer 450  \tloss 1.12002, train acc 57.40, train corr acc 32.23, val acc 60.65, val corr acc 27.21\n",
      "Ep 7 \titer 451  \tloss 1.10963, train acc 58.39, train corr acc 27.33, val acc 60.11, val corr acc 26.50\n",
      "Ep 7 \titer 452  \tloss 1.05559, train acc 60.69, train corr acc 28.30, val acc 60.47, val corr acc 27.42\n",
      "Ep 7 \titer 453  \tloss 1.20068, train acc 61.35, train corr acc 33.44, val acc 59.93, val corr acc 26.29\n",
      "Ep 7 \titer 454  \tloss 1.12919, train acc 61.18, train corr acc 32.63, val acc 60.98, val corr acc 28.34\n",
      "Ep 7 \titer 455  \tloss 1.10383, train acc 60.03, train corr acc 30.27, val acc 60.04, val corr acc 27.07\n",
      "Ep 7 \titer 456  \tloss 1.09515, train acc 58.39, train corr acc 30.61, val acc 60.91, val corr acc 28.34\n",
      "Ep 7 \titer 457  \tloss 1.03424, train acc 60.03, train corr acc 27.91, val acc 60.51, val corr acc 27.63\n",
      "Ep 7 \titer 458  \tloss 1.07160, train acc 61.68, train corr acc 30.43, val acc 60.40, val corr acc 27.84\n",
      "Ep 7 \titer 459  \tloss 1.07045, train acc 59.38, train corr acc 26.89, val acc 60.25, val corr acc 27.07\n",
      "Ep 7 \titer 460  \tloss 1.02939, train acc 63.32, train corr acc 28.99, val acc 60.11, val corr acc 26.71\n",
      "Ep 7 \titer 461  \tloss 1.07587, train acc 59.05, train corr acc 26.81, val acc 59.71, val corr acc 25.94\n",
      "Ep 7 \titer 462  \tloss 0.98815, train acc 66.12, train corr acc 37.26, val acc 60.62, val corr acc 27.14\n",
      "Ep 7 \titer 463  \tloss 1.08024, train acc 60.20, train corr acc 29.38, val acc 60.15, val corr acc 26.01\n",
      "Ep 7 \titer 464  \tloss 1.07084, train acc 61.84, train corr acc 33.44, val acc 60.51, val corr acc 26.50\n",
      "Ep 7 \titer 465  \tloss 0.96775, train acc 66.94, train corr acc 30.50, val acc 60.40, val corr acc 26.29\n",
      "Ep 7 \titer 466  \tloss 1.14613, train acc 59.87, train corr acc 28.36, val acc 60.40, val corr acc 26.22\n",
      "Ep 7 \titer 467  \tloss 1.17853, train acc 56.25, train corr acc 29.91, val acc 60.69, val corr acc 27.14\n",
      "Ep 7 \titer 468  \tloss 1.26488, train acc 50.99, train corr acc 23.77, val acc 60.98, val corr acc 28.27\n",
      "Ep 7 \titer 469  \tloss 1.10253, train acc 58.39, train corr acc 30.25, val acc 60.87, val corr acc 28.69\n",
      "Ep 7 \titer 470  \tloss 1.02472, train acc 63.98, train corr acc 41.67, val acc 60.87, val corr acc 29.12\n",
      "Ep 7 \titer 471  \tloss 1.22647, train acc 56.74, train corr acc 34.29, val acc 60.73, val corr acc 29.26\n",
      "Ep 7 \titer 472  \tloss 1.07553, train acc 59.05, train corr acc 30.96, val acc 60.83, val corr acc 30.32\n",
      "Ep 7 \titer 473  \tloss 1.01737, train acc 63.49, train corr acc 34.78, val acc 60.65, val corr acc 29.82\n",
      "Ep 7 \titer 474  \tloss 1.03964, train acc 64.47, train corr acc 33.44, val acc 60.69, val corr acc 29.61\n",
      "Ep 7 \titer 475  \tloss 0.96545, train acc 66.94, train corr acc 33.56, val acc 60.51, val corr acc 29.33\n",
      "Ep 7 \titer 476  \tloss 1.08450, train acc 58.88, train corr acc 29.45, val acc 60.33, val corr acc 28.76\n",
      "Ep 7 \titer 477  \tloss 1.12229, train acc 59.87, train corr acc 31.36, val acc 60.36, val corr acc 28.48\n",
      "Ep 7 \titer 478  \tloss 1.00527, train acc 63.49, train corr acc 37.42, val acc 60.25, val corr acc 27.63\n",
      "Ep 7 \titer 479  \tloss 1.13955, train acc 57.89, train corr acc 29.34, val acc 59.64, val corr acc 26.86\n",
      "Ep 7 \titer 480  \tloss 1.08679, train acc 62.17, train corr acc 36.39, val acc 60.11, val corr acc 27.21\n",
      "Ep 7 \titer 481  \tloss 1.07630, train acc 60.36, train corr acc 23.25, val acc 59.85, val corr acc 27.07\n",
      "Ep 7 \titer 482  \tloss 1.16085, train acc 55.10, train corr acc 27.10, val acc 60.44, val corr acc 28.13\n",
      "Ep 7 \titer 483  \tloss 1.02797, train acc 61.18, train corr acc 33.43, val acc 60.65, val corr acc 28.41\n",
      "Ep 7 \titer 484  \tloss 0.99860, train acc 63.32, train corr acc 33.73, val acc 60.73, val corr acc 29.47\n",
      "Ep 7 \titer 485  \tloss 1.12339, train acc 58.88, train corr acc 29.41, val acc 60.98, val corr acc 31.02\n",
      "Ep 7 \titer 486  \tloss 0.97143, train acc 66.78, train corr acc 35.67, val acc 61.02, val corr acc 31.52\n",
      "Ep 7 \titer 487  \tloss 1.16823, train acc 56.91, train corr acc 37.05, val acc 61.09, val corr acc 31.87\n",
      "Ep 7 \titer 488  \tloss 1.10568, train acc 60.36, train corr acc 34.50, val acc 61.02, val corr acc 31.59\n",
      "Ep 7 \titer 489  \tloss 1.07145, train acc 62.83, train corr acc 28.48, val acc 60.98, val corr acc 31.45\n",
      "Ep 7 \titer 490  \tloss 1.12745, train acc 55.26, train corr acc 34.19, val acc 61.38, val corr acc 31.52\n",
      "Ep 7 \titer 491  \tloss 0.97657, train acc 65.46, train corr acc 43.61, val acc 61.45, val corr acc 30.60\n",
      "Ep 7 \titer 492  \tloss 1.11074, train acc 60.20, train corr acc 29.55, val acc 61.74, val corr acc 30.81\n",
      "Ep 7 \titer 493  \tloss 1.09119, train acc 58.55, train corr acc 27.36, val acc 61.34, val corr acc 29.40\n",
      "Ep 7 \titer 494  \tloss 1.19439, train acc 57.40, train corr acc 31.76, val acc 60.98, val corr acc 28.55\n",
      "Ep 7 \titer 495  \tloss 1.05551, train acc 59.87, train corr acc 30.35, val acc 60.36, val corr acc 27.92\n",
      "Ep 7 \titer 496  \tloss 1.20259, train acc 59.54, train corr acc 31.97, val acc 60.98, val corr acc 28.34\n",
      "Ep 7 \titer 497  \tloss 1.12486, train acc 58.39, train corr acc 28.44, val acc 60.65, val corr acc 28.27\n",
      "Ep 7 \titer 498  \tloss 0.97977, train acc 63.82, train corr acc 30.27, val acc 60.65, val corr acc 28.20\n",
      "Ep 7 \titer 499  \tloss 0.97170, train acc 65.46, train corr acc 33.10, val acc 60.44, val corr acc 28.20\n",
      "Ep 7 \titer 500  \tloss 1.05009, train acc 61.84, train corr acc 26.86, val acc 60.65, val corr acc 28.76\n",
      "Ep 7 \titer 501  \tloss 1.02629, train acc 64.31, train corr acc 35.44, val acc 60.44, val corr acc 28.41\n",
      "Ep 7 \titer 502  \tloss 1.14545, train acc 57.73, train corr acc 33.02, val acc 60.80, val corr acc 28.20\n",
      "Ep 7 \titer 503  \tloss 0.96359, train acc 66.61, train corr acc 37.50, val acc 60.25, val corr acc 27.35\n",
      "Ep 7 \titer 504  \tloss 1.12150, train acc 56.91, train corr acc 25.83, val acc 60.73, val corr acc 27.70\n",
      "\n",
      "Epoch 8 of 8 took 22.468s\n",
      "  training loss:\t\t1.116258\n",
      "  training accuracy:\t\t57.10 %\n",
      "  training corrected acc:\t\t27.99 %\n",
      "  validation loss:\t\t1.054695\n",
      "  validation accuracy:\t\t60.73 %\n",
      "  validation corrected acc:\t\t27.70 % \n",
      "\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Training!\n",
    "train_losses, train_accs, train_corrected_accs, val_losses, val_accs, val_corrected_accs = model.train(train_data, val_data, train_loss_acc, compute_loss_acc,\\\n",
    "                                                              compute_pred, num_epochs=num_epochs, batchsize=batchsize, record_per_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Final results:\n",
      "  test loss:\t\t\t1.068040\n",
      "  test raw accuracy:\t\t61.09 %\n",
      "  test corrected accuracy:\t27.75 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_raw_acc, test_corrected_acc, pred_test = model.check_accuracy(test_data, compute_loss_acc, dataset_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all, mask_all, _ = all_data\n",
    "ast_embeddings = generate_hidden_reps(X_all, mask_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2314, 256)\n",
      "(2314, 19, 6)\n"
     ]
    }
   ],
   "source": [
    "print ast_embeddings.shape\n",
    "print X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09564256  0.12010601 -0.14471232  0.03233207  0.0092966  -0.30389927\n",
      "  -0.00350801 -0.28258349 -0.56213086  0.1975325 ]\n",
      " [-0.00952602  0.09160703 -0.12065622  0.12200775  0.00967831 -0.24530033\n",
      "   0.05101336 -0.25496152 -0.07230631  0.43806564]\n",
      " [-0.25903327  0.13675017 -0.3137715   0.23230389  0.00969295 -0.48429453\n",
      "   0.00064772 -0.13579737 -0.37562295  0.036104  ]\n",
      " [-0.24501731  0.06020714 -0.146662    0.20097479  0.00366048 -0.40478106\n",
      "   0.004316   -0.14788355 -0.2099546   0.04359592]\n",
      " [-0.09496466  0.08787513 -0.11709445  0.04719868  0.00596931 -0.33923934\n",
      "   0.00067602 -0.29179712 -0.52984889  0.29522163]\n",
      " [-0.02865978  0.05928655 -0.06830794  0.149172    0.00331502 -0.18569762\n",
      "   0.00573558 -0.28611373 -0.28784729  0.49670875]\n",
      " [-0.07915447  0.10210764 -0.19332581  0.27701947  0.00764762 -0.3643594\n",
      "  -0.02953888 -0.34072351 -0.18025459  0.0696902 ]\n",
      " [-0.08949476  0.1555579  -0.23392714  0.30810338  0.01202019 -0.35497929\n",
      "  -0.04341698 -0.34705117 -0.3045248   0.04484843]\n",
      " [-0.21851734  0.1401283  -0.19415573  0.19733599  0.00748387 -0.36663486\n",
      "   0.01113278 -0.24766807 -0.43570697  0.24906903]\n",
      " [-0.10492128  0.13199321 -0.15530877  0.05561707  0.00865671 -0.280113\n",
      "  -0.00407859 -0.28287768 -0.52380716  0.16488335]]\n"
     ]
    }
   ],
   "source": [
    "print ast_embeddings[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.save_ast_embeddings(ast_embeddings, HOC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ast_embeddings_ast_row_to_ast_id_map = pickle.load('map_row_index_to_ast_id_2.pickle')\n",
    "traj_mat_ast_row_to_ast_id_map = pickle.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa8AAAE+CAYAAAA6UZ/nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfUcQr0AgEAj0Hf8fvDmxvh1tjxsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d6e1d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss and the accuracies for both training and validation sets for each epoch\n",
    "num_train = train_data[0].shape[0]\n",
    "visualize.plot_loss_acc('hoc' + str(HOC_NUM) + '_train', train_losses, train_corrected_accs, val_corrected_accs, learning_rate, reg_strength, num_epochs, num_train, xlabel='iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Experiment: \n",
    "Is it possible to train a single model to create embeddings for programs across all Hocs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "(99950, 19, 8)\n"
     ]
    }
   ],
   "source": [
    "# prepare data for all HOCs\n",
    "X_all_hocs_mat, mask_all_hocs_mat, y_all_hocs_mat, split_indices = utils.load_dataset_predict_block_all_hocs()\n",
    "# Shuffle\n",
    "X_all_hocs_mat, mask_all_hocs_mat, y_all_hocs_mat = shuffle(X_all_hocs_mat, mask_all_hocs_mat, y_all_hocs_mat, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99950, 19, 8)\n",
      "(99950, 19)\n",
      "(99950, 19)\n"
     ]
    }
   ],
   "source": [
    "print X_all_hocs_mat.shape\n",
    "print mask_all_hocs_mat.shape\n",
    "print y_all_hocs_mat.shape\n",
    "num_samples_total, num_timesteps, num_blocks = X_all_hocs_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trying out sklearn kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.cross_validation.KFold(n=99950, n_folds=6, shuffle=False, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(num_samples_total, n_folds=6)\n",
    "print(kf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 128 # size of hidden layer of neurons\n",
    "learning_rate = 1e-2\n",
    "lr_decay = 0.995\n",
    "reg_strength = 2e-2\n",
    "grad_clip = 10\n",
    "batchsize = 8\n",
    "num_epochs = 1\n",
    "dropout_p = 0.5\n",
    "num_lstm_layers = 1\n",
    "theano.config.optimizer='fast_compile'\n",
    "theano.config.exception_verbosity='high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Compiling done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa1010/anaconda/lib/python2.7/site-packages/theano/scan_module/scan.py:1019: Warning: In the strict mode, all neccessary shared variables must be passed as a part of non_sequences\n",
      "  'must be passed as a part of non_sequences', Warning)\n"
     ]
    }
   ],
   "source": [
    "train_loss_acc, compute_loss_acc, probs, generate_hidden_reps, compute_pred = model.create_model(num_timesteps, num_blocks, hidden_size, learning_rate, \\\n",
    "                                                             grad_clip, dropout_p, num_lstm_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([16659, 16660, 16661, ..., 99947, 99948, 99949]), 'TEST:', array([    0,     1,     2, ..., 16656, 16657, 16658]))\n",
      "Starting training :)...\n",
      "Total training iterations: 10411\n",
      "Ep 0 \titer 1  \tloss 1.98693, train acc 36.84, train corr acc 2.04, val acc 45.52, val corr acc 6.10\n",
      "Ep 0 \titer 2  \tloss 1.99642, train acc 42.76, train corr acc 8.51, val acc 46.69, val corr acc 8.78\n",
      "Ep 0 \titer 3  \tloss 1.69416, train acc 46.05, train corr acc 7.87, val acc 47.24, val corr acc 11.46\n",
      "Ep 0 \titer 4  \tloss 1.73230, train acc 42.76, train corr acc 7.45, val acc 39.99, val corr acc 12.58\n",
      "Ep 0 \titer 5  \tloss 1.75391, train acc 37.50, train corr acc 12.84, val acc 23.49, val corr acc 9.00\n",
      "Ep 0 \titer 6  \tloss 2.41399, train acc 7.89, train corr acc 2.97, val acc 46.98, val corr acc 13.61\n",
      "Ep 0 \titer 7  \tloss 1.59824, train acc 48.68, train corr acc 12.36, val acc 47.56, val corr acc 10.17\n",
      "Ep 0 \titer 8  \tloss 1.59321, train acc 44.74, train corr acc 11.58, val acc 47.48, val corr acc 9.46\n",
      "Ep 0 \titer 9  \tloss 1.59193, train acc 46.71, train corr acc 10.00, val acc 47.51, val corr acc 9.37\n",
      "Ep 0 \titer 10  \tloss 1.48139, train acc 53.95, train corr acc 14.63, val acc 47.68, val corr acc 9.61\n",
      "Ep 0 \titer 11  \tloss 1.41621, train acc 54.61, train corr acc 11.54, val acc 47.59, val corr acc 9.44\n",
      "Ep 0 \titer 12  \tloss 1.55231, train acc 46.05, train corr acc 9.89, val acc 47.76, val corr acc 9.75\n",
      "Ep 0 \titer 13  \tloss 1.65652, train acc 45.39, train corr acc 13.54, val acc 48.23, val corr acc 10.65\n",
      "Ep 0 \titer 14  \tloss 1.90469, train acc 38.82, train corr acc 8.82, val acc 49.23, val corr acc 12.86\n",
      "Ep 0 \titer 15  \tloss 1.53786, train acc 48.68, train corr acc 9.30, val acc 49.87, val corr acc 16.37\n",
      "Ep 0 \titer 16  \tloss 1.53012, train acc 50.66, train corr acc 13.79, val acc 49.63, val corr acc 18.27\n",
      "Ep 0 \titer 17  \tloss 1.46528, train acc 52.63, train corr acc 16.67, val acc 49.93, val corr acc 18.14\n",
      "Ep 0 \titer 18  \tloss 1.63135, train acc 47.37, train corr acc 17.53, val acc 50.13, val corr acc 17.45\n",
      "Ep 0 \titer 19  \tloss 1.42393, train acc 61.18, train corr acc 22.67, val acc 50.06, val corr acc 15.04\n",
      "Ep 0 \titer 20  \tloss 1.49580, train acc 55.92, train corr acc 20.24, val acc 49.43, val corr acc 13.23\n",
      "Ep 0 \titer 21  \tloss 1.62463, train acc 48.68, train corr acc 13.48, val acc 48.92, val corr acc 12.03\n",
      "Ep 0 \titer 22  \tloss 1.63455, train acc 50.66, train corr acc 12.94, val acc 48.53, val corr acc 11.22\n",
      "Ep 0 \titer 23  \tloss 1.30560, train acc 57.89, train corr acc 14.67, val acc 48.32, val corr acc 10.81\n",
      "Ep 0 \titer 24  \tloss 1.81739, train acc 38.16, train corr acc 5.05, val acc 48.35, val corr acc 10.85\n",
      "Ep 0 \titer 25  \tloss 1.72153, train acc 42.76, train corr acc 10.31, val acc 48.54, val corr acc 11.24\n",
      "Ep 0 \titer 26  \tloss 1.59595, train acc 45.39, train corr acc 9.78, val acc 48.94, val corr acc 12.07\n",
      "Ep 0 \titer 27  \tloss 1.54607, train acc 48.03, train corr acc 13.19, val acc 49.53, val corr acc 13.42\n",
      "Ep 0 \titer 28  \tloss 1.69559, train acc 36.84, train corr acc 8.57, val acc 50.11, val corr acc 15.42\n",
      "Ep 0 \titer 29  \tloss 1.45107, train acc 54.61, train corr acc 15.85, val acc 50.34, val corr acc 17.64\n",
      "Ep 0 \titer 30  \tloss 1.49182, train acc 55.92, train corr acc 16.46, val acc 50.15, val corr acc 18.74\n",
      "Ep 0 \titer 31  \tloss 1.44367, train acc 62.50, train corr acc 28.00, val acc 50.30, val corr acc 18.49\n",
      "Ep 0 \titer 32  \tloss 1.48634, train acc 48.03, train corr acc 13.19, val acc 50.37, val corr acc 18.10\n",
      "Ep 0 \titer 33  \tloss 1.40838, train acc 59.21, train corr acc 26.19, val acc 50.41, val corr acc 17.70\n",
      "Ep 0 \titer 34  \tloss 1.41937, train acc 52.63, train corr acc 18.18, val acc 50.44, val corr acc 17.69\n",
      "Ep 0 \titer 35  \tloss 1.37317, train acc 55.92, train corr acc 22.37, val acc 50.48, val corr acc 17.07\n",
      "Ep 0 \titer 36  \tloss 1.64005, train acc 48.03, train corr acc 12.22, val acc 50.42, val corr acc 16.51\n",
      "Ep 0 \titer 37  \tloss 1.59238, train acc 48.03, train corr acc 11.24, val acc 50.30, val corr acc 15.89\n",
      "Ep 0 \titer 38  \tloss 1.28707, train acc 60.53, train corr acc 23.08, val acc 50.32, val corr acc 15.95\n",
      "Ep 0 \titer 39  \tloss 1.51940, train acc 46.71, train corr acc 14.29, val acc 50.33, val corr acc 15.96\n",
      "Ep 0 \titer 40  \tloss 1.53806, train acc 51.97, train corr acc 18.89, val acc 50.49, val corr acc 16.84\n",
      "Ep 0 \titer 41  \tloss 1.48078, train acc 49.34, train corr acc 21.33, val acc 50.50, val corr acc 17.17\n",
      "Ep 0 \titer 42  \tloss 1.43452, train acc 55.26, train corr acc 18.99, val acc 50.50, val corr acc 16.69\n",
      "Ep 0 \titer 43  \tloss 1.56034, train acc 40.13, train corr acc 13.33, val acc 50.46, val corr acc 16.60\n",
      "Ep 0 \titer 44  \tloss 1.63095, train acc 44.74, train corr acc 15.15, val acc 50.58, val corr acc 17.63\n",
      "Ep 0 \titer 45  \tloss 1.43955, train acc 48.68, train corr acc 14.29, val acc 50.32, val corr acc 19.11\n",
      "Ep 0 \titer 46  \tloss 1.32630, train acc 60.53, train corr acc 25.00, val acc 49.87, val corr acc 20.35\n",
      "Ep 0 \titer 47  \tloss 1.50779, train acc 48.68, train corr acc 19.59, val acc 48.66, val corr acc 22.07\n",
      "Ep 0 \titer 48  \tloss 1.51268, train acc 47.37, train corr acc 19.39, val acc 46.68, val corr acc 23.90\n",
      "Ep 0 \titer 49  \tloss 1.59891, train acc 46.05, train corr acc 19.61, val acc 41.65, val corr acc 27.00\n",
      "Ep 0 \titer 50  \tloss 1.44704, train acc 46.71, train corr acc 29.17, val acc 42.98, val corr acc 26.31\n",
      "Ep 0 \titer 51  \tloss 1.51709, train acc 50.00, train corr acc 25.00, val acc 47.67, val corr acc 23.31\n",
      "Ep 0 \titer 52  \tloss 1.52288, train acc 50.66, train corr acc 22.68, val acc 50.05, val corr acc 20.36\n",
      "Ep 0 \titer 53  \tloss 1.47154, train acc 51.32, train corr acc 19.57, val acc 50.55, val corr acc 18.99\n",
      "Ep 0 \titer 54  \tloss 1.40498, train acc 57.89, train corr acc 20.99, val acc 50.65, val corr acc 17.46\n",
      "Ep 0 \titer 55  \tloss 1.47093, train acc 50.00, train corr acc 17.50, val acc 50.40, val corr acc 16.05\n",
      "Ep 0 \titer 56  \tloss 1.63445, train acc 48.68, train corr acc 17.02, val acc 50.32, val corr acc 15.74\n",
      "Ep 0 \titer 57  \tloss 1.48865, train acc 46.71, train corr acc 15.62, val acc 50.45, val corr acc 16.21\n",
      "Ep 0 \titer 58  \tloss 1.39359, train acc 54.61, train corr acc 19.05, val acc 50.64, val corr acc 16.93\n",
      "Ep 0 \titer 59  \tloss 1.61987, train acc 42.11, train corr acc 14.12, val acc 50.77, val corr acc 17.78\n",
      "Ep 0 \titer 60  \tloss 1.34532, train acc 54.61, train corr acc 24.18, val acc 50.60, val corr acc 19.34\n",
      "Ep 0 \titer 61  \tloss 1.51312, train acc 51.97, train corr acc 20.69, val acc 50.45, val corr acc 19.98\n",
      "Ep 0 \titer 62  \tloss 1.41716, train acc 53.29, train corr acc 21.13, val acc 50.77, val corr acc 19.13\n",
      "Ep 0 \titer 63  \tloss 1.53596, train acc 49.34, train corr acc 17.20, val acc 50.91, val corr acc 18.45\n",
      "Ep 0 \titer 64  \tloss 1.60680, train acc 43.42, train corr acc 11.34, val acc 50.91, val corr acc 17.96\n",
      "Ep 0 \titer 65  \tloss 1.68753, train acc 40.79, train corr acc 14.77, val acc 50.93, val corr acc 17.45\n",
      "Ep 0 \titer 66  \tloss 1.57602, train acc 40.79, train corr acc 11.76, val acc 50.97, val corr acc 17.40\n",
      "Ep 0 \titer 67  \tloss 1.39018, train acc 53.95, train corr acc 21.35, val acc 51.08, val corr acc 17.62\n",
      "Ep 0 \titer 68  \tloss 1.61654, train acc 46.05, train corr acc 17.17, val acc 51.26, val corr acc 18.31\n",
      "Ep 0 \titer 69  \tloss 1.43086, train acc 50.00, train corr acc 17.39, val acc 51.34, val corr acc 19.00\n",
      "Ep 0 \titer 70  \tloss 1.77565, train acc 28.29, train corr acc 10.38, val acc 51.48, val corr acc 20.12\n",
      "Ep 0 \titer 71  \tloss 1.52012, train acc 47.37, train corr acc 20.79, val acc 51.11, val corr acc 22.02\n",
      "Ep 0 \titer 72  \tloss 1.41242, train acc 63.16, train corr acc 29.49, val acc 51.37, val corr acc 21.70\n",
      "Ep 0 \titer 73  \tloss 1.49355, train acc 50.66, train corr acc 25.61, val acc 51.65, val corr acc 20.53\n",
      "Ep 0 \titer 74  \tloss 1.36523, train acc 62.50, train corr acc 33.33, val acc 51.59, val corr acc 18.58\n",
      "Ep 0 \titer 75  \tloss 1.60718, train acc 37.50, train corr acc 26.76, val acc 50.96, val corr acc 16.18\n",
      "Ep 0 \titer 76  \tloss 1.48149, train acc 50.00, train corr acc 19.15, val acc 50.35, val corr acc 14.72\n",
      "Ep 0 \titer 77  \tloss 1.44599, train acc 49.34, train corr acc 15.38, val acc 50.09, val corr acc 14.18\n",
      "Ep 0 \titer 78  \tloss 1.49703, train acc 51.32, train corr acc 11.90, val acc 49.91, val corr acc 13.81\n",
      "Ep 0 \titer 79  \tloss 1.51896, train acc 48.68, train corr acc 12.36, val acc 49.88, val corr acc 13.73\n",
      "Ep 0 \titer 80  \tloss 1.26906, train acc 59.87, train corr acc 15.28, val acc 49.86, val corr acc 13.68\n",
      "Ep 0 \titer 81  \tloss 1.60001, train acc 44.74, train corr acc 11.58, val acc 50.00, val corr acc 13.95\n",
      "Ep 0 \titer 82  \tloss 1.58017, train acc 42.11, train corr acc 10.20, val acc 50.34, val corr acc 14.64\n",
      "Ep 0 \titer 83  \tloss 1.45776, train acc 53.29, train corr acc 14.46, val acc 50.69, val corr acc 15.44\n",
      "Ep 0 \titer 84  \tloss 1.55738, train acc 45.39, train corr acc 11.70, val acc 51.09, val corr acc 16.43\n",
      "Ep 0 \titer 85  \tloss 1.58137, train acc 47.37, train corr acc 14.89, val acc 51.37, val corr acc 17.23\n",
      "Ep 0 \titer 86  \tloss 1.42375, train acc 54.61, train corr acc 19.77, val acc 51.58, val corr acc 17.74\n",
      "Ep 0 \titer 87  \tloss 1.50565, train acc 46.05, train corr acc 13.68, val acc 51.75, val corr acc 18.20\n",
      "Ep 0 \titer 88  \tloss 1.47722, train acc 48.68, train corr acc 16.13, val acc 51.80, val corr acc 18.44\n",
      "Ep 0 \titer 89  \tloss 1.53575, train acc 49.34, train corr acc 19.79, val acc 51.92, val corr acc 18.77\n",
      "Ep 0 \titer 90  \tloss 1.38663, train acc 53.95, train corr acc 14.63, val acc 51.91, val corr acc 18.75\n",
      "Ep 0 \titer 91  \tloss 1.54367, train acc 40.13, train corr acc 14.89, val acc 51.98, val corr acc 18.78\n",
      "Ep 0 \titer 92  \tloss 1.46953, train acc 46.71, train corr acc 17.35, val acc 51.99, val corr acc 19.05\n",
      "Ep 0 \titer 93  \tloss 1.39179, train acc 51.97, train corr acc 17.98, val acc 52.03, val corr acc 19.39\n",
      "Ep 0 \titer 94  \tloss 1.26145, train acc 60.53, train corr acc 20.00, val acc 51.97, val corr acc 19.09\n",
      "Ep 0 \titer 95  \tloss 1.29562, train acc 55.26, train corr acc 15.38, val acc 51.82, val corr acc 18.46\n",
      "Ep 0 \titer 96  \tloss 1.78199, train acc 35.53, train corr acc 12.50, val acc 51.87, val corr acc 18.98\n",
      "Ep 0 \titer 97  \tloss 1.28329, train acc 55.26, train corr acc 16.05, val acc 51.91, val corr acc 19.58\n",
      "Ep 0 \titer 98  \tloss 1.57559, train acc 45.39, train corr acc 19.77, val acc 52.04, val corr acc 20.18\n",
      "Ep 0 \titer 99  \tloss 1.14248, train acc 65.13, train corr acc 26.39, val acc 52.07, val corr acc 19.81\n",
      "Ep 0 \titer 100  \tloss 1.33937, train acc 59.87, train corr acc 22.08, val acc 51.90, val corr acc 18.45\n",
      "Ep 0 \titer 101  \tloss 1.23374, train acc 57.24, train corr acc 15.58, val acc 51.62, val corr acc 17.45\n",
      "Ep 0 \titer 102  \tloss 1.26753, train acc 56.58, train corr acc 18.52, val acc 51.55, val corr acc 17.20\n",
      "Ep 0 \titer 103  \tloss 1.32936, train acc 51.97, train corr acc 12.05, val acc 51.58, val corr acc 17.31\n",
      "Ep 0 \titer 104  \tloss 1.35171, train acc 54.61, train corr acc 20.69, val acc 51.92, val corr acc 18.31\n",
      "Ep 0 \titer 105  \tloss 1.48219, train acc 46.71, train corr acc 17.35, val acc 52.32, val corr acc 20.52\n",
      "Ep 0 \titer 106  \tloss 1.34791, train acc 50.00, train corr acc 24.69, val acc 52.25, val corr acc 22.76\n",
      "Ep 0 \titer 107  \tloss 1.48357, train acc 50.66, train corr acc 22.47, val acc 51.54, val corr acc 24.75\n",
      "Ep 0 \titer 108  \tloss 1.27389, train acc 58.55, train corr acc 30.59, val acc 51.22, val corr acc 25.76\n",
      "Ep 0 \titer 109  \tloss 1.32729, train acc 55.26, train corr acc 32.91, val acc 51.84, val corr acc 24.59\n",
      "Ep 0 \titer 110  \tloss 1.61724, train acc 38.82, train corr acc 18.28, val acc 52.44, val corr acc 23.01\n",
      "Ep 0 \titer 111  \tloss 1.35362, train acc 53.29, train corr acc 26.58, val acc 52.55, val corr acc 20.62\n",
      "Ep 0 \titer 112  \tloss 1.35846, train acc 55.92, train corr acc 24.72, val acc 52.26, val corr acc 19.07\n",
      "Ep 0 \titer 113  \tloss 1.42071, train acc 48.68, train corr acc 19.59, val acc 52.29, val corr acc 18.85\n",
      "Ep 0 \titer 114  \tloss 1.39989, train acc 54.61, train corr acc 23.33, val acc 52.34, val corr acc 18.90\n",
      "Ep 0 \titer 115  \tloss 1.29228, train acc 59.87, train corr acc 14.08, val acc 52.19, val corr acc 18.42\n",
      "Ep 0 \titer 116  \tloss 1.67157, train acc 42.76, train corr acc 16.35, val acc 52.41, val corr acc 18.88\n",
      "Ep 0 \titer 117  \tloss 1.42825, train acc 48.03, train corr acc 15.56, val acc 52.72, val corr acc 19.42\n",
      "Ep 0 \titer 118  \tloss 1.42093, train acc 48.68, train corr acc 20.41, val acc 53.40, val corr acc 20.92\n",
      "Ep 0 \titer 119  \tloss 1.19711, train acc 61.18, train corr acc 21.33, val acc 54.03, val corr acc 22.28\n",
      "Ep 0 \titer 120  \tloss 1.46998, train acc 50.66, train corr acc 20.21, val acc 54.55, val corr acc 23.89\n",
      "Ep 0 \titer 121  \tloss 1.32252, train acc 53.95, train corr acc 22.73, val acc 54.81, val corr acc 25.26\n",
      "Ep 0 \titer 122  \tloss 1.37952, train acc 49.34, train corr acc 16.67, val acc 54.87, val corr acc 25.59\n",
      "Ep 0 \titer 123  \tloss 1.34938, train acc 56.58, train corr acc 17.50, val acc 54.74, val corr acc 24.49\n",
      "Ep 0 \titer 124  \tloss 1.35601, train acc 52.63, train corr acc 26.80, val acc 54.50, val corr acc 23.86\n",
      "Ep 0 \titer 125  \tloss 1.48937, train acc 46.05, train corr acc 20.59, val acc 54.56, val corr acc 24.25\n",
      "Ep 0 \titer 126  \tloss 1.18774, train acc 61.84, train corr acc 30.12, val acc 54.69, val corr acc 25.02\n",
      "Ep 0 \titer 127  \tloss 1.28588, train acc 51.32, train corr acc 19.57, val acc 55.16, val corr acc 26.03\n",
      "Ep 0 \titer 128  \tloss 1.22106, train acc 57.89, train corr acc 24.10, val acc 55.65, val corr acc 26.49\n",
      "Ep 0 \titer 129  \tloss 1.27309, train acc 59.87, train corr acc 27.38, val acc 55.74, val corr acc 26.00\n",
      "Ep 0 \titer 130  \tloss 1.26273, train acc 57.24, train corr acc 26.14, val acc 55.82, val corr acc 26.10\n",
      "Ep 0 \titer 131  \tloss 1.30976, train acc 53.95, train corr acc 18.60, val acc 56.01, val corr acc 26.54\n",
      "Ep 0 \titer 132  \tloss 1.18315, train acc 63.16, train corr acc 27.27, val acc 55.80, val corr acc 26.23\n",
      "Ep 0 \titer 133  \tloss 1.17857, train acc 59.87, train corr acc 25.00, val acc 55.46, val corr acc 25.48\n",
      "Ep 0 \titer 134  \tloss 1.26386, train acc 57.24, train corr acc 19.75, val acc 54.89, val corr acc 24.18\n",
      "Ep 0 \titer 135  \tloss 1.28168, train acc 53.29, train corr acc 23.66, val acc 54.66, val corr acc 24.09\n",
      "Ep 0 \titer 136  \tloss 1.38166, train acc 51.32, train corr acc 21.28, val acc 54.70, val corr acc 25.33\n",
      "Ep 0 \titer 137  \tloss 1.53188, train acc 46.71, train corr acc 20.79, val acc 54.39, val corr acc 28.45\n",
      "Ep 0 \titer 138  \tloss 1.33949, train acc 53.95, train corr acc 30.95, val acc 55.96, val corr acc 27.55\n",
      "Ep 0 \titer 139  \tloss 1.26740, train acc 56.58, train corr acc 19.75, val acc 56.23, val corr acc 26.71\n",
      "Ep 0 \titer 140  \tloss 1.25947, train acc 57.24, train corr acc 25.29, val acc 56.19, val corr acc 25.83\n",
      "Ep 0 \titer 141  \tloss 1.40326, train acc 49.34, train corr acc 20.62, val acc 56.06, val corr acc 25.61\n",
      "Ep 0 \titer 142  \tloss 1.53661, train acc 43.42, train corr acc 19.80, val acc 56.14, val corr acc 26.70\n",
      "Ep 0 \titer 143  \tloss 1.19917, train acc 61.18, train corr acc 29.27, val acc 55.80, val corr acc 27.90\n",
      "Ep 0 \titer 144  \tloss 1.30703, train acc 55.92, train corr acc 22.35, val acc 55.70, val corr acc 28.80\n",
      "Ep 0 \titer 145  \tloss 1.23697, train acc 55.92, train corr acc 17.33, val acc 56.15, val corr acc 27.39\n",
      "Ep 0 \titer 146  \tloss 1.14071, train acc 61.84, train corr acc 32.91, val acc 56.08, val corr acc 25.53\n",
      "Ep 0 \titer 147  \tloss 1.24237, train acc 59.87, train corr acc 25.35, val acc 54.78, val corr acc 22.40\n",
      "Ep 0 \titer 148  \tloss 1.42093, train acc 53.29, train corr acc 21.11, val acc 54.28, val corr acc 21.35\n",
      "Ep 0 \titer 149  \tloss 1.44193, train acc 48.68, train corr acc 18.75, val acc 55.68, val corr acc 23.87\n",
      "Ep 0 \titer 150  \tloss 1.25867, train acc 55.26, train corr acc 17.07, val acc 56.45, val corr acc 25.34\n",
      "Ep 0 \titer 151  \tloss 1.33349, train acc 51.32, train corr acc 28.57, val acc 56.71, val corr acc 26.46\n",
      "Ep 0 \titer 152  \tloss 1.26491, train acc 63.16, train corr acc 33.33, val acc 56.71, val corr acc 27.29\n",
      "Ep 0 \titer 153  \tloss 1.33563, train acc 56.58, train corr acc 33.33, val acc 56.94, val corr acc 27.50\n",
      "Ep 0 \titer 154  \tloss 1.27562, train acc 59.87, train corr acc 32.18, val acc 57.19, val corr acc 27.19\n",
      "Ep 0 \titer 155  \tloss 1.37301, train acc 51.32, train corr acc 21.28, val acc 57.30, val corr acc 27.04\n",
      "Ep 0 \titer 156  \tloss 1.21619, train acc 61.18, train corr acc 26.25, val acc 57.14, val corr acc 26.57\n",
      "Ep 0 \titer 157  \tloss 1.04871, train acc 72.37, train corr acc 46.15, val acc 56.89, val corr acc 26.06\n",
      "Ep 0 \titer 158  \tloss 1.41037, train acc 46.05, train corr acc 21.90, val acc 57.06, val corr acc 26.55\n",
      "Ep 0 \titer 159  \tloss 1.84147, train acc 34.87, train corr acc 14.66, val acc 57.50, val corr acc 28.13\n",
      "Ep 0 \titer 160  \tloss 1.23254, train acc 62.50, train corr acc 32.94, val acc 57.40, val corr acc 29.82\n",
      "Ep 0 \titer 161  \tloss 1.25615, train acc 61.84, train corr acc 26.58, val acc 57.16, val corr acc 30.53\n",
      "Ep 0 \titer 162  \tloss 1.24053, train acc 63.82, train corr acc 31.65, val acc 57.29, val corr acc 29.90\n",
      "Ep 0 \titer 163  \tloss 1.12188, train acc 66.45, train corr acc 38.55, val acc 57.42, val corr acc 29.20\n",
      "Ep 0 \titer 164  \tloss 1.24861, train acc 56.58, train corr acc 31.25, val acc 57.38, val corr acc 29.03\n",
      "Ep 0 \titer 165  \tloss 1.15136, train acc 59.87, train corr acc 26.92, val acc 57.31, val corr acc 28.59\n",
      "Ep 0 \titer 166  \tloss 1.29377, train acc 55.26, train corr acc 29.17, val acc 57.46, val corr acc 29.40\n",
      "Ep 0 \titer 167  \tloss 1.34467, train acc 49.34, train corr acc 19.79, val acc 57.20, val corr acc 31.19\n",
      "Ep 0 \titer 168  \tloss 1.27573, train acc 53.95, train corr acc 27.84, val acc 55.43, val corr acc 33.97\n",
      "Ep 0 \titer 169  \tloss 1.39516, train acc 55.26, train corr acc 37.50, val acc 55.81, val corr acc 33.68\n",
      "Ep 0 \titer 170  \tloss 1.13431, train acc 63.82, train corr acc 45.98, val acc 56.64, val corr acc 32.90\n",
      "Ep 0 \titer 171  \tloss 1.29192, train acc 53.29, train corr acc 32.50, val acc 57.60, val corr acc 30.97\n",
      "Ep 0 \titer 172  \tloss 1.19666, train acc 60.53, train corr acc 24.68, val acc 57.67, val corr acc 29.29\n",
      "Ep 0 \titer 173  \tloss 1.15770, train acc 63.82, train corr acc 28.57, val acc 57.49, val corr acc 28.15\n",
      "Ep 0 \titer 174  \tloss 1.02646, train acc 63.82, train corr acc 28.57, val acc 57.30, val corr acc 27.39\n",
      "Ep 0 \titer 175  \tloss 1.34042, train acc 51.97, train corr acc 22.34, val acc 57.25, val corr acc 27.23\n",
      "Ep 0 \titer 176  \tloss 1.41810, train acc 54.61, train corr acc 24.18, val acc 57.29, val corr acc 27.43\n",
      "Ep 0 \titer 177  \tloss 1.32483, train acc 49.34, train corr acc 17.20, val acc 57.27, val corr acc 28.11\n",
      "Ep 0 \titer 178  \tloss 1.28286, train acc 53.95, train corr acc 30.00, val acc 57.11, val corr acc 28.72\n",
      "Ep 0 \titer 179  \tloss 1.34897, train acc 51.97, train corr acc 28.00, val acc 56.47, val corr acc 29.87\n",
      "Ep 0 \titer 180  \tloss 1.22542, train acc 54.61, train corr acc 22.73, val acc 55.50, val corr acc 30.93\n",
      "Ep 0 \titer 181  \tloss 1.24989, train acc 59.87, train corr acc 37.08, val acc 55.18, val corr acc 31.80\n",
      "Ep 0 \titer 182  \tloss 1.25889, train acc 54.61, train corr acc 30.86, val acc 56.08, val corr acc 31.66\n",
      "Ep 0 \titer 183  \tloss 1.19809, train acc 57.89, train corr acc 29.21, val acc 56.86, val corr acc 31.00\n",
      "Ep 0 \titer 184  \tloss 1.22630, train acc 55.26, train corr acc 25.88, val acc 57.41, val corr acc 30.26\n",
      "Ep 0 \titer 185  \tloss 1.13885, train acc 61.18, train corr acc 26.58, val acc 57.63, val corr acc 29.19\n",
      "Ep 0 \titer 186  \tloss 1.36205, train acc 48.03, train corr acc 29.70, val acc 57.63, val corr acc 28.79\n",
      "Ep 0 \titer 187  \tloss 1.22878, train acc 51.97, train corr acc 31.40, val acc 57.60, val corr acc 28.34\n",
      "Ep 0 \titer 188  \tloss 1.33827, train acc 51.97, train corr acc 30.48, val acc 57.57, val corr acc 28.54\n",
      "Ep 0 \titer 189  \tloss 1.16311, train acc 61.18, train corr acc 27.16, val acc 57.41, val corr acc 28.55\n",
      "Ep 0 \titer 190  \tloss 1.12859, train acc 57.89, train corr acc 16.88, val acc 57.20, val corr acc 28.74\n",
      "Ep 0 \titer 191  \tloss 1.07600, train acc 63.82, train corr acc 39.56, val acc 56.96, val corr acc 29.29\n",
      "Ep 0 \titer 192  \tloss 1.11692, train acc 64.47, train corr acc 30.67, val acc 56.96, val corr acc 29.45\n",
      "Ep 0 \titer 193  \tloss 1.39147, train acc 53.29, train corr acc 25.26, val acc 56.94, val corr acc 29.84\n",
      "Ep 0 \titer 194  \tloss 1.44096, train acc 50.00, train corr acc 23.23, val acc 56.60, val corr acc 31.27\n",
      "Ep 0 \titer 195  \tloss 1.34135, train acc 50.66, train corr acc 29.25, val acc 55.43, val corr acc 33.58\n",
      "Ep 0 \titer 196  \tloss 1.22201, train acc 59.87, train corr acc 31.25, val acc 55.60, val corr acc 34.19\n",
      "Ep 0 \titer 197  \tloss 1.21838, train acc 61.18, train corr acc 42.86, val acc 56.67, val corr acc 33.08\n",
      "Ep 0 \titer 198  \tloss 1.05036, train acc 69.74, train corr acc 52.50, val acc 57.65, val corr acc 31.46\n",
      "Ep 0 \titer 199  \tloss 1.05849, train acc 63.82, train corr acc 36.78, val acc 57.89, val corr acc 30.61\n",
      "Ep 0 \titer 200  \tloss 1.15918, train acc 59.21, train corr acc 28.74, val acc 57.78, val corr acc 30.44\n",
      "Ep 0 \titer 201  \tloss 1.23733, train acc 57.24, train corr acc 33.67, val acc 57.57, val corr acc 31.01\n",
      "Ep 0 \titer 202  \tloss 1.13640, train acc 63.82, train corr acc 32.93, val acc 57.36, val corr acc 31.24\n",
      "Ep 0 \titer 203  \tloss 1.20164, train acc 57.24, train corr acc 21.62, val acc 57.16, val corr acc 30.98\n",
      "Ep 0 \titer 204  \tloss 1.27993, train acc 55.92, train corr acc 32.50, val acc 57.12, val corr acc 30.32\n",
      "Ep 0 \titer 205  \tloss 1.23223, train acc 61.84, train corr acc 34.83, val acc 57.10, val corr acc 30.49\n",
      "Ep 0 \titer 206  \tloss 1.42128, train acc 51.97, train corr acc 23.96, val acc 57.05, val corr acc 30.87\n",
      "Ep 0 \titer 207  \tloss 1.04785, train acc 65.79, train corr acc 33.33, val acc 56.88, val corr acc 31.61\n",
      "Ep 0 \titer 208  \tloss 1.01155, train acc 69.74, train corr acc 43.90, val acc 56.53, val corr acc 32.65\n",
      "Ep 0 \titer 209  \tloss 1.28692, train acc 51.32, train corr acc 28.77, val acc 56.64, val corr acc 32.60\n",
      "Ep 0 \titer 210  \tloss 1.12482, train acc 61.84, train corr acc 34.18, val acc 57.04, val corr acc 31.94\n",
      "Ep 0 \titer 211  \tloss 1.34642, train acc 51.32, train corr acc 24.49, val acc 57.10, val corr acc 32.07\n",
      "Ep 0 \titer 212  \tloss 1.21987, train acc 63.16, train corr acc 45.24, val acc 57.37, val corr acc 31.41\n",
      "Ep 0 \titer 213  \tloss 1.09439, train acc 66.45, train corr acc 37.33, val acc 57.68, val corr acc 30.36\n",
      "Ep 0 \titer 214  \tloss 1.29054, train acc 59.87, train corr acc 32.97, val acc 57.74, val corr acc 29.62\n",
      "Ep 0 \titer 215  \tloss 1.27470, train acc 56.58, train corr acc 23.26, val acc 57.76, val corr acc 29.17\n",
      "Ep 0 \titer 216  \tloss 1.39675, train acc 48.68, train corr acc 23.53, val acc 57.96, val corr acc 29.55\n",
      "Ep 0 \titer 217  \tloss 1.27150, train acc 54.61, train corr acc 24.18, val acc 58.12, val corr acc 30.23\n",
      "Ep 0 \titer 218  \tloss 1.23341, train acc 56.58, train corr acc 22.62, val acc 58.19, val corr acc 30.84\n",
      "Ep 0 \titer 219  \tloss 1.26204, train acc 62.50, train corr acc 34.88, val acc 57.99, val corr acc 31.32\n",
      "Ep 0 \titer 220  \tloss 1.31708, train acc 54.61, train corr acc 30.85, val acc 57.82, val corr acc 31.50\n",
      "Ep 0 \titer 221  \tloss 1.45282, train acc 55.92, train corr acc 34.18, val acc 57.85, val corr acc 31.23\n",
      "Ep 0 \titer 222  \tloss 1.29007, train acc 48.68, train corr acc 15.79, val acc 57.92, val corr acc 30.53\n",
      "Ep 0 \titer 223  \tloss 1.24629, train acc 52.63, train corr acc 26.19, val acc 57.99, val corr acc 29.81\n",
      "Ep 0 \titer 224  \tloss 1.39875, train acc 49.34, train corr acc 17.58, val acc 57.93, val corr acc 29.05\n",
      "Ep 0 \titer 225  \tloss 1.16094, train acc 62.50, train corr acc 25.00, val acc 57.81, val corr acc 28.32\n",
      "Ep 0 \titer 226  \tloss 1.17292, train acc 55.26, train corr acc 26.09, val acc 57.72, val corr acc 27.90\n",
      "Ep 0 \titer 227  \tloss 1.07056, train acc 67.11, train corr acc 35.06, val acc 57.60, val corr acc 27.56\n",
      "Ep 0 \titer 228  \tloss 1.14679, train acc 60.53, train corr acc 24.05, val acc 57.52, val corr acc 27.30\n",
      "Ep 0 \titer 229  \tloss 1.34095, train acc 51.97, train corr acc 23.96, val acc 57.60, val corr acc 27.45\n",
      "Ep 0 \titer 230  \tloss 1.15691, train acc 63.82, train corr acc 32.93, val acc 57.68, val corr acc 27.60\n",
      "Ep 0 \titer 231  \tloss 1.65559, train acc 50.00, train corr acc 19.15, val acc 57.91, val corr acc 28.12\n",
      "Ep 0 \titer 232  \tloss 1.43104, train acc 55.92, train corr acc 31.63, val acc 58.14, val corr acc 28.77\n",
      "Ep 0 \titer 233  \tloss 1.23612, train acc 57.24, train corr acc 32.99, val acc 58.43, val corr acc 29.86\n",
      "Ep 0 \titer 234  \tloss 1.25455, train acc 59.21, train corr acc 26.19, val acc 58.54, val corr acc 30.82\n",
      "Ep 0 \titer 235  \tloss 1.16057, train acc 62.50, train corr acc 33.72, val acc 58.50, val corr acc 31.54\n",
      "Ep 0 \titer 236  \tloss 1.27248, train acc 56.58, train corr acc 29.55, val acc 58.63, val corr acc 32.02\n",
      "Ep 0 \titer 237  \tloss 1.42670, train acc 51.32, train corr acc 30.12, val acc 58.67, val corr acc 31.96\n",
      "Ep 0 \titer 238  \tloss 1.11088, train acc 63.16, train corr acc 31.71, val acc 58.69, val corr acc 31.69\n",
      "Ep 0 \titer 239  \tloss 1.47197, train acc 54.61, train corr acc 22.47, val acc 58.75, val corr acc 31.51\n",
      "Ep 0 \titer 240  \tloss 1.34200, train acc 47.37, train corr acc 29.47, val acc 58.63, val corr acc 31.13\n",
      "Ep 0 \titer 241  \tloss 1.17930, train acc 60.53, train corr acc 24.36, val acc 58.52, val corr acc 30.39\n",
      "Ep 0 \titer 242  \tloss 1.08649, train acc 60.53, train corr acc 28.57, val acc 58.38, val corr acc 29.84\n",
      "Ep 0 \titer 243  \tloss 1.25075, train acc 54.61, train corr acc 16.05, val acc 58.20, val corr acc 29.30\n",
      "Ep 0 \titer 244  \tloss 1.35092, train acc 51.32, train corr acc 22.92, val acc 58.16, val corr acc 29.16\n",
      "Ep 0 \titer 245  \tloss 1.15605, train acc 59.87, train corr acc 35.00, val acc 58.10, val corr acc 28.90\n",
      "Ep 0 \titer 246  \tloss 1.23172, train acc 54.61, train corr acc 23.33, val acc 58.22, val corr acc 29.24\n",
      "Ep 0 \titer 247  \tloss 1.40389, train acc 46.71, train corr acc 29.79, val acc 58.30, val corr acc 29.39\n",
      "Ep 0 \titer 248  \tloss 1.21637, train acc 55.26, train corr acc 25.84, val acc 58.48, val corr acc 29.84\n",
      "Ep 0 \titer 249  \tloss 1.36744, train acc 42.76, train corr acc 18.48, val acc 58.56, val corr acc 30.13\n",
      "Ep 0 \titer 250  \tloss 1.18300, train acc 55.92, train corr acc 20.24, val acc 58.51, val corr acc 30.60\n",
      "Ep 0 \titer 251  \tloss 1.10794, train acc 64.47, train corr acc 28.00, val acc 58.49, val corr acc 30.89\n",
      "Ep 0 \titer 252  \tloss 1.21432, train acc 55.26, train corr acc 25.27, val acc 58.34, val corr acc 31.30\n",
      "Ep 0 \titer 253  \tloss 1.07821, train acc 66.45, train corr acc 40.00, val acc 58.19, val corr acc 31.88\n",
      "Ep 0 \titer 254  \tloss 1.14786, train acc 61.18, train corr acc 39.13, val acc 58.12, val corr acc 32.29\n",
      "Ep 0 \titer 255  \tloss 1.26260, train acc 57.24, train corr acc 37.50, val acc 57.89, val corr acc 33.23\n",
      "Ep 0 \titer 256  \tloss 1.17824, train acc 58.55, train corr acc 29.21, val acc 57.60, val corr acc 33.73\n",
      "Ep 0 \titer 257  \tloss 1.19918, train acc 61.18, train corr acc 35.44, val acc 58.26, val corr acc 32.91\n",
      "Ep 0 \titer 258  \tloss 1.23207, train acc 54.61, train corr acc 30.68, val acc 58.70, val corr acc 32.09\n",
      "Ep 0 \titer 259  \tloss 1.15792, train acc 59.21, train corr acc 31.87, val acc 58.72, val corr acc 31.33\n",
      "Ep 0 \titer 260  \tloss 1.09745, train acc 63.16, train corr acc 32.53, val acc 58.71, val corr acc 30.74\n",
      "Ep 0 \titer 261  \tloss 1.02395, train acc 60.53, train corr acc 22.08, val acc 58.52, val corr acc 30.18\n",
      "Ep 0 \titer 262  \tloss 0.97688, train acc 67.11, train corr acc 35.06, val acc 58.49, val corr acc 29.95\n",
      "Ep 0 \titer 263  \tloss 1.19817, train acc 60.53, train corr acc 31.82, val acc 58.49, val corr acc 30.34\n",
      "Ep 0 \titer 264  \tloss 1.63405, train acc 38.16, train corr acc 18.97, val acc 58.34, val corr acc 32.18\n",
      "Ep 0 \titer 265  \tloss 1.20733, train acc 65.13, train corr acc 35.00, val acc 57.57, val corr acc 33.67\n",
      "Ep 0 \titer 266  \tloss 1.15904, train acc 57.89, train corr acc 35.35, val acc 55.81, val corr acc 35.71\n",
      "Ep 0 \titer 267  \tloss 1.12465, train acc 62.50, train corr acc 38.10, val acc 54.59, val corr acc 36.65\n",
      "Ep 0 \titer 268  \tloss 1.11816, train acc 61.18, train corr acc 46.67, val acc 54.97, val corr acc 36.24\n",
      "Ep 0 \titer 269  \tloss 1.21777, train acc 53.29, train corr acc 40.78, val acc 55.24, val corr acc 36.28\n",
      "Ep 0 \titer 270  \tloss 1.13990, train acc 59.21, train corr acc 41.00, val acc 55.40, val corr acc 36.25\n",
      "Ep 0 \titer 271  \tloss 1.23156, train acc 51.97, train corr acc 34.78, val acc 56.12, val corr acc 35.73\n",
      "Ep 0 \titer 272  \tloss 1.22669, train acc 51.32, train corr acc 30.84, val acc 56.04, val corr acc 35.73\n",
      "Ep 0 \titer 273  \tloss 1.12118, train acc 61.18, train corr acc 36.96, val acc 56.26, val corr acc 35.31\n",
      "Ep 0 \titer 274  \tloss 1.23250, train acc 60.53, train corr acc 35.96, val acc 56.91, val corr acc 34.46\n",
      "Ep 0 \titer 275  \tloss 1.07052, train acc 62.50, train corr acc 40.00, val acc 57.20, val corr acc 34.29\n",
      "Ep 0 \titer 276  \tloss 1.27380, train acc 59.21, train corr acc 38.27, val acc 57.91, val corr acc 33.96\n",
      "Ep 0 \titer 277  \tloss 1.24176, train acc 55.92, train corr acc 30.53, val acc 58.28, val corr acc 33.44\n",
      "Ep 0 \titer 278  \tloss 1.28619, train acc 53.95, train corr acc 24.39, val acc 58.67, val corr acc 32.75\n",
      "Ep 0 \titer 279  \tloss 1.18851, train acc 60.53, train corr acc 33.33, val acc 58.88, val corr acc 32.01\n",
      "Ep 0 \titer 280  \tloss 1.34061, train acc 52.63, train corr acc 19.32, val acc 58.92, val corr acc 31.67\n",
      "Ep 0 \titer 281  \tloss 1.22930, train acc 54.61, train corr acc 31.58, val acc 58.92, val corr acc 31.60\n",
      "Ep 0 \titer 282  \tloss 1.12138, train acc 60.53, train corr acc 25.00, val acc 59.01, val corr acc 31.81\n",
      "Ep 0 \titer 283  \tloss 1.18753, train acc 61.84, train corr acc 40.21, val acc 59.01, val corr acc 32.68\n",
      "Ep 0 \titer 284  \tloss 1.25817, train acc 53.29, train corr acc 23.66, val acc 58.53, val corr acc 33.75\n",
      "Ep 0 \titer 285  \tloss 1.01029, train acc 67.76, train corr acc 35.53, val acc 58.84, val corr acc 33.09\n",
      "Ep 0 \titer 286  \tloss 1.14051, train acc 59.21, train corr acc 33.33, val acc 58.88, val corr acc 32.79\n",
      "Ep 0 \titer 287  \tloss 1.22617, train acc 53.95, train corr acc 29.89, val acc 58.77, val corr acc 32.30\n",
      "Ep 0 \titer 288  \tloss 1.09846, train acc 67.76, train corr acc 52.04, val acc 58.82, val corr acc 32.52\n",
      "Ep 0 \titer 289  \tloss 1.16539, train acc 56.58, train corr acc 38.82, val acc 59.00, val corr acc 32.57\n",
      "Ep 0 \titer 290  \tloss 1.16501, train acc 63.16, train corr acc 33.33, val acc 59.03, val corr acc 32.80\n",
      "Ep 0 \titer 291  \tloss 1.23040, train acc 53.29, train corr acc 39.80, val acc 59.13, val corr acc 32.95\n",
      "Ep 0 \titer 292  \tloss 1.32083, train acc 53.29, train corr acc 35.96, val acc 59.20, val corr acc 32.98\n",
      "Ep 0 \titer 293  \tloss 1.25491, train acc 55.92, train corr acc 27.78, val acc 58.84, val corr acc 32.80\n",
      "Ep 0 \titer 294  \tloss 1.38541, train acc 52.63, train corr acc 25.53, val acc 59.35, val corr acc 32.32\n",
      "Ep 0 \titer 295  \tloss 1.25398, train acc 51.32, train corr acc 38.10, val acc 59.02, val corr acc 30.73\n",
      "Ep 0 \titer 296  \tloss 1.12524, train acc 58.55, train corr acc 28.41, val acc 58.92, val corr acc 30.19\n",
      "Ep 0 \titer 297  \tloss 0.99214, train acc 67.76, train corr acc 42.35, val acc 59.02, val corr acc 30.21\n",
      "Ep 0 \titer 298  \tloss 1.20306, train acc 55.92, train corr acc 23.86, val acc 59.19, val corr acc 30.59\n",
      "Ep 0 \titer 299  \tloss 1.07055, train acc 64.47, train corr acc 32.91, val acc 59.34, val corr acc 30.97\n",
      "Ep 0 \titer 300  \tloss 1.08565, train acc 69.74, train corr acc 45.24, val acc 59.32, val corr acc 30.80\n",
      "Ep 0 \titer 301  \tloss 1.01042, train acc 67.76, train corr acc 42.35, val acc 59.17, val corr acc 30.32\n",
      "Ep 0 \titer 302  \tloss 1.11306, train acc 59.21, train corr acc 34.74, val acc 59.24, val corr acc 30.55\n",
      "Ep 0 \titer 303  \tloss 1.06761, train acc 61.18, train corr acc 35.16, val acc 59.26, val corr acc 31.02\n",
      "Ep 0 \titer 304  \tloss 1.21047, train acc 57.89, train corr acc 29.67, val acc 59.27, val corr acc 31.58\n",
      "Ep 0 \titer 305  \tloss 1.20668, train acc 60.53, train corr acc 33.33, val acc 59.21, val corr acc 32.07\n",
      "Ep 0 \titer 306  \tloss 1.17165, train acc 59.21, train corr acc 35.42, val acc 58.88, val corr acc 32.80\n",
      "Ep 0 \titer 307  \tloss 1.10795, train acc 65.13, train corr acc 41.57, val acc 58.92, val corr acc 32.60\n",
      "Ep 0 \titer 308  \tloss 1.28582, train acc 52.63, train corr acc 30.77, val acc 58.98, val corr acc 32.74\n",
      "Ep 0 \titer 309  \tloss 1.25992, train acc 55.26, train corr acc 26.88, val acc 58.89, val corr acc 32.95\n",
      "Ep 0 \titer 310  \tloss 1.05592, train acc 65.13, train corr acc 38.82, val acc 58.78, val corr acc 32.96\n",
      "Ep 0 \titer 311  \tloss 1.27224, train acc 57.89, train corr acc 33.78, val acc 59.31, val corr acc 32.06\n",
      "Ep 0 \titer 312  \tloss 1.25336, train acc 56.58, train corr acc 23.26, val acc 59.37, val corr acc 31.27\n",
      "Ep 0 \titer 313  \tloss 1.22245, train acc 59.21, train corr acc 29.89, val acc 59.20, val corr acc 30.65\n",
      "Ep 0 \titer 314  \tloss 1.15202, train acc 62.50, train corr acc 26.92, val acc 59.23, val corr acc 30.56\n",
      "Ep 0 \titer 315  \tloss 1.50795, train acc 39.47, train corr acc 17.86, val acc 58.33, val corr acc 33.05\n",
      "Ep 0 \titer 316  \tloss 1.71155, train acc 41.45, train corr acc 19.19, val acc 54.40, val corr acc 34.58\n",
      "Ep 0 \titer 317  \tloss 1.21465, train acc 66.45, train corr acc 45.12, val acc 58.67, val corr acc 32.19\n",
      "Ep 0 \titer 318  \tloss 1.22283, train acc 56.58, train corr acc 30.53, val acc 59.01, val corr acc 31.43\n",
      "Ep 0 \titer 319  \tloss 1.15173, train acc 63.82, train corr acc 40.45, val acc 58.95, val corr acc 30.93\n",
      "Ep 0 \titer 320  \tloss 1.22178, train acc 57.24, train corr acc 24.00, val acc 58.85, val corr acc 30.28\n",
      "Ep 0 \titer 321  \tloss 1.24416, train acc 55.26, train corr acc 39.62, val acc 58.81, val corr acc 29.82\n",
      "Ep 0 \titer 322  \tloss 1.34393, train acc 56.58, train corr acc 36.00, val acc 58.81, val corr acc 29.72\n",
      "Ep 0 \titer 323  \tloss 1.32865, train acc 57.24, train corr acc 28.57, val acc 58.89, val corr acc 29.81\n",
      "Ep 0 \titer 324  \tloss 1.13117, train acc 65.13, train corr acc 40.45, val acc 58.99, val corr acc 29.99\n",
      "Ep 0 \titer 325  \tloss 1.24738, train acc 55.26, train corr acc 27.66, val acc 59.07, val corr acc 30.41\n",
      "Ep 0 \titer 326  \tloss 1.32468, train acc 50.66, train corr acc 27.59, val acc 59.13, val corr acc 30.64\n",
      "Ep 0 \titer 327  \tloss 1.16043, train acc 59.87, train corr acc 31.46, val acc 59.13, val corr acc 30.99\n",
      "Ep 0 \titer 328  \tloss 1.12658, train acc 57.24, train corr acc 32.99, val acc 59.09, val corr acc 31.67\n",
      "Ep 0 \titer 329  \tloss 1.22229, train acc 58.55, train corr acc 37.00, val acc 58.88, val corr acc 32.55\n",
      "Ep 0 \titer 330  \tloss 1.11387, train acc 62.50, train corr acc 41.94, val acc 58.42, val corr acc 33.28\n",
      "Ep 0 \titer 331  \tloss 1.22903, train acc 57.24, train corr acc 33.67, val acc 57.94, val corr acc 34.08\n",
      "Ep 0 \titer 332  \tloss 1.39798, train acc 53.95, train corr acc 44.83, val acc 58.34, val corr acc 33.28\n",
      "Ep 0 \titer 333  \tloss 1.18733, train acc 58.55, train corr acc 35.37, val acc 58.73, val corr acc 32.39\n",
      "Ep 0 \titer 334  \tloss 1.09470, train acc 66.45, train corr acc 37.97, val acc 58.92, val corr acc 31.29\n",
      "Ep 0 \titer 335  \tloss 1.10502, train acc 59.87, train corr acc 27.71, val acc 58.80, val corr acc 30.22\n",
      "Ep 0 \titer 336  \tloss 1.08574, train acc 66.45, train corr acc 43.96, val acc 58.71, val corr acc 29.64\n",
      "Ep 0 \titer 337  \tloss 1.12938, train acc 61.18, train corr acc 33.71, val acc 58.60, val corr acc 29.28\n",
      "Ep 0 \titer 338  \tloss 1.10354, train acc 59.21, train corr acc 27.06, val acc 58.60, val corr acc 29.24\n",
      "Ep 0 \titer 339  \tloss 1.40867, train acc 48.68, train corr acc 28.44, val acc 58.72, val corr acc 29.70\n",
      "Ep 0 \titer 340  \tloss 1.25315, train acc 60.53, train corr acc 25.64, val acc 58.83, val corr acc 30.01\n",
      "Ep 0 \titer 341  \tloss 1.12339, train acc 64.47, train corr acc 26.03, val acc 58.80, val corr acc 30.07\n",
      "Ep 0 \titer 342  \tloss 1.17183, train acc 61.84, train corr acc 37.21, val acc 58.85, val corr acc 30.23\n",
      "Ep 0 \titer 343  \tloss 1.37702, train acc 50.66, train corr acc 25.00, val acc 58.92, val corr acc 30.78\n",
      "Ep 0 \titer 344  \tloss 1.18767, train acc 57.24, train corr acc 28.57, val acc 58.92, val corr acc 31.38\n",
      "Ep 0 \titer 345  \tloss 1.26836, train acc 50.66, train corr acc 25.00, val acc 58.79, val corr acc 32.09\n",
      "Ep 0 \titer 346  \tloss 1.30135, train acc 55.26, train corr acc 35.24, val acc 58.44, val corr acc 33.11\n",
      "Ep 0 \titer 347  \tloss 1.05300, train acc 62.50, train corr acc 35.53, val acc 58.16, val corr acc 33.60\n",
      "Ep 0 \titer 348  \tloss 1.05198, train acc 67.11, train corr acc 47.62, val acc 58.10, val corr acc 33.80\n",
      "Ep 0 \titer 349  \tloss 1.22173, train acc 51.32, train corr acc 36.71, val acc 58.49, val corr acc 33.48\n",
      "Ep 0 \titer 350  \tloss 1.35197, train acc 49.34, train corr acc 26.25, val acc 58.98, val corr acc 32.70\n",
      "Ep 0 \titer 351  \tloss 1.25270, train acc 53.95, train corr acc 31.37, val acc 59.22, val corr acc 32.12\n",
      "Ep 0 \titer 352  \tloss 1.16610, train acc 60.53, train corr acc 29.41, val acc 59.25, val corr acc 31.44\n",
      "Ep 0 \titer 353  \tloss 1.25330, train acc 52.63, train corr acc 30.12, val acc 59.28, val corr acc 30.94\n",
      "Ep 0 \titer 354  \tloss 1.20385, train acc 63.16, train corr acc 43.82, val acc 59.23, val corr acc 30.47\n",
      "Ep 0 \titer 355  \tloss 1.25794, train acc 53.95, train corr acc 24.73, val acc 59.06, val corr acc 29.94\n",
      "Ep 0 \titer 356  \tloss 1.21957, train acc 55.92, train corr acc 29.47, val acc 59.05, val corr acc 29.84\n",
      "Ep 0 \titer 357  \tloss 1.21553, train acc 57.89, train corr acc 32.26, val acc 59.01, val corr acc 29.71\n",
      "Ep 0 \titer 358  \tloss 1.04347, train acc 68.42, train corr acc 42.17, val acc 59.00, val corr acc 29.65\n",
      "Ep 0 \titer 359  \tloss 1.16774, train acc 61.18, train corr acc 28.92, val acc 59.05, val corr acc 29.72\n",
      "Ep 0 \titer 360  \tloss 1.35329, train acc 53.29, train corr acc 30.39, val acc 59.17, val corr acc 29.99\n",
      "Ep 0 \titer 361  \tloss 1.10582, train acc 59.87, train corr acc 26.25, val acc 59.28, val corr acc 30.21\n",
      "Ep 0 \titer 362  \tloss 1.11498, train acc 57.24, train corr acc 28.57, val acc 59.31, val corr acc 30.33\n",
      "Ep 0 \titer 363  \tloss 1.00094, train acc 64.47, train corr acc 37.21, val acc 59.42, val corr acc 30.64\n",
      "Ep 0 \titer 364  \tloss 1.23010, train acc 55.92, train corr acc 34.57, val acc 59.46, val corr acc 30.78\n",
      "Ep 0 \titer 365  \tloss 1.13346, train acc 62.50, train corr acc 37.36, val acc 59.42, val corr acc 30.83\n",
      "Ep 0 \titer 366  \tloss 1.09605, train acc 65.13, train corr acc 36.14, val acc 59.50, val corr acc 31.11\n",
      "Ep 0 \titer 367  \tloss 1.03538, train acc 65.13, train corr acc 27.40, val acc 59.49, val corr acc 31.03\n",
      "Ep 0 \titer 368  \tloss 1.13593, train acc 62.50, train corr acc 32.94, val acc 59.47, val corr acc 31.06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-eb78b606ca5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_all_hocs_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_all_hocs_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_all_hocs_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_corrected_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_corrected_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss_acc\u001b[0m\u001b[0;34m,\u001b[0m                                                                  \u001b[0mcompute_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_per_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/lisa1010/dev/deepcode/code/model_predict_block.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, val_data, train_loss_acc, compute_loss_acc, compute_pred, num_epochs, batchsize, record_per_iter)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mtrain_corrected_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_corrected_acc_on_block_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mnum_iters\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_corrected_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_loss_acc_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# print(\"Ep {} \\titer {}  \\tloss {:.5f}, train acc {:.2f}, val acc {:.2f}\".format(epoch, num_iters, float(train_loss), train_acc * 100, val_acc *100) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    950\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                         self, node)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/Users/lisa1010/.theano/compiledir_Darwin-15.2.0-x86_64-i386-64bit-i386-2.7.11-64/scan_perform/mod.cpp:4193)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/anaconda/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training!\n",
    "for train_index, val_index in kf:\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", val_index)\n",
    "    train_data = (X_all_hocs_mat[train_index], mask_all_hocs_mat[train_index],y_all_hocs_mat[train_index])\n",
    "    val_data = (X_all_hocs_mat[val_index], mask_all_hocs_mat[val_index],y_all_hocs_mat[val_index])\n",
    "    \n",
    "    train_losses, train_accs, train_corrected_accs, val_losses, val_accs, val_corrected_accs = model.train(train_data, val_data, train_loss_acc, compute_loss_acc,\\\n",
    "                                                                  compute_pred, num_epochs=num_epochs, batchsize=batchsize, record_per_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n",
      "Preparing network inputs and targets, and the block maps...\n"
     ]
    }
   ],
   "source": [
    "X_all_hocs_mat, mask_all_hocs_mat, y_all_hocs_mat, split_indices = utils.load_dataset_predict_block_all_hocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ast_embeddings_all_hocs = generate_hidden_reps(X_all_hocs_mat, mask_all_hocs_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.save_ast_embeddings_for_all_hocs(ast_embeddings_all_hocs, split_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99950, 256)\n"
     ]
    }
   ],
   "source": [
    "print ast_embeddings_all_hocs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24856442  0.10019749 -0.0796252  -0.15180447  0.17074073  0.02689021\n",
      "  -0.37280924 -0.00674305  0.1848056   0.23062699]\n",
      " [-0.26208442  0.12359322 -0.1123555  -0.0350733   0.38807956  0.01827913\n",
      "  -0.21843464 -0.00298318  0.13060198  0.20982095]\n",
      " [-0.16009985  0.09530191 -0.0996798  -0.00805891  0.36616792  0.00974815\n",
      "  -0.17657568 -0.00148262  0.13147062  0.17863403]\n",
      " [-0.20231345  0.10337988 -0.17679867  0.05752091  0.34736497 -0.00959668\n",
      "  -0.1904667  -0.00090355  0.12189703  0.1582936 ]\n",
      " [-0.2328481   0.06610216  0.00269448  0.12086811  0.42449915  0.03474641\n",
      "  -0.12188486 -0.0046818   0.08928466  0.17146715]\n",
      " [-0.25416435  0.13397439 -0.17267386  0.01881143  0.40407     0.01572802\n",
      "  -0.30246928 -0.00157162  0.17562135  0.18961318]\n",
      " [-0.18529212  0.04885819 -0.06132809  0.10952254  0.42513228  0.03436801\n",
      "  -0.09217294 -0.00175755  0.10920363  0.12910401]\n",
      " [-0.2649687   0.18198574 -0.06355394 -0.03889779  0.25625495  0.03025404\n",
      "  -0.26337129 -0.00298908  0.12798115  0.23181765]\n",
      " [-0.16339058  0.02792025 -0.28834859  0.04769521  0.40013512 -0.00325146\n",
      "  -0.11264743 -0.00160993  0.09721302  0.10062269]\n",
      " [-0.25482132  0.14854597 -0.09071459 -0.02748132  0.35998015  0.01638157\n",
      "  -0.20469637 -0.00302285  0.1187577   0.2242183 ]]\n"
     ]
    }
   ],
   "source": [
    "print ast_embeddings_all_hocs[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
