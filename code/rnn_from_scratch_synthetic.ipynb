{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\"\"\"\n",
    "Implements an RNN on a synthetic data set, following the architecture \n",
    "described in \"Deep Knowledge Tracing\" by Chris Piech et al.\n",
    "The RNN implementation is based on min-char-rnn.py by Andrej Karpathy (@karpathy).\n",
    "BSD License\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "# allows plots to show inline in ipython notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# our own modules\n",
    "import utils\n",
    "\n",
    "synthetic_data_set = \"syntheticDetailed/naive_c5_q50_s4000_v0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Vectorization done!\n"
     ]
    }
   ],
   "source": [
    "# Read in the data set\n",
    "# This function can be moved to utils.py\n",
    "data_array = np.array(list(csv.reader(open(synthetic_data_set,\"rb\"),delimiter=','))).astype('int')\n",
    "data_array = data_array[:500]\n",
    "num_samples = data_array.shape[0]\n",
    "num_problems = data_array.shape[1]\n",
    "\n",
    "# time steps is number of problems - 1 because we cannot predict on the last problem.\n",
    "num_timesteps = num_problems - 1 \n",
    "# Split data into train and test (half and half)\n",
    "train = data_array[0:num_samples/2,:]\n",
    "test = data_array[num_samples/2:num_samples,:]\n",
    "\n",
    "num_train = train.shape[0]\n",
    "num_test = test.shape[0]\n",
    "\n",
    "print('Vectorization...')\n",
    "X_train, y_train, corr_train = utils.vectorize_syn_data(train, num_timesteps)\n",
    "X_test, y_test, corr_test = utils.vectorize_syn_data(test, num_timesteps)\n",
    "print (\"Vectorization done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 200 # size of hidden layer of neurons\n",
    "learning_rate = 1e-1\n",
    "epochs = 40\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, num_problems * 2)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(num_problems, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((num_problems, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, correctness, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps, ps_denom = {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = inputs[t,:].reshape((num_problems * 2, 1))\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps_denom[t] = np.sum(np.exp(ys[t]))\n",
    "        ps[t] = np.exp(ys[t]) / ps_denom[t] # probabilities for next chars\n",
    "\n",
    "        # softmax (cross-entropy loss)\n",
    "        if correctness[targets[t]] == 1:\n",
    "            loss += -np.log(ps[t][targets[t],0]) \n",
    "        else:\n",
    "            loss += -np.log(1-ps[t][targets[t],0]) \n",
    "        # backward pass: compute gradients going backwards\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "        dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        if correctness[targets[t]] == 1:\n",
    "            dy[targets[t]] -= 1 # backprop into y\n",
    "        else:\n",
    "            for p in xrange(num_problems):\n",
    "                if p != targets[t]:\n",
    "                    dy[p] -= np.exp(ys[t][p]) / (ps_denom[t] - np.exp(ys[t][targets[t]]))\n",
    "\n",
    "\n",
    "\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(ps, targets, correctness):\n",
    "    \"\"\"\n",
    "    Computes the accuracy using the predictions at each time step.\n",
    "    For each t, if probability of next problem is > 0.5 for correct, or <= 0.5 \n",
    "    for incorrect, then count this as correct prediction.\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    for t in xrange(num_timesteps):\n",
    "        predicted_prob = ps[t][targets[t],0] \n",
    "        if (predicted_prob >= 0.5 and correctness[targets[t]] == 1) or (predicted_prob < 0.5 and correctness[targets[t]] == 0):\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / float(num_timesteps)\n",
    "    return accuracy\n",
    "\n",
    "def forward_pass(inputs):\n",
    "    xs, hs, ys, ps, ps_denom = {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = inputs[t,:].reshape((num_problems * 2, 1))\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps_denom[t] = np.sum(np.exp(ys[t]))\n",
    "        ps[t] = np.exp(ys[t]) / ps_denom[t] # probabilities for next chars\n",
    "    return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_x_y_corr_for_sample(X, y, corr):\n",
    "    num_timesteps = X.shape[1]\n",
    "    num_problems = corr.shape[1]\n",
    "    inputs = X[i,:,:].reshape((num_timesteps, num_problems * 2))\n",
    "    targets = y[i,:].reshape((num_timesteps,))\n",
    "    correctness = corr[i,:].reshape((num_problems))\n",
    "    return inputs, targets, correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, iter 20, loss: 306.298537, train acc: 0.102041\n",
      "epoch 0, iter 40, loss: 161.828249, train acc: 0.448980\n",
      "epoch 0, iter 60, loss: 172.151292, train acc: 0.346939\n",
      "epoch 0, iter 80, loss: 175.268805, train acc: 0.061224\n",
      "epoch 0, iter 100, loss: 134.441745, train acc: 0.612245\n",
      "epoch 0, iter 120, loss: 124.909992, train acc: 0.387755\n",
      "epoch 0, iter 140, loss: 119.775909, train acc: 0.469388\n",
      "epoch 0, iter 160, loss: 125.154584, train acc: 0.530612\n",
      "epoch 0, iter 180, loss: 152.902720, train acc: 0.204082\n",
      "epoch 0, iter 200, loss: 117.893093, train acc: 0.551020\n",
      "epoch 0, iter 220, loss: 113.688187, train acc: 0.653061\n",
      "epoch 0, iter 240, loss: 112.579918, train acc: 0.265306\n",
      "epoch 0, train acc: 0.380245, test acc: 0.397714\n",
      "epoch 1, iter 20, loss: 136.607899, train acc: 0.102041\n",
      "epoch 1, iter 40, loss: 89.412291, train acc: 0.448980\n",
      "epoch 1, iter 60, loss: 96.575266, train acc: 0.346939\n",
      "epoch 1, iter 80, loss: 92.889443, train acc: 0.122449\n",
      "epoch 1, iter 100, loss: 64.893429, train acc: 0.591837\n",
      "epoch 1, iter 120, loss: 53.359943, train acc: 0.408163\n",
      "epoch 1, iter 140, loss: 54.067873, train acc: 0.489796\n",
      "epoch 1, iter 160, loss: 49.856154, train acc: 0.469388\n",
      "epoch 1, iter 180, loss: 50.083847, train acc: 0.408163\n",
      "epoch 1, iter 200, loss: 41.783750, train acc: 0.510204\n",
      "epoch 1, iter 220, loss: 35.580716, train acc: 0.469388\n",
      "epoch 1, iter 240, loss: 41.581584, train acc: 0.571429\n",
      "epoch 1, train acc: 0.425714, test acc: 0.529633\n",
      "epoch 2, iter 20, loss: 35.580469, train acc: 0.469388\n",
      "epoch 2, iter 40, loss: 36.211661, train acc: 0.428571\n",
      "epoch 2, iter 60, loss: 32.989933, train acc: 0.612245\n",
      "epoch 2, iter 80, loss: 35.085023, train acc: 0.653061\n",
      "epoch 2, iter 100, loss: 34.692868, train acc: 0.530612\n",
      "epoch 2, iter 120, loss: 39.627976, train acc: 0.448980\n",
      "epoch 2, iter 140, loss: 36.051966, train acc: 0.408163\n",
      "epoch 2, iter 160, loss: 37.444832, train acc: 0.489796\n",
      "epoch 2, iter 180, loss: 34.494255, train acc: 0.632653\n",
      "epoch 2, iter 200, loss: 35.100393, train acc: 0.551020\n",
      "epoch 2, iter 220, loss: 33.212048, train acc: 0.387755\n",
      "epoch 2, iter 240, loss: 38.016448, train acc: 0.571429\n",
      "epoch 2, train acc: 0.544327, test acc: 0.563592\n",
      "epoch 3, iter 20, loss: 32.501339, train acc: 0.653061\n",
      "epoch 3, iter 40, loss: 37.097222, train acc: 0.428571\n",
      "epoch 3, iter 60, loss: 31.387826, train acc: 0.591837\n",
      "epoch 3, iter 80, loss: 34.742389, train acc: 0.775510\n",
      "epoch 3, iter 100, loss: 35.103838, train acc: 0.551020\n",
      "epoch 3, iter 120, loss: 37.378955, train acc: 0.448980\n",
      "epoch 3, iter 140, loss: 34.952894, train acc: 0.632653\n",
      "epoch 3, iter 160, loss: 37.211809, train acc: 0.469388\n",
      "epoch 3, iter 180, loss: 32.083493, train acc: 0.755102\n",
      "epoch 3, iter 200, loss: 34.740436, train acc: 0.510204\n",
      "epoch 3, iter 220, loss: 30.908440, train acc: 0.387755\n",
      "epoch 3, iter 240, loss: 37.248999, train acc: 0.653061\n",
      "epoch 3, train acc: 0.573714, test acc: 0.575020\n",
      "epoch 4, iter 20, loss: 32.259210, train acc: 0.673469\n",
      "epoch 4, iter 40, loss: 33.307574, train acc: 0.346939\n",
      "epoch 4, iter 60, loss: 30.639754, train acc: 0.612245\n",
      "epoch 4, iter 80, loss: 33.135846, train acc: 0.714286\n",
      "epoch 4, iter 100, loss: 36.248525, train acc: 0.591837\n",
      "epoch 4, iter 120, loss: 39.417119, train acc: 0.408163\n",
      "epoch 4, iter 140, loss: 35.496359, train acc: 0.571429\n",
      "epoch 4, iter 160, loss: 35.230854, train acc: 0.428571\n",
      "epoch 4, iter 180, loss: 32.200995, train acc: 0.734694\n",
      "epoch 4, iter 200, loss: 34.861006, train acc: 0.448980\n",
      "epoch 4, iter 220, loss: 31.672124, train acc: 0.408163\n",
      "epoch 4, iter 240, loss: 35.519929, train acc: 0.612245\n",
      "epoch 4, train acc: 0.579592, test acc: 0.590041\n",
      "epoch 5, iter 20, loss: 31.027534, train acc: 0.755102\n",
      "epoch 5, iter 40, loss: 33.720904, train acc: 0.306122\n",
      "epoch 5, iter 60, loss: 30.251197, train acc: 0.551020\n",
      "epoch 5, iter 80, loss: 32.821003, train acc: 0.714286\n",
      "epoch 5, iter 100, loss: 33.423834, train acc: 0.632653\n",
      "epoch 5, iter 120, loss: 37.504673, train acc: 0.489796\n",
      "epoch 5, iter 140, loss: 34.084145, train acc: 0.632653\n",
      "epoch 5, iter 160, loss: 35.265062, train acc: 0.408163\n",
      "epoch 5, iter 180, loss: 31.594408, train acc: 0.714286\n",
      "epoch 5, iter 200, loss: 33.519274, train acc: 0.489796\n",
      "epoch 5, iter 220, loss: 29.807003, train acc: 0.387755\n",
      "epoch 5, iter 240, loss: 34.880955, train acc: 0.612245\n",
      "epoch 5, train acc: 0.585796, test acc: 0.595755\n",
      "epoch 6, iter 20, loss: 30.023427, train acc: 0.775510\n",
      "epoch 6, iter 40, loss: 33.601982, train acc: 0.367347\n",
      "epoch 6, iter 60, loss: 29.903641, train acc: 0.551020\n",
      "epoch 6, iter 80, loss: 31.493322, train acc: 0.755102\n",
      "epoch 6, iter 100, loss: 32.387321, train acc: 0.571429\n",
      "epoch 6, iter 120, loss: 36.713519, train acc: 0.530612\n",
      "epoch 6, iter 140, loss: 32.895123, train acc: 0.653061\n",
      "epoch 6, iter 160, loss: 33.416813, train acc: 0.510204\n",
      "epoch 6, iter 180, loss: 30.145337, train acc: 0.734694\n",
      "epoch 6, iter 200, loss: 32.937486, train acc: 0.489796\n",
      "epoch 6, iter 220, loss: 29.367733, train acc: 0.489796\n",
      "epoch 6, iter 240, loss: 33.043640, train acc: 0.673469\n",
      "epoch 6, train acc: 0.596163, test acc: 0.606694\n",
      "epoch 7, iter 20, loss: 29.237167, train acc: 0.714286\n",
      "epoch 7, iter 40, loss: 32.501698, train acc: 0.346939\n",
      "epoch 7, iter 60, loss: 29.295692, train acc: 0.591837\n",
      "epoch 7, iter 80, loss: 30.678489, train acc: 0.775510\n",
      "epoch 7, iter 100, loss: 31.856748, train acc: 0.591837\n",
      "epoch 7, iter 120, loss: 35.771590, train acc: 0.551020\n",
      "epoch 7, iter 140, loss: 32.109480, train acc: 0.632653\n",
      "epoch 7, iter 160, loss: 32.811100, train acc: 0.448980\n",
      "epoch 7, iter 180, loss: 29.830038, train acc: 0.714286\n",
      "epoch 7, iter 200, loss: 32.241183, train acc: 0.510204\n",
      "epoch 7, iter 220, loss: 28.818010, train acc: 0.469388\n",
      "epoch 7, iter 240, loss: 32.479027, train acc: 0.612245\n",
      "epoch 7, train acc: 0.599510, test acc: 0.589469\n",
      "epoch 8, iter 20, loss: 28.592698, train acc: 0.714286\n",
      "epoch 8, iter 40, loss: 32.604871, train acc: 0.367347\n",
      "epoch 8, iter 60, loss: 28.866755, train acc: 0.591837\n",
      "epoch 8, iter 80, loss: 29.808539, train acc: 0.816327\n",
      "epoch 8, iter 100, loss: 31.818260, train acc: 0.551020\n",
      "epoch 8, iter 120, loss: 35.036871, train acc: 0.510204\n",
      "epoch 8, iter 140, loss: 31.461369, train acc: 0.693878\n",
      "epoch 8, iter 160, loss: 32.606737, train acc: 0.510204\n",
      "epoch 8, iter 180, loss: 29.474645, train acc: 0.714286\n",
      "epoch 8, iter 200, loss: 32.753012, train acc: 0.489796\n",
      "epoch 8, iter 220, loss: 29.219285, train acc: 0.469388\n",
      "epoch 8, iter 240, loss: 31.533455, train acc: 0.632653\n",
      "epoch 8, train acc: 0.605061, test acc: 0.587510\n",
      "epoch 9, iter 20, loss: 28.195942, train acc: 0.734694\n",
      "epoch 9, iter 40, loss: 33.018893, train acc: 0.408163\n",
      "epoch 9, iter 60, loss: 28.552127, train acc: 0.612245\n",
      "epoch 9, iter 80, loss: 29.302655, train acc: 0.795918\n",
      "epoch 9, iter 100, loss: 31.600881, train acc: 0.530612\n",
      "epoch 9, iter 120, loss: 34.203131, train acc: 0.530612\n",
      "epoch 9, iter 140, loss: 31.503400, train acc: 0.734694\n",
      "epoch 9, iter 160, loss: 33.059414, train acc: 0.530612\n",
      "epoch 9, iter 180, loss: 29.170071, train acc: 0.714286\n",
      "epoch 9, iter 200, loss: 31.610464, train acc: 0.428571\n",
      "epoch 9, iter 220, loss: 29.438793, train acc: 0.448980\n",
      "epoch 9, iter 240, loss: 30.776605, train acc: 0.632653\n",
      "epoch 9, train acc: 0.604082, test acc: 0.612082\n",
      "epoch 10, iter 20, loss: 27.065780, train acc: 0.693878\n",
      "epoch 10, iter 40, loss: 32.375814, train acc: 0.387755\n",
      "epoch 10, iter 60, loss: 28.709697, train acc: 0.612245\n",
      "epoch 10, iter 80, loss: 28.974074, train acc: 0.795918\n",
      "epoch 10, iter 100, loss: 31.949263, train acc: 0.551020\n",
      "epoch 10, iter 120, loss: 35.231390, train acc: 0.551020\n",
      "epoch 10, iter 140, loss: 31.288325, train acc: 0.673469\n",
      "epoch 10, iter 160, loss: 32.169210, train acc: 0.489796\n",
      "epoch 10, iter 180, loss: 28.965954, train acc: 0.693878\n",
      "epoch 10, iter 200, loss: 31.670188, train acc: 0.489796\n",
      "epoch 10, iter 220, loss: 28.525007, train acc: 0.448980\n",
      "epoch 10, iter 240, loss: 30.963486, train acc: 0.653061\n",
      "epoch 10, train acc: 0.608980, test acc: 0.591592\n",
      "epoch 11, iter 20, loss: 27.545731, train acc: 0.755102\n",
      "epoch 11, iter 40, loss: 31.062623, train acc: 0.469388\n",
      "epoch 11, iter 60, loss: 28.772912, train acc: 0.551020\n",
      "epoch 11, iter 80, loss: 29.134740, train acc: 0.816327\n",
      "epoch 11, iter 100, loss: 30.932626, train acc: 0.510204\n",
      "epoch 11, iter 120, loss: 34.756207, train acc: 0.571429\n",
      "epoch 11, iter 140, loss: 31.548741, train acc: 0.693878\n",
      "epoch 11, iter 160, loss: 31.140972, train acc: 0.489796\n",
      "epoch 11, iter 180, loss: 28.971363, train acc: 0.714286\n",
      "epoch 11, iter 200, loss: 31.303581, train acc: 0.428571\n",
      "epoch 11, iter 220, loss: 27.843625, train acc: 0.428571\n",
      "epoch 11, iter 240, loss: 29.938866, train acc: 0.673469\n",
      "epoch 11, train acc: 0.610041, test acc: 0.611102\n",
      "epoch 12, iter 20, loss: 27.005472, train acc: 0.755102\n",
      "epoch 12, iter 40, loss: 31.296848, train acc: 0.428571\n",
      "epoch 12, iter 60, loss: 28.533797, train acc: 0.551020\n",
      "epoch 12, iter 80, loss: 29.182260, train acc: 0.795918\n",
      "epoch 12, iter 100, loss: 32.092104, train acc: 0.571429\n",
      "epoch 12, iter 120, loss: 35.992638, train acc: 0.551020\n",
      "epoch 12, iter 140, loss: 31.273467, train acc: 0.693878\n",
      "epoch 12, iter 160, loss: 31.057657, train acc: 0.551020\n",
      "epoch 12, iter 180, loss: 28.891567, train acc: 0.755102\n",
      "epoch 12, iter 200, loss: 31.255685, train acc: 0.489796\n",
      "epoch 12, iter 220, loss: 27.531459, train acc: 0.408163\n",
      "epoch 12, iter 240, loss: 30.502258, train acc: 0.591837\n",
      "epoch 12, train acc: 0.612653, test acc: 0.612490\n",
      "epoch 13, iter 20, loss: 27.482938, train acc: 0.755102\n",
      "epoch 13, iter 40, loss: 31.344714, train acc: 0.448980\n",
      "epoch 13, iter 60, loss: 27.948418, train acc: 0.571429\n",
      "epoch 13, iter 80, loss: 28.282753, train acc: 0.775510\n",
      "epoch 13, iter 100, loss: 31.709845, train acc: 0.571429\n",
      "epoch 13, iter 120, loss: 33.639098, train acc: 0.551020\n",
      "epoch 13, iter 140, loss: 31.118679, train acc: 0.653061\n",
      "epoch 13, iter 160, loss: 30.273072, train acc: 0.530612\n",
      "epoch 13, iter 180, loss: 28.722274, train acc: 0.755102\n",
      "epoch 13, iter 200, loss: 31.636004, train acc: 0.448980\n",
      "epoch 13, iter 220, loss: 26.993859, train acc: 0.469388\n",
      "epoch 13, iter 240, loss: 30.247637, train acc: 0.693878\n",
      "epoch 13, train acc: 0.614694, test acc: 0.608082\n",
      "epoch 14, iter 20, loss: 26.687189, train acc: 0.795918\n",
      "epoch 14, iter 40, loss: 31.321059, train acc: 0.448980\n",
      "epoch 14, iter 60, loss: 27.539050, train acc: 0.571429\n",
      "epoch 14, iter 80, loss: 27.784396, train acc: 0.775510\n",
      "epoch 14, iter 100, loss: 31.040679, train acc: 0.551020\n",
      "epoch 14, iter 120, loss: 33.697470, train acc: 0.591837\n",
      "epoch 14, iter 140, loss: 30.209539, train acc: 0.673469\n",
      "epoch 14, iter 160, loss: 30.094303, train acc: 0.551020\n",
      "epoch 14, iter 180, loss: 28.492625, train acc: 0.755102\n",
      "epoch 14, iter 200, loss: 30.948705, train acc: 0.469388\n",
      "epoch 14, iter 220, loss: 26.409647, train acc: 0.489796\n",
      "epoch 14, iter 240, loss: 29.466197, train acc: 0.714286\n",
      "epoch 14, train acc: 0.612735, test acc: 0.616245\n",
      "epoch 15, iter 20, loss: 25.972780, train acc: 0.816327\n",
      "epoch 15, iter 40, loss: 30.615827, train acc: 0.428571\n",
      "epoch 15, iter 60, loss: 27.363655, train acc: 0.632653\n",
      "epoch 15, iter 80, loss: 27.800068, train acc: 0.795918\n",
      "epoch 15, iter 100, loss: 30.571906, train acc: 0.612245\n",
      "epoch 15, iter 120, loss: 33.688713, train acc: 0.571429\n",
      "epoch 15, iter 140, loss: 29.653357, train acc: 0.693878\n",
      "epoch 15, iter 160, loss: 29.505905, train acc: 0.489796\n",
      "epoch 15, iter 180, loss: 28.119230, train acc: 0.775510\n",
      "epoch 15, iter 200, loss: 31.044592, train acc: 0.408163\n",
      "epoch 15, iter 220, loss: 26.968490, train acc: 0.530612\n",
      "epoch 15, iter 240, loss: 28.869000, train acc: 0.714286\n",
      "epoch 15, train acc: 0.615837, test acc: 0.620082\n",
      "epoch 16, iter 20, loss: 26.104063, train acc: 0.795918\n",
      "epoch 16, iter 40, loss: 31.326687, train acc: 0.428571\n",
      "epoch 16, iter 60, loss: 27.879067, train acc: 0.571429\n",
      "epoch 16, iter 80, loss: 27.745453, train acc: 0.795918\n",
      "epoch 16, iter 100, loss: 31.028951, train acc: 0.530612\n",
      "epoch 16, iter 120, loss: 33.196868, train acc: 0.612245\n",
      "epoch 16, iter 140, loss: 30.273242, train acc: 0.673469\n",
      "epoch 16, iter 160, loss: 29.057562, train acc: 0.489796\n",
      "epoch 16, iter 180, loss: 27.794035, train acc: 0.795918\n",
      "epoch 16, iter 200, loss: 30.999985, train acc: 0.408163\n",
      "epoch 16, iter 220, loss: 26.697909, train acc: 0.428571\n",
      "epoch 16, iter 240, loss: 28.522530, train acc: 0.632653\n",
      "epoch 16, train acc: 0.614857, test acc: 0.612898\n",
      "epoch 17, iter 20, loss: 25.663584, train acc: 0.775510\n",
      "epoch 17, iter 40, loss: 30.786667, train acc: 0.428571\n",
      "epoch 17, iter 60, loss: 27.973849, train acc: 0.653061\n",
      "epoch 17, iter 80, loss: 26.796457, train acc: 0.816327\n",
      "epoch 17, iter 100, loss: 30.956913, train acc: 0.571429\n",
      "epoch 17, iter 120, loss: 32.555819, train acc: 0.591837\n",
      "epoch 17, iter 140, loss: 30.018021, train acc: 0.693878\n",
      "epoch 17, iter 160, loss: 30.011798, train acc: 0.448980\n",
      "epoch 17, iter 180, loss: 27.468255, train acc: 0.775510\n",
      "epoch 17, iter 200, loss: 29.905998, train acc: 0.428571\n",
      "epoch 17, iter 220, loss: 25.973242, train acc: 0.448980\n",
      "epoch 17, iter 240, loss: 28.132005, train acc: 0.612245\n",
      "epoch 17, train acc: 0.617143, test acc: 0.617143\n",
      "epoch 18, iter 20, loss: 25.859207, train acc: 0.795918\n",
      "epoch 18, iter 40, loss: 31.625575, train acc: 0.489796\n",
      "epoch 18, iter 60, loss: 26.790780, train acc: 0.673469\n",
      "epoch 18, iter 80, loss: 26.740618, train acc: 0.795918\n",
      "epoch 18, iter 100, loss: 30.965598, train acc: 0.653061\n",
      "epoch 18, iter 120, loss: 32.207672, train acc: 0.551020\n",
      "epoch 18, iter 140, loss: 29.936408, train acc: 0.755102\n",
      "epoch 18, iter 160, loss: 29.070920, train acc: 0.489796\n",
      "epoch 18, iter 180, loss: 26.797748, train acc: 0.795918\n",
      "epoch 18, iter 200, loss: 29.788919, train acc: 0.469388\n",
      "epoch 18, iter 220, loss: 25.897605, train acc: 0.469388\n",
      "epoch 18, iter 240, loss: 28.092209, train acc: 0.653061\n",
      "epoch 18, train acc: 0.620245, test acc: 0.625959\n",
      "epoch 19, iter 20, loss: 25.973463, train acc: 0.775510\n",
      "epoch 19, iter 40, loss: 30.651228, train acc: 0.428571\n",
      "epoch 19, iter 60, loss: 26.876972, train acc: 0.693878\n",
      "epoch 19, iter 80, loss: 26.752065, train acc: 0.795918\n",
      "epoch 19, iter 100, loss: 30.047007, train acc: 0.612245\n",
      "epoch 19, iter 120, loss: 31.824119, train acc: 0.612245\n",
      "epoch 19, iter 140, loss: 30.162836, train acc: 0.734694\n",
      "epoch 19, iter 160, loss: 30.412639, train acc: 0.469388\n",
      "epoch 19, iter 180, loss: 26.937590, train acc: 0.755102\n",
      "epoch 19, iter 200, loss: 30.483436, train acc: 0.551020\n",
      "epoch 19, iter 220, loss: 25.641496, train acc: 0.448980\n",
      "epoch 19, iter 240, loss: 28.463176, train acc: 0.673469\n",
      "epoch 19, train acc: 0.619184, test acc: 0.639102\n",
      "epoch 20, iter 20, loss: 25.119031, train acc: 0.795918\n",
      "epoch 20, iter 40, loss: 30.188476, train acc: 0.448980\n",
      "epoch 20, iter 60, loss: 26.212539, train acc: 0.714286\n",
      "epoch 20, iter 80, loss: 26.643111, train acc: 0.795918\n",
      "epoch 20, iter 100, loss: 29.441175, train acc: 0.591837\n",
      "epoch 20, iter 120, loss: 31.241257, train acc: 0.530612\n",
      "epoch 20, iter 140, loss: 28.907909, train acc: 0.734694\n",
      "epoch 20, iter 160, loss: 29.089370, train acc: 0.469388\n",
      "epoch 20, iter 180, loss: 26.738854, train acc: 0.795918\n",
      "epoch 20, iter 200, loss: 31.099215, train acc: 0.530612\n",
      "epoch 20, iter 220, loss: 25.925104, train acc: 0.510204\n",
      "epoch 20, iter 240, loss: 28.514900, train acc: 0.673469\n",
      "epoch 20, train acc: 0.622122, test acc: 0.616245\n",
      "epoch 21, iter 20, loss: 25.415609, train acc: 0.775510\n",
      "epoch 21, iter 40, loss: 30.815261, train acc: 0.326531\n",
      "epoch 21, iter 60, loss: 26.150994, train acc: 0.653061\n",
      "epoch 21, iter 80, loss: 25.865924, train acc: 0.816327\n",
      "epoch 21, iter 100, loss: 29.281096, train acc: 0.591837\n",
      "epoch 21, iter 120, loss: 32.092128, train acc: 0.530612\n",
      "epoch 21, iter 140, loss: 29.968722, train acc: 0.673469\n",
      "epoch 21, iter 160, loss: 28.925387, train acc: 0.510204\n",
      "epoch 21, iter 180, loss: 26.435643, train acc: 0.795918\n",
      "epoch 21, iter 200, loss: 29.580609, train acc: 0.469388\n",
      "epoch 21, iter 220, loss: 25.734797, train acc: 0.510204\n",
      "epoch 21, iter 240, loss: 27.234899, train acc: 0.632653\n",
      "epoch 21, train acc: 0.620735, test acc: 0.604653\n",
      "epoch 22, iter 20, loss: 26.086558, train acc: 0.775510\n",
      "epoch 22, iter 40, loss: 32.182512, train acc: 0.408163\n",
      "epoch 22, iter 60, loss: 27.320227, train acc: 0.632653\n",
      "epoch 22, iter 80, loss: 27.356425, train acc: 0.816327\n",
      "epoch 22, iter 100, loss: 30.174542, train acc: 0.632653\n",
      "epoch 22, iter 120, loss: 32.740693, train acc: 0.551020\n",
      "epoch 22, iter 140, loss: 29.273181, train acc: 0.673469\n",
      "epoch 22, iter 160, loss: 29.564742, train acc: 0.428571\n",
      "epoch 22, iter 180, loss: 26.607521, train acc: 0.775510\n",
      "epoch 22, iter 200, loss: 29.965015, train acc: 0.489796\n",
      "epoch 22, iter 220, loss: 25.519424, train acc: 0.489796\n",
      "epoch 22, iter 240, loss: 27.633169, train acc: 0.571429\n",
      "epoch 22, train acc: 0.620898, test acc: 0.630939\n",
      "epoch 23, iter 20, loss: 24.557505, train acc: 0.795918\n",
      "epoch 23, iter 40, loss: 31.045618, train acc: 0.428571\n",
      "epoch 23, iter 60, loss: 26.518349, train acc: 0.693878\n",
      "epoch 23, iter 80, loss: 26.565179, train acc: 0.816327\n",
      "epoch 23, iter 100, loss: 31.089347, train acc: 0.755102\n",
      "epoch 23, iter 120, loss: 32.927527, train acc: 0.530612\n",
      "epoch 23, iter 140, loss: 29.366979, train acc: 0.734694\n",
      "epoch 23, iter 160, loss: 30.410912, train acc: 0.408163\n",
      "epoch 23, iter 180, loss: 26.614614, train acc: 0.795918\n",
      "epoch 23, iter 200, loss: 29.462936, train acc: 0.469388\n",
      "epoch 23, iter 220, loss: 25.185177, train acc: 0.510204\n",
      "epoch 23, iter 240, loss: 27.237221, train acc: 0.673469\n",
      "epoch 23, train acc: 0.623755, test acc: 0.634286\n",
      "epoch 24, iter 20, loss: 24.282444, train acc: 0.816327\n",
      "epoch 24, iter 40, loss: 31.172851, train acc: 0.346939\n",
      "epoch 24, iter 60, loss: 26.497053, train acc: 0.693878\n",
      "epoch 24, iter 80, loss: 25.982124, train acc: 0.795918\n",
      "epoch 24, iter 100, loss: 29.138411, train acc: 0.591837\n",
      "epoch 24, iter 120, loss: 31.610989, train acc: 0.673469\n",
      "epoch 24, iter 140, loss: 28.493559, train acc: 0.673469\n",
      "epoch 24, iter 160, loss: 28.857924, train acc: 0.530612\n",
      "epoch 24, iter 180, loss: 26.386825, train acc: 0.795918\n",
      "epoch 24, iter 200, loss: 30.016198, train acc: 0.489796\n",
      "epoch 24, iter 220, loss: 25.005759, train acc: 0.551020\n",
      "epoch 24, iter 240, loss: 27.561928, train acc: 0.653061\n",
      "epoch 24, train acc: 0.624163, test acc: 0.629306\n",
      "epoch 25, iter 20, loss: 24.243048, train acc: 0.795918\n",
      "epoch 25, iter 40, loss: 29.744684, train acc: 0.428571\n",
      "epoch 25, iter 60, loss: 27.049914, train acc: 0.673469\n",
      "epoch 25, iter 80, loss: 25.820298, train acc: 0.816327\n",
      "epoch 25, iter 100, loss: 30.318097, train acc: 0.612245\n",
      "epoch 25, iter 120, loss: 32.123591, train acc: 0.571429\n",
      "epoch 25, iter 140, loss: 27.076195, train acc: 0.734694\n",
      "epoch 25, iter 160, loss: 28.752637, train acc: 0.469388\n",
      "epoch 25, iter 180, loss: 26.688596, train acc: 0.775510\n",
      "epoch 25, iter 200, loss: 29.910591, train acc: 0.510204\n",
      "epoch 25, iter 220, loss: 24.886404, train acc: 0.489796\n",
      "epoch 25, iter 240, loss: 27.208386, train acc: 0.653061\n",
      "epoch 25, train acc: 0.618041, test acc: 0.610041\n",
      "epoch 26, iter 20, loss: 24.382866, train acc: 0.836735\n",
      "epoch 26, iter 40, loss: 30.353582, train acc: 0.387755\n",
      "epoch 26, iter 60, loss: 26.238411, train acc: 0.673469\n",
      "epoch 26, iter 80, loss: 25.391615, train acc: 0.795918\n",
      "epoch 26, iter 100, loss: 29.114137, train acc: 0.530612\n",
      "epoch 26, iter 120, loss: 33.097028, train acc: 0.551020\n",
      "epoch 26, iter 140, loss: 28.113275, train acc: 0.693878\n",
      "epoch 26, iter 160, loss: 29.018881, train acc: 0.510204\n",
      "epoch 26, iter 180, loss: 26.860342, train acc: 0.795918\n",
      "epoch 26, iter 200, loss: 30.356268, train acc: 0.489796\n",
      "epoch 26, iter 220, loss: 25.113938, train acc: 0.510204\n",
      "epoch 26, iter 240, loss: 26.799290, train acc: 0.693878\n",
      "epoch 26, train acc: 0.625878, test acc: 0.620653\n",
      "epoch 27, iter 20, loss: 23.966300, train acc: 0.775510\n",
      "epoch 27, iter 40, loss: 29.216169, train acc: 0.408163\n",
      "epoch 27, iter 60, loss: 25.610926, train acc: 0.653061\n",
      "epoch 27, iter 80, loss: 24.943700, train acc: 0.816327\n",
      "epoch 27, iter 100, loss: 27.756701, train acc: 0.673469\n",
      "epoch 27, iter 120, loss: 31.329958, train acc: 0.551020\n",
      "epoch 27, iter 140, loss: 28.538288, train acc: 0.693878\n",
      "epoch 27, iter 160, loss: 27.891620, train acc: 0.489796\n",
      "epoch 27, iter 180, loss: 27.711846, train acc: 0.816327\n",
      "epoch 27, iter 200, loss: 28.425801, train acc: 0.510204\n",
      "epoch 27, iter 220, loss: 24.903626, train acc: 0.510204\n",
      "epoch 27, iter 240, loss: 26.277937, train acc: 0.653061\n",
      "epoch 27, train acc: 0.624735, test acc: 0.621388\n",
      "epoch 28, iter 20, loss: 23.616417, train acc: 0.795918\n",
      "epoch 28, iter 40, loss: 29.100177, train acc: 0.367347\n",
      "epoch 28, iter 60, loss: 25.429118, train acc: 0.673469\n",
      "epoch 28, iter 80, loss: 24.543967, train acc: 0.816327\n",
      "epoch 28, iter 100, loss: 28.118267, train acc: 0.693878\n",
      "epoch 28, iter 120, loss: 31.012611, train acc: 0.551020\n",
      "epoch 28, iter 140, loss: 27.627001, train acc: 0.714286\n",
      "epoch 28, iter 160, loss: 27.802045, train acc: 0.551020\n",
      "epoch 28, iter 180, loss: 27.077117, train acc: 0.775510\n",
      "epoch 28, iter 200, loss: 28.986589, train acc: 0.428571\n",
      "epoch 28, iter 220, loss: 24.678285, train acc: 0.530612\n",
      "epoch 28, iter 240, loss: 26.450330, train acc: 0.693878\n",
      "epoch 28, train acc: 0.621551, test acc: 0.624408\n",
      "epoch 29, iter 20, loss: 23.424545, train acc: 0.795918\n",
      "epoch 29, iter 40, loss: 29.553936, train acc: 0.489796\n",
      "epoch 29, iter 60, loss: 25.870945, train acc: 0.673469\n",
      "epoch 29, iter 80, loss: 25.143637, train acc: 0.795918\n",
      "epoch 29, iter 100, loss: 27.696991, train acc: 0.673469\n",
      "epoch 29, iter 120, loss: 32.111971, train acc: 0.591837\n",
      "epoch 29, iter 140, loss: 28.001341, train acc: 0.734694\n",
      "epoch 29, iter 160, loss: 27.817186, train acc: 0.530612\n",
      "epoch 29, iter 180, loss: 28.895926, train acc: 0.795918\n",
      "epoch 29, iter 200, loss: 30.096545, train acc: 0.469388\n",
      "epoch 29, iter 220, loss: 24.798212, train acc: 0.510204\n",
      "epoch 29, iter 240, loss: 26.952330, train acc: 0.653061\n",
      "epoch 29, train acc: 0.624653, test acc: 0.629796\n",
      "epoch 30, iter 20, loss: 23.252954, train acc: 0.816327\n",
      "epoch 30, iter 40, loss: 30.084564, train acc: 0.408163\n",
      "epoch 30, iter 60, loss: 25.827432, train acc: 0.632653\n",
      "epoch 30, iter 80, loss: 24.439064, train acc: 0.816327\n",
      "epoch 30, iter 100, loss: 27.815293, train acc: 0.714286\n",
      "epoch 30, iter 120, loss: 32.638422, train acc: 0.510204\n",
      "epoch 30, iter 140, loss: 29.368123, train acc: 0.653061\n",
      "epoch 30, iter 160, loss: 28.593749, train acc: 0.551020\n",
      "epoch 30, iter 180, loss: 26.632925, train acc: 0.755102\n",
      "epoch 30, iter 200, loss: 28.756258, train acc: 0.489796\n",
      "epoch 30, iter 220, loss: 24.583957, train acc: 0.530612\n",
      "epoch 30, iter 240, loss: 26.877925, train acc: 0.591837\n",
      "epoch 30, train acc: 0.623837, test acc: 0.614612\n",
      "epoch 31, iter 20, loss: 22.876601, train acc: 0.775510\n",
      "epoch 31, iter 40, loss: 29.258805, train acc: 0.448980\n",
      "epoch 31, iter 60, loss: 25.655975, train acc: 0.714286\n",
      "epoch 31, iter 80, loss: 24.730829, train acc: 0.816327\n",
      "epoch 31, iter 100, loss: 26.457364, train acc: 0.693878\n",
      "epoch 31, iter 120, loss: 31.994280, train acc: 0.551020\n",
      "epoch 31, iter 140, loss: 28.956653, train acc: 0.714286\n",
      "epoch 31, iter 160, loss: 28.043071, train acc: 0.530612\n",
      "epoch 31, iter 180, loss: 26.862518, train acc: 0.714286\n",
      "epoch 31, iter 200, loss: 29.911041, train acc: 0.530612\n",
      "epoch 31, iter 220, loss: 24.268454, train acc: 0.489796\n",
      "epoch 31, iter 240, loss: 26.314262, train acc: 0.612245\n",
      "epoch 31, train acc: 0.622612, test acc: 0.610857\n",
      "epoch 32, iter 20, loss: 22.539416, train acc: 0.816327\n",
      "epoch 32, iter 40, loss: 29.455586, train acc: 0.489796\n",
      "epoch 32, iter 60, loss: 26.506257, train acc: 0.653061\n",
      "epoch 32, iter 80, loss: 26.004714, train acc: 0.836735\n",
      "epoch 32, iter 100, loss: 28.211368, train acc: 0.632653\n",
      "epoch 32, iter 120, loss: 32.564579, train acc: 0.571429\n",
      "epoch 32, iter 140, loss: 28.579450, train acc: 0.612245\n",
      "epoch 32, iter 160, loss: 30.391580, train acc: 0.571429\n",
      "epoch 32, iter 180, loss: 25.890956, train acc: 0.714286\n",
      "epoch 32, iter 200, loss: 28.494965, train acc: 0.428571\n",
      "epoch 32, iter 220, loss: 24.622751, train acc: 0.571429\n",
      "epoch 32, iter 240, loss: 26.704419, train acc: 0.673469\n",
      "epoch 32, train acc: 0.622939, test acc: 0.615592\n",
      "epoch 33, iter 20, loss: 23.245876, train acc: 0.816327\n",
      "epoch 33, iter 40, loss: 29.489591, train acc: 0.408163\n",
      "epoch 33, iter 60, loss: 26.369566, train acc: 0.693878\n",
      "epoch 33, iter 80, loss: 24.712873, train acc: 0.795918\n",
      "epoch 33, iter 100, loss: 26.339533, train acc: 0.693878\n",
      "epoch 33, iter 120, loss: 30.911339, train acc: 0.591837\n",
      "epoch 33, iter 140, loss: 27.887202, train acc: 0.693878\n",
      "epoch 33, iter 160, loss: 27.737244, train acc: 0.510204\n",
      "epoch 33, iter 180, loss: 26.567738, train acc: 0.775510\n",
      "epoch 33, iter 200, loss: 28.423168, train acc: 0.469388\n",
      "epoch 33, iter 220, loss: 25.171122, train acc: 0.489796\n",
      "epoch 33, iter 240, loss: 26.929545, train acc: 0.673469\n",
      "epoch 33, train acc: 0.624816, test acc: 0.628571\n",
      "epoch 34, iter 20, loss: 22.467005, train acc: 0.795918\n",
      "epoch 34, iter 40, loss: 28.708576, train acc: 0.448980\n",
      "epoch 34, iter 60, loss: 25.459723, train acc: 0.673469\n",
      "epoch 34, iter 80, loss: 24.318862, train acc: 0.795918\n",
      "epoch 34, iter 100, loss: 25.717694, train acc: 0.734694\n",
      "epoch 34, iter 120, loss: 30.858327, train acc: 0.612245\n",
      "epoch 34, iter 140, loss: 28.507077, train acc: 0.673469\n",
      "epoch 34, iter 160, loss: 28.441583, train acc: 0.428571\n",
      "epoch 34, iter 180, loss: 25.591230, train acc: 0.734694\n",
      "epoch 34, iter 200, loss: 30.570715, train acc: 0.489796\n",
      "epoch 34, iter 220, loss: 25.110592, train acc: 0.469388\n",
      "epoch 34, iter 240, loss: 27.101613, train acc: 0.714286\n",
      "epoch 34, train acc: 0.624898, test acc: 0.618857\n",
      "epoch 35, iter 20, loss: 24.276667, train acc: 0.795918\n",
      "epoch 35, iter 40, loss: 28.316208, train acc: 0.489796\n",
      "epoch 35, iter 60, loss: 25.429350, train acc: 0.653061\n",
      "epoch 35, iter 80, loss: 24.546500, train acc: 0.775510\n",
      "epoch 35, iter 100, loss: 26.552470, train acc: 0.673469\n",
      "epoch 35, iter 120, loss: 29.778615, train acc: 0.571429\n",
      "epoch 35, iter 140, loss: 29.185166, train acc: 0.571429\n",
      "epoch 35, iter 160, loss: 29.472189, train acc: 0.510204\n",
      "epoch 35, iter 180, loss: 25.205204, train acc: 0.714286\n",
      "epoch 35, iter 200, loss: 29.629686, train acc: 0.551020\n",
      "epoch 35, iter 220, loss: 25.012406, train acc: 0.489796\n",
      "epoch 35, iter 240, loss: 26.757637, train acc: 0.734694\n",
      "epoch 35, train acc: 0.624980, test acc: 0.621306\n",
      "epoch 36, iter 20, loss: 22.939574, train acc: 0.795918\n",
      "epoch 36, iter 40, loss: 28.604114, train acc: 0.510204\n",
      "epoch 36, iter 60, loss: 25.839841, train acc: 0.653061\n",
      "epoch 36, iter 80, loss: 25.242943, train acc: 0.816327\n",
      "epoch 36, iter 100, loss: 26.492641, train acc: 0.734694\n",
      "epoch 36, iter 120, loss: 31.431189, train acc: 0.612245\n",
      "epoch 36, iter 140, loss: 28.594825, train acc: 0.632653\n",
      "epoch 36, iter 160, loss: 29.011277, train acc: 0.530612\n",
      "epoch 36, iter 180, loss: 25.551492, train acc: 0.734694\n",
      "epoch 36, iter 200, loss: 28.802688, train acc: 0.469388\n",
      "epoch 36, iter 220, loss: 23.824849, train acc: 0.469388\n",
      "epoch 36, iter 240, loss: 25.816670, train acc: 0.653061\n",
      "epoch 36, train acc: 0.628163, test acc: 0.613714\n",
      "epoch 37, iter 20, loss: 22.928741, train acc: 0.775510\n",
      "epoch 37, iter 40, loss: 28.754589, train acc: 0.469388\n",
      "epoch 37, iter 60, loss: 25.921648, train acc: 0.714286\n",
      "epoch 37, iter 80, loss: 23.662315, train acc: 0.857143\n",
      "epoch 37, iter 100, loss: 26.892610, train acc: 0.693878\n",
      "epoch 37, iter 120, loss: 32.018351, train acc: 0.551020\n",
      "epoch 37, iter 140, loss: 26.650465, train acc: 0.591837\n",
      "epoch 37, iter 160, loss: 29.110219, train acc: 0.489796\n",
      "epoch 37, iter 180, loss: 26.547484, train acc: 0.714286\n",
      "epoch 37, iter 200, loss: 29.874355, train acc: 0.469388\n",
      "epoch 37, iter 220, loss: 24.908346, train acc: 0.510204\n",
      "epoch 37, iter 240, loss: 26.067811, train acc: 0.653061\n",
      "epoch 37, train acc: 0.629143, test acc: 0.616816\n",
      "epoch 38, iter 20, loss: 22.139402, train acc: 0.816327\n",
      "epoch 38, iter 40, loss: 28.637612, train acc: 0.551020\n",
      "epoch 38, iter 60, loss: 25.320801, train acc: 0.571429\n",
      "epoch 38, iter 80, loss: 24.506240, train acc: 0.795918\n",
      "epoch 38, iter 100, loss: 27.317260, train acc: 0.693878\n",
      "epoch 38, iter 120, loss: 32.134584, train acc: 0.571429\n",
      "epoch 38, iter 140, loss: 27.265459, train acc: 0.653061\n",
      "epoch 38, iter 160, loss: 29.732961, train acc: 0.530612\n",
      "epoch 38, iter 180, loss: 26.472728, train acc: 0.734694\n",
      "epoch 38, iter 200, loss: 29.196946, train acc: 0.510204\n",
      "epoch 38, iter 220, loss: 25.900359, train acc: 0.510204\n",
      "epoch 38, iter 240, loss: 24.467696, train acc: 0.714286\n",
      "epoch 38, train acc: 0.634612, test acc: 0.613224\n",
      "epoch 39, iter 20, loss: 22.183449, train acc: 0.795918\n",
      "epoch 39, iter 40, loss: 28.388963, train acc: 0.489796\n",
      "epoch 39, iter 60, loss: 25.741632, train acc: 0.551020\n",
      "epoch 39, iter 80, loss: 24.158907, train acc: 0.816327\n",
      "epoch 39, iter 100, loss: 26.822721, train acc: 0.673469\n",
      "epoch 39, iter 120, loss: 29.894847, train acc: 0.551020\n",
      "epoch 39, iter 140, loss: 27.583254, train acc: 0.673469\n",
      "epoch 39, iter 160, loss: 27.384105, train acc: 0.591837\n",
      "epoch 39, iter 180, loss: 26.199521, train acc: 0.755102\n",
      "epoch 39, iter 200, loss: 27.214152, train acc: 0.591837\n",
      "epoch 39, iter 220, loss: 24.865314, train acc: 0.489796\n",
      "epoch 39, iter 240, loss: 25.431174, train acc: 0.693878\n",
      "epoch 39, train acc: 0.630041, test acc: 0.620163\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/num_problems)*num_timesteps # loss at iteration 0\n",
    "\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "print_batch_sz = 20\n",
    "for e in xrange(epochs):\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    total_acc_train = 0.0\n",
    "    total_acc_test = 0.0\n",
    "    for i in xrange(num_train):\n",
    "        inputs_train, targets_train, correctness_train = extract_x_y_corr_for_sample(X_train, y_train, corr_train)\n",
    "        # forward num_timesteps characters through the net and fetch gradient\n",
    "        loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs_train, targets_train, correctness_train, hprev)\n",
    "        smooth_loss = smooth_loss * 0.7 + loss * 0.3\n",
    "        losses.append(smooth_loss)\n",
    "\n",
    "        # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                      [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                      [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "        ps_train = forward_pass(inputs_train)\n",
    "        acc_train = accuracy(ps_train, targets_train, correctness_train)\n",
    "        train_accuracies.append(acc_train)\n",
    "        \n",
    "        if i%print_batch_sz == 0 and i != 0:\n",
    "            print ('epoch %d, iter %d, loss: %f, train acc: %f' % (e, i, smooth_loss, acc_train)) \n",
    "            \n",
    "        total_acc_train += acc_train\n",
    "    total_acc_train /= num_train\n",
    "        \n",
    "    for i in xrange(num_test):\n",
    "        inputs_test, targets_test, correctness_test = extract_x_y_corr_for_sample(X_test, y_test, corr_test)\n",
    "        ps_test = forward_pass(inputs_test)\n",
    "        acc_test = accuracy(ps_test, targets_test, correctness_test) \n",
    "        test_accuracies.append(acc_test)\n",
    "        total_acc_test += acc_test\n",
    "    \n",
    "    total_acc_test /= num_test\n",
    "    print ('epoch %d, train acc: %f, test acc: %f' % (e, total_acc_train, total_acc_test)) # print progress\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAaoAAAE+CAYAAADceFSjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzsnXfYXEX1xz/f9N6AkECA0AWUKkGK9F4V6SoiiHQUBBEp\n",
       "+y4qIAJiAeQHSAdp0jtIAOklCSUEEyBAIIQWSEJCSDm/P2bu7t27d99syuZ98+Z8nmef9965M/fO\n",
       "7rs7Z+acM+fIzHAcx3Gc1kq7lu6A4ziO4zSHCyrHcRynVeOCynEcx2nVuKByHMdxWjUuqBzHcZxW\n",
       "jQsqx3Ecp1XToaU70Agkuc+94zgOYGZq6T7ML21SUEHb+Oc4juPMD21l0u6qP8dxHKdV44LKcRzH\n",
       "adW4oHIcx3FaNS6oFjKSxkrapqX74ThO60LSxZJObel+tEZcUC18LL4cZ7EgTs62XgD3OUjSEwui\n",
       "T60RMzvCzH7f0v1ojbigchyn0RiwWHjhSvIxtQH4h9pCSOok6QJJ78fXnyV1iteWlHS3pImSPpX0\n",
       "eKrdSZLGSZokaVQyU1XgN5LGSPpE0o2S+sZrXSRdG8snSnpOUv+WeefO4oSka4DlgbskTZZ0Qiz/\n",
       "jqSn4vdxuKQtUm0OkvRm/I6/JekASd8A/gFsHO/zWY3n/VTSyNj2TUk/z1zfIz7vi/hb2SGW95N0\n",
       "RfwtfibptlRfnsjcY7akleLxlVFld6+kKcCWknaRNCw+411JhUz7zVLv/V1JB6bu9btUvV1jXydK\n",
       "elLSt1LXcseBNouZtblXeFst348afXsb2AY4A3gKWDK+ngTOiHXOAi4G2sfXprF8deBdYEA8Xx5Y\n",
       "KR7/It5vGaAj4Ud9fbx2GHAn0IUws10P6NnSn4W/Fo9X/M5vnTpfFvgE2DGebxvPlwC6A18Aq8Zr\n",
       "SwNrxuOfAE/M4Vk7AyvG482BL4H14vkQ4HNgm3i+DLB6PL4HuAHoTdhf+t1YflD2mcDs1O/uynjP\n",
       "jeN5Z2ALYK14/i3gQ2CPeL4CMAnYN/62+wHrxGtXpMaA9YAJwIbxN3tg/Bw7NjcO5Hwe1tL//wXx\n",
       "arMbfufIgtgIN3+big8AjjazT0J3VAQuAU4HvgYGAoPN7E2CEAOYRfghrCXpUzN7N3W/w+L9Pkjd\n",
       "7x1JP473W4Lw438FGDYf/XYWQVRcMBs/rbBANtL/CLjXzO4HMLOHJb0A7ALcQhAE35I0zswmEAZs\n",
       "qEN9aGb3po4fl/Qg8F3Cd/4Q4HIzeyReT34rA4EdgX5m9kVsPje2sNvN7Ol4z+nAY6k+vCLpXwTh\n",
       "dQfhd/+Qmd0Yq3wWX1l+DlxiZs/H86sl/RbYGHif2uNAm2TxFVQtH7liGeCd1Pm7sQzgT0AT8KAk\n",
       "gP8zsz+a2RhJv4zX1pL0AHC8mY0HBgO3SZqduudMoD9wDbAc8C9JfYBrgVPMbGaD3pvTylhAAmZB\n",
       "sQKwt6TdUmUdgP+Y2VRJ+wInAJdLehL4lZm9Uc+NJe0EFIBVCaaNbsDL8fIgwsopy3LAZykhNTcY\n",
       "MC7Th42As4G1gE4EoXJT6llv1XHfFYADJR2TKusIDIwCuNY40CZxG1XL8QFBuCQsH8swsylmdoKZ\n",
       "rQzsDhyf6KDN7AYz+y7hi2zAH2P7dwmqlL6pVzczG29mM83sDDNbC9gE2JWgSnCchUF2NfcucE3m\n",
       "u9rTzM4BMLMHzWx7YAAwCri0xn0qkNQZuBU4B+hvZn2BeymvxN4DVslp+h7QT1LvnGtfEoRd8owB\n",
       "zfUhcj1wOzDIzPoQ1PBJH94FVq7jHu8Cf8h8Rj2SlVgz40CbxAVVy3EDcGp0nFiSoPK7BkpG1FUU\n",
       "llOTCCq/WZJWk7R1/EFOB76K1yD8GM6UtHy8x1KSdo/HW0r6lqT2wGRgRqqd4zSaCVQOztcCu0na\n",
       "XlL76OyzpaRlJfWPDg/dCd/TLyl/VycAgyR1rPGcTvH1CTA7rq62T12/HPhp/A21i89bPa5E7gMu\n",
       "ktRHUkdJm8c2IwirlnUkdSGsYtLkrVR7ABPN7GtJQwjqvoTrgW0l7S2pg6QlJK2Tuldyv0uBwyUN\n",
       "UaB7dNLoMYdxoE3igqplMOD3wAsEtcTL8TjZQ7EK8BBBqDwFXGhmjxFUCGcBHwPjCU4YJ8c2fyE4\n",
       "TDwoaRLwNMF4DGFmejPBSD0SGEoUio6zEDiLMCmbKOl4MxsH7AH8FviIsHr4FWGQbgccR7DDfEqw\n",
       "Lx0R7/MI8BrwoaSPsg8xs8nAsQQ122fA/gS7UHL9eeCnwJ8JDhBDCZoMgB8TBOMogkA8Nrb5H8Hx\n",
       "6WHgDYLtKr2yy9sXeSRwRvwdngbcWKoc7Ek7x/f7KcF2tnb2Xmb2InAo8Pf4XkZT1oI0Nw60SRQ9\n",
       "Q9oUksxa3gblOI7TorSVsdBXVI7jOE6rxgWV4ziO06pxQeU4juO0alxQOY7jOK0aF1SO4zhOq8YF\n",
       "leM4jtOqcUHlOI7jtGpcUDmO4zitGhdUiyhaTNNWL67v23EWZzwyRQsgaSxwsJn9p6X74jhO26W1\n",
       "j4X14iuqlqHZ1NyS2nT6FXm6bsdx5gIfMBYyyknNLWlwTG99sKR3CAEwkXSzpPGSPpf0mKQ1U/cp\n",
       "pa2OkafHSTpe0gRJH0g6qJk+/FSerttxnEUEF1QLGTP7MSFa9K4xB8+5qcubA98Adojn9xAiqS8F\n",
       "vARcl74VlVGblwZ6EZIvHgJcqPz8OhCiQ+9iZr2I0aQlrQcQ0xJcRUhW1zv2aWxsdw0hnf2ahISM\n",
       "58/FW98f+J2Z9SBkLJ4C/Cg+YxfgCEl7xD6sQMgj9BdCZOh1CekWKt537PPlhCjT/QgZku+MaRpW\n",
       "B44Cvh3f5/ap9+E4ziLEYiuoJGx+Xw3oVpOZTYvprDGzK83sSzObARSBdST1TL+N1PEM4Awzm2Vm\n",
       "9xEEwep5DzGze83s7Xj8OJCk64acdN1m9obK6boPN7MvYjLGeU7XbWaPmdlr8fwVIEnXDal03fH9\n",
       "fGZmI3LuWUrXbYGrCfl5NiZkN07SdXc0s3fNrJ7Mqo7jtDIWW0Flhub31YBuvZccxMRuZ0fV2xfA\n",
       "2/HSkjXafmpm6TT0UwkJ3KqQtJOkZyR9KmkiIT/OEvHyIODNnGbzm677vXSBpI0kPSrpI0mfA4el\n",
       "+jA36bp/FdV+E+N7GURI1/0mkKTrniDphihsHcdZxFhsBVULU2s1li7/ISEN/TZRPbZiLFeN+nUh\n",
       "T9ftOM4ihguqliGbmjuPHgQ11mcKabnPzFxPp62eGzxdt+M4ixQuqFqGitTcsSy7OroaeIeQkvtV\n",
       "Qmr55lJg17W68nTdjuMsaviGX8dxnDZKWxkLfUXlOI7jtGpcUDmO4zitGhdUjuM4TqvGBZXjOI7T\n",
       "qnFB5TiO47RqXFA5juM4rZo2m05CUtvzu3ccx1kMaZP7qBzHcZy2g6v+HMdxnFaNCyrHcRynVeOC\n",
       "ynEcx2nVuKByHMdxWjUNFVQSO0qMkhgtcVLO9RMkhsXXKxIzJfrEa2MlXo7XnmtkPx3HcRxAao80\n",
       "DOmueN4P6SGk/yE9iNQnVfdkpNFIo5C2r3XLBdKtRnn9SbQnpHzYlpCq4nlgfzNer1F/V+CXZmwb\n",
       "z98GNjDjs4Z00HEcx6kkpB3aAOiJ2e5I5wCfYHYO0klAX8x+g7QmIW/chsCyhBQ/q1GZZXyB0cgV\n",
       "1RBgjBljzZgB/AvYo5n6BwA3ZMoW+fD0juM4iwTSIEIeuMsoj727A1fF46uA78XjPYAbMJuB2Vhg\n",
       "DGHMbwiNFFTLEtKXJ4yLZVVIdAN2IKRITzDgYYkXJA5tWC8dx3EcCMlSTwTSq6KlMZsQjycAS8fj\n",
       "ZQhjekLN8X1B0MjIFHOjU9wN+K8Zn6fKNjVjvMRSwEMSo8x4YsF20XEcx0HaFfgIs2FIW+bWMTOa\n",
       "j/jTsOgRjRRU7wPLpc6Xo1ICp9mPjNrPjPHx78cStxGWlRWCysMkOY7jzBuZzL+bALsj7Qx0AXoh\n",
       "XQNMQBqA2YdIA4GPYv3s+D4oljWssw15gXUAexNsMFgnsOFga+TU6w32KVjXVFk3sJ7xuDvYk2Db\n",
       "V7fFGtX/Re0FNLV0H1rLyz8L/yz8s5jjZ2E1r8MWBnfF43MMTorHvzE4Ox6vaTDcoJPBigZvWnTO\n",
       "a8SrYSsqM2ZKHA08ALQHLjfjdYnD4vVLYtXvAQ+YMS3VfGngNgV53wG4zowHG9VXx3Ecp4JEW3U2\n",
       "cBPSIcBYYJ9w1UYi3QSMBGYCRyYSsBE0NHq6GfcB92XKLsmcX0XZqyQpextYt5F9cxzHcXIwewx4\n",
       "LB5/RthilFfvTODMhdElj0zRdhja0h1oRQxt6Q60Ioa2dAdaEUNbugPOvLFIp/mQZFZpEHQcx3Hm\n",
       "wKI2dvqKynEcx2nVuKByHMdZjJBYv6X7MLe4oHIcx1m82K+lOzC3uKByHMdxWjUuqBzHcRYvFhkn\n",
       "igQXVI7jOIsXLqgcx3GcVo0LKsdxHMdZkLigchzHWbzwFZXjOI7TqnFB5TiO47RqXFA5juM4zoLE\n",
       "BZXjOM7iha+oHMdxnFaNCyrHcRynVeOCynEcx3EWJC6oHMdxFi98RZVGYkeJURKjJU7KuX6CxLD4\n",
       "ekVipkSfeto6juM480S1oJK6ID2LNBxpJNJZsbwJaRzSsPjaKdXmZKTRSKOQtm9ohxuVil6iPfAG\n",
       "sC3wPvA8sL8Zr9eovyvwSzO2rbftopZO2XEcp6WRuAh0RNXYKXXDbCpSB+C/wAnANsBkzM7P1F0T\n",
       "uB7YEFgWeBhYDbPZjehzI1dUQ4AxZow1YwbwL2CPZuofANww120lF1SO4zjzi9nUeNQJaA9MjOd5\n",
       "Y+wewA2YzcBsLDCGMG43hEYKqmWB91Ln42JZFRLdgB2AW+e2LbDv/HXTcRxnsSJ/ci+1QxoOTAAe\n",
       "xey1eOUYpBFIlyP1iWXLEMblhObG6PmmkYJqbnSKuwH/NePzeWjbsA/HcRynDZIvqMxmY7YuMAjY\n",
       "HGlL4GJgRWBdYDxwXjP3bYwdCejQqBsTbEvLpc6Xo1ICp9mPstpvrtpeDTv+ROoZT4ea2dB56q3j\n",
       "OE4bRUHobBnOfvLtZiubfYF0D/Bt0uOpdBlwVzzLjtGDYllDaKQzRQeCQ8Q2wAfAc+Q6RNAbeAsY\n",
       "ZMa0uWsrMzgJs3Ma8iYcx3HaGBKXgH5e4UwhLQnMxOxzpK7AA0AReA2zD2Od44ANMTsg5UwxhLIz\n",
       "xSo0SKA0bEVlxkyJowlvuD1wuRmvSxwWr18Sq34PeCARUs21bVRfHcdxFiPyVH8DgauQ2hFMQtdg\n",
       "9gjS1UjrEtR6b0MYvzEbiXQTMBKYCRzZKCEFDVxRLQziiurXmP2ppfviOI6zKCBxKehni9LWnrYQ\n",
       "mWLRlbSO4zgLn0VGQCW0BUHlOI7j1I8LKsdxHMdZkLQFQeWqP8dxnPrxFVUL0JDYUo7jOG0UF1SO\n",
       "4zhOq8YFVQsws6U74DiO4zSOtiCoZrR0BxzHcRYhfEXVArgzheM4Tv24oHIcx3FaNS6oWgBfUTmO\n",
       "47RhXFA5juMsXviKynEcx2nVuKBqAXxF5TiOUz8uqBzHcRxnQdIWBJWvqBzHcerHV1QtgAsqx3Gc\n",
       "+nFB5TiO47RqXFA5juM4zoKkoYJKYkeJURKjJU6qUWdLiWESr0oMTZWPlXg5Xnuumcesu6D77TiO\n",
       "04apXlFJXZCeRRqONBLprFjeD+khpP8hPYjUJ9XmZKTRSKOQtm9oh80aY+KRaA+8AWwLvA88D+xv\n",
       "xuupOn2AJ4EdzBgnsaQZn8RrbwMbmPFZ7Wco9N5skVvKOo7jtAQSN4P2suy4KXXDbCpSB+C/wAnA\n",
       "7sAnmJ2DdBLQF7PfIK0JXA9sCCwLPAyshllD8gM2ckU1BBhjxlgzZgD/AvbI1DkAuNWMcQCJkErh\n",
       "AshxHGfBkj+umk2NR52A9sBEgqC6KpZfBXwvHu8B3IDZDMzGAmMIY35DaKSgWhZ4L3U+LpalWRXo\n",
       "J/GoxAsSP05dM+DhWH5oA/vpOG0OFdVfRS3d0v1wGoPEoRK/m8fm7WvctB3ScGAC8ChmrwFLYzYh\n",
       "1pgAJN+pZQhjekLe+L7A6NCoG1Of23hHYH1gG6Ab8LTEM2aMBjYz4wOJpYCHJEaZ8UTeTSQ1xcOh\n",
       "ZjZ0/rvuOIs8zwI9gSUXxsMkjgJeMePxhfG8RRWJzYAPzHhrPm/1f/HvaTnPWB/oYsZT5TJtCWwZ\n",
       "zg5fM/eOQW23LlJv4AGkrTLXDam5cb1hW4UaKajeB5ZLnS9HpQSGsOL6xIxpwDSJx4F1gNFmfABg\n",
       "xscStxGWlbmCysyaFnDfHadVo6I2Bp61gs2WWNmMNzNVlgR6AEgYcJ8ZOy/wfojOwFfpogX9jIWJ\n",
       "RAdgkBljm6nT24wv5vERTxDs9TXVZBICZgMbm/FMjWpfAV1qXHsx3uceM3YFiBP4obF8CPxjtZo9\n",
       "NPsC6R5gA2AC0gDMPkQaCHwUa2XH90GxrCE0UvX3ArCqxGCJTsC+wJ2ZOncAm0m0l+gGbASMlOgm\n",
       "0RNAojuwPfBKA/vqOHWjopZUUZu3cDeeAraQ2IBgH8iSNWrvND8Pk/gs/o6zdJ+f+8Z7d5LYeA51\n",
       "TGLg/D6rDp4C3m6mH98EPp/PZ2wocXEz178f//av0YcewLRm2icOaLtEp7Ys1eO+tGTJo0/qCmwH\n",
       "DCOM2T+JtX4C3B6P7wT2Q+qEtCLBjNOcd/Z80TBBZcZM4GjgAWAkcKMZr0scJnFYrDMKuB94maCq\n",
       "uNSMkcAA4AmJ4bH8bjMebFRfHWcuOQt4TEXlDdxAaWB9Y34fJHGgxFk1Lq+MZiYTum6lNkW1A3rF\n",
       "4/lWx0h0BPqSr0bcdr7uXVQ7vnfgebSb8VTNOiqt0pp9lsTAuHqct74E9WVNO4vEt4BT4nGVAJDo\n",
       "LHGZxID0/6MGh9d4xjeAW+Ppr2u0nUz4fyQrwObokVOWN+4PBP4TbVTPAndh9ghwNrAd0v+AreM5\n",
       "mI0EbiKM7fcBR9IoF3Iaq/rDjPsIbyJddknm/Fzg3EzZW/j+KKcVoqK2B34WT/sT1dlRIFxmBUs7\n",
       "/qwmsXlzdhuJfsCkOLFLP6c9sBpY4nF1ck7zS9ngUnjhCAj2qMRra8Wcuk83/86aJVntLAVBJZ9i\n",
       "Quq4prBphp+y7jVHM6sLZbNLFR3j3z9JbGjGsTXqfQNAoq8ZE9MXomZmCnAGcEliWihdL6oXOx77\n",
       "d+7/S1JfZlVC719AYt9ZCvgwc31Z4JD4gjmoQSU6Ro/oNNukju9prn2kV3xvn5sxOed6T6hSU1YL\n",
       "KrNXCP4C2fLPqDVBMDsTOLOOPs43HpnCcTKoKKnDVyvGVdHymctHlo5G/PByLf/kTSrqW7HkZ1Tz\n",
       "2Bwe9ylUDVYA+wEj6VxtClFR5VnyrqXu9EpV2SznfrVtEnNmv/g3TxWVnrF3zmsscZbE36WS2ijN\n",
       "ZQBscCkSU6IqM0uiXlwaOKaZfp4X//bLuTYg/j0dODHn+hd856/QY3xynmf/SatYe+dcr2UzqkVe\n",
       "/Y9Tx13rvMe7lF3IoSzYIf9/tsiN+4tchx2n4cxu/wWbn5l4ZWVtUeXfzDrXbc8hm+1NUF1DsLlW\n",
       "IVUPSBLLSbm2pYRgj1n5obxrlTPnJsGQv30jVfJl5cNmAyzRzLNqInEMcEQ8XSqnSlpQ1bJX/QY4\n",
       "CthDCiqrGnSnvE8nWz6nfu4JrNdMP9P2rV9WtC2qLNhOWCY56plzj+mp4z4519NtXqvR1TR5gijt\n",
       "8JLn0Zf+Ln0CJRXjkHj9tUw/qldJ+erAVo0LKmexQ2IDie2kkpG4fK2oEbSb1ZMtSltUskua4QDM\n",
       "bv91zq33UFG/VlF70iHtCJc7cF4DrByPL6y6agwG4Os6x5SVH9ykojXcxuOn/AqA1e6CfIeLevhr\n",
       "6ri5FdWFRNXbHKhp14uMbeYZzXF+6niZOVzPetLleUPmCar0yrdCUEU73j6porWyjaWq8TZvRZUO\n",
       "epA38UkL+iWB0fE4sa0lqsn+wL3A+FR9tNOxv6NJG+bct1XjgspZHHkBeBC4Mufa2pnzrTPnp2GM\n",
       "4aVDag24fwRuZcDwdFmvnHpbpI5nVl0Nhn3Ya99wmue9dd09ZTvM6nf/JjpRANwCfIP//D4MUjO6\n",
       "QzOrEolVpZKnWXNUCCoV1ZuVHh7MGrfcSG3Df5bSKkJFBVXh8APDtpN2M2r1c8867pte7eQ5fVyf\n",
       "On49cy27baZWP9I2/azqbyvg+NR5nr1ok8z5d3PqGMFj88dAJwlFFXTy/+9H5cbbEhLrpE4/J0wK\n",
       "KlfSS406NeeZrR4XVM4iRfSoWqWOep3jHp/6711U2fg9seSP8MvqiqxCh+lVxRV0rNC+5QmqtOdA\n",
       "5ey8qNVLJ6N3Se7VXUW1U1HJLHxP3t00u6pbVUUlHmNrAH15a2to/9V4yPdCk9iJIFz/nfs+1roJ\n",
       "+pd2hmQFwOUcuN2p7Lv3vjRpGjCrDi+0tLorbCh96sQwYG9yHnScmqcaTNtcZsTVS5a0IM+zUf0o\n",
       "dfzTzLWezOr4Pvf+DcZt9A4Dho0gX1Alq6xryar+NrxoAIOezqsLRLtik56g+0cQBMjXlJ1f0qwF\n",
       "nAS8Q/jeJLbF5HM7HljarLSfKU1pdhSdNLYFdij1QfRMfa8XKVxQOYsWQ/42nt7vjq4lrCQ6RKP9\n",
       "V1COxl8nwSli9A5f07e0leafVbUeOvt0+lYFFvgR8IvS2Q6/So6+JN/wPil1nFVBrlru0Q1wSg8I\n",
       "6q9ZlPfPTGRG9+ysehTl1cf7QG/6vg0/3G0g7WbUMszfC/mrKZ3S/Wr23heOLC0yu0l0lDgnnqdV\n",
       "mv0Iq5qKyYGKOoomJepHoCJMWmDK0vDJ6rDtyXBSv1My7ZdnyN+b0rXJV8ulhVeekEmcND7NubYR\n",
       "H6/Rh5mdYdCzK3D4+uvUuEcvgs3yQ7KCapejruJnpQXTn3PaHgZAl4mJELmb/PF3J+AHBJVzb8pu\n",
       "7N1U1Ersfkj6ud/KNs5wHlF4xT1wk/h2Tc/KVo0LKqduJLpLLCOxxnze53t5RnUV1UFr3jpZqnDR\n",
       "rWTnY+E7fwYYLZVsPGlWpBz8+Ds5z1bmPG0nCCuff1+bnP8TKvYntQdm88wvh7FCDJJy5iSA71jB\n",
       "riOo3AKvlcwVw8kIKon+pIUaPJLpZnjmq/umyyrtNE+euDSzO0BTza0rKwPfKAnc3Q7rFDfWL5/9\n",
       "DGrSaWpWqHQlqEyD19zsdumH70DPcdNJ2V1UVEfg7wAcsHtSnI6gcR8ff+Mjpi4FS8YtZx2md4xt\n",
       "d1NRnTAOYedjoOOUT8wQQcDnCf7k8zmPzGclldRf9xLcv1/NtD2FAS93Z1aFNvfnFfcoan+WHrFU\n",
       "fP7n5DtTJOrLEwHLfM5hC07ZdjmVjDBMTb6OIagOv0l5Rd+dWR1OZP1/7knYm4pZ1ftISGYWXwL7\n",
       "S3SlFD5p0cQFlTM3TCHM1EfmXYwRBrpJvJfn6RbrdAFug9z0LRuw7149YHae51cy8KV/7HkqoJoR\n",
       "I1RUO3418ICqZ5YZwOz2t/NVv06YoN3X99N3TL/Ydj3CAPk1szofzaXPwj0Xwtc9ocleiu3Lq6Tv\n",
       "nglN6kbw/soOar/P9D27QujGp6s+zBu7lUt2OK7SAP7QOf+q9T4BaLIZwIGl8/WuAGbfSVAp7aKi\n",
       "Bquoij0wNdV2U5dI+tyNOOBJdGDSoLSDxnX8arm+VDoIVArXE5aeDLRXUQNUVNBBLTWqykFDRe1B\n",
       "iHwwHXE6AEuMTtSOX5CvSp0MnEpwLsiuhpJ9mr8n/I/ynTM2uDR9tnfm6vXscfBqsf0XpP6nKqo8\n",
       "6Tq9E2bMIqwuu8br5c/1Zxu/GI++BLorBJd9tPSMwNtUR784mfYzw+pqxf/skCr/LnBcRc0mLaGi\n",
       "ehOE4XqEycID0ftzkcQFlQOUVks1Fdi1BE+G6YQf4CDyVzMH01zol1kdwrVv3ljLhTnYZMo/uDwV\n",
       "UHM2koPo+eG1dJwK0BTL0m7La/H21itj7UAGJwxs4herbh8HmpcIUQm6APfy/hB4/kgI7zf5bL4E\n",
       "/sTs9tDpSwgDRcWgFsk6T2TfxyX0GL9xhUPGxhdcm7pesm2YId7b+P5M+1sJIcvghtvLK4MN/7Ez\n",
       "hw6BrU7dCtiF6k3EaUeH8ufY7VMI+3t2LJV1+bw7fd49hOeOgskDxpbKO01aoeb76vFRT9pP70bw\n",
       "RGsuKGv1fqutT01saL2odERB4g/ACsBFhP/BoRIrpmxZ/4t/pxEmW9WC6rxx8OzR5YnGDsejojqq\n",
       "qL4qKgiIZV6C45b7Pp0mfVnx3sYNyfvddIl9gvSm2I7TkomREVz2/49ktdN++oYs9ySEzbsVG5Yh\n",
       "lUHiJ2WFgxn/hVQ4phUeA3iUkEsqWXkeDMDa6a/QooULKifhz8BbEmtJuV5TeeqW5sjbuLlCTlmZ\n",
       "mV22A2CN234oVbpsl2bgAOtflhzlzazTDgbDMtcuB+C7Z2JGMZatEu/fD2ji/Q0Hl2p3+yxx9U3c\n",
       "khOvrh0JA+LGhMGvC4AVzKxgv+aulB2gSSfm9DMZhF4h7BtKwiA9EUPvdKLzlO6MzE7qS1Sopbj8\n",
       "qStKx3+YfD/BTTo88409jEQVutlZsOzzMOTCH5Ko5CrpFlW7q1Hpag1ZwbLaXcEFvO+bX9Dzw/LK\n",
       "assznkzV6snHa8B1qQALOx+9ZcV9Lnr5XhKh2hyr3ZvY3gYDf5EYKpUEwG/j3y8oxzi8nfJ3YRpr\n",
       "/et2mjSSlKCS2Evb/boPxtdMXgZe228QiVPGxn8mtv+MtCDoPe48ftv7n8CPpOgSfv8FLzJ+PZjZ\n",
       "eTZUbMhONA9BVfpoYQLlwNqHk3Vh/9FOX3HIZtCkUVXvf41KX5eUhydmJY/He/jplknxqSS2x6VG\n",
       "wuHrQO93qm67qOCCajFCYrPo6ppno0h27r9K5e74hC3n+IAmwS6lSAl/qrp+wC7fYKvTa/VNdJ4S\n",
       "VDTvbQLpCBCBXUpH7Uorqg1UVBcVlY7PljbmJxtAKz3pNv8DKqorwdMtGWRDBIpln02HBKrFTsC/\n",
       "YmTr6ijWww7O1t9RYkepFBKnI2FQXZ+QSXWQxC2EiBKDgLd45A938P6QP1ihMgurFUzRHgZwdfzb\n",
       "kwf/BEP1P7d1AAAgAElEQVRPhxk9drSCzaYsXMeTqJFmxm52/bzshFGpDhpMCHn2BnddHP5//77m\n",
       "V4QVXNoJYTJr/DuEVbrjn70r3v8m5yHxjsRewA9Y6nX4KrWg3OCyyuSpH32rO+H7tiKTls1XZ778\n",
       "w7zSLcg4ZpgxmxDAGtLbDH64037svf/3gOkcu8osOkzrEt3xb+atbfdhdseJoFFmTI6fXR0YwBkS\n",
       "97Lsc2CCsz5P9pyV1LT6xcplAf/Zqp8A343epRUu8iqqHSs+mnyOia1q11KFfX8Q/t5xeVJSEdbI\n",
       "DNGk9Gf7Et0nhPioR60FA16GrfN/e4sCLqgWL86If/P2AOXZe9JU7QtRUb1VVOVqYcPS5PPSTN1e\n",
       "rHbvPqmNtJWzxhP7lw0yXXKDU4c9IiOil3GnyRDcqqdRuQ8mWfllPemys9TfEwbgbirqUJLV14R1\n",
       "pmT7XsGIHyb9TP7mpVuYyCer7V866/zF+gRDfrKxtA/wmxjfL/FjjyMRHYFJjNlxDxKVzRXlKEzR\n",
       "EQOCU8Lv4/EknjoBhhZJ0RO4yox7SP63S+Ts+e1V+uheJKxGwgDfbtYyPH84jNxzLNCN4wftS1OU\n",
       "mb9YsQtr3B5GzCkDLyar2mzS8oSBNHTIqOXLP5UgcKZawcZy1cNNVTVeOAxeDAvIqI5M7z0alHPP\n",
       "ihBLKmp1Vr1/o1JBvzenc2o3SNzxD9j9XNrPWJr6NiuX2fVwYPZGwE7s9MugFpzVJVlxh2gQ7b+G\n",
       "fm8FTcAr+/2VkT9IHEk2Jrun6pPVr8l5ypTwJmaVS8qToF+rqN0y9dMqzfU5ccCx9HpvSqnk5QO+\n",
       "4Ovu8x0ouSVwQdWGkFhNIvvlDdd6v/cO5UC/eXtq5ioKc1yVDYcQcFVFhdnwmJKdN7sy+VvmvHe8\n",
       "T7KSKzsLbFmEZZ8tP6uogSRx9B7+Yyj8bS9KA2eos69+vMOerH7H2QSbxA1EW5CKKjtu3FyatB/J\n",
       "vt9fn2NW/SPpPU3DDl6XstdgNbddm2StTjz1vgK6SvxK5VQV3en0ZTm79cl9IAYolbiEJh3KyT2T\n",
       "SAnZQbwzRk+m94TEfvZuDN33xMmzgQfiZ78yZSF3K9X0JcSAg5D/KJ9vXQ/hs/+StBNC94/gy/4w\n",
       "s1sw9Pd6P3gpDh4KfcemJzXvE7zTApOjyW+7E8uhe3626Q/Iw5QI2uC99uk3JnHueLj0WbjyUbjx\n",
       "Vrj7H/DOZkkcqaOBJ/NuVbpldWDWI3Irdpwa3mOH6Xl2zmoeLVTajL79f7D2dWux89HhfOzm56eC\n",
       "2J7L3nuvyOBHy/WfPHEjZnVJhMaTZhlniSXfCE4+M7qmV1pBTbjRX0MMw6GnH06IuvE5wXsxmzap\n",
       "WhV+/PLl3+3a1/em05e5qUNaOy6oWgEqqtksrHF3ej3/qzeo/vIGjl9+eQY9kzgp/EBF9YveVQmz\n",
       "8pql2L00s+sy0Vj+8SUIqqJkN3ywMwQX30+Brpl06FmbR7LyGS8xhO4fhx3zT8X9R4d+Bw0YkQjU\n",
       "MKhPHgBTa35U/2KVB29l/+/BqV1WSqJSx/0jyfs+i9f2TQRoR9a4fU2WGDOg4i4fr3UvcFhUuZVV\n",
       "ZPdcCG9uuy5hpfI65RXbNwlu7OcCp0n0ATrRfUJl6KWyp2JYHnSe0lVF9ciJ0L0fszotGUMnBbdq\n",
       "azeU6T1g2MHtCCraZECaCiWVVwmJ8UAheZYVbBLTe5Y9xf72RtmOt+3J0KQ3473Kk4UeH8LUpSDr\n",
       "fXZQZdJX4GESl/PPVjqGnjFiz6bnhns9cTIksRC/7v5yRcuP1noOmGJWErjTmDIA3h8CY7eE1xOz\n",
       "VLtkmb5T6fMK38Vrdfh6g/jxdrDGrem07Gej2bD9CZBsA/hgvUphfkp3OLFiG1p682113qsvl+7O\n",
       "h2tXJjvc80AYEk2pgx9PAt2GSdFat7zNt24IJRe+Bh+uvxGQ9TgdSrePYZ0YT9YEk5Y9D0BF9TFj\n",
       "NoevcxY7Hh9+FEOLl5oxnvyIKlC2I1b9k1Kc28y1VosLqhYm6qs/jjaTWtzGnAXJnOkyEQ7YBX69\n",
       "5O8JK5zbVVTiCp4OpVCxr0eiL/1f/SGFuOj6TT9x8BaJHetxFVVWmwSV3HCCB9kE4EMVtSpp9Zhm\n",
       "Xkgw3Cf7scqzvJldyhbfQ4f8N6bPCNG777wMTNkwNNV0mJ6sDjuwyn2lGG9WsN8CfZjd7hnITSg3\n",
       "mKCes1j/I16KGRteOQCueWgiQVC9m0nPkDiOfJfEUaL9zMoZePdc01cy+y+UStrNOJEOX/dmRnfM\n",
       "Si78Bc6aDJ+tAkFQjSOkBkkLkXR21UT4ljNid54cbEzX3/k2n67WKbP/6hzCht2wkbf99LBiMBEF\n",
       "wx/zOs/57wF0sIKtYgUT/d6qVp+9dPBIK9h7VjDZH6aESc1f3gz7vy5+5T0qU2WkhUV6v1XiBdhZ\n",
       "RW3LMi9AoQMctv6ODBz+His/DPvudVpqwrcNy7wQIl0kTFsyX/04vSdcMRQq1cfhuz0+lWnogw2M\n",
       "f4x4IU5gqnTTKdtWOSrGulEAfVzK/P4kQVWdvM9l+XV/+P5B4WzC2thfRydGqIkq6gEGvFzyzExN\n",
       "SPKi7UNZK/JujesAVzRzrdXigqrlSdQtJQODivpKRaVn5LVVUYScRmomYVzJ1XiXo2C1e6Hbp0tT\n",
       "nt0llup0PLWuEl2i40VnYADdolxqX/V73xxiUsu7L4YVh0KTtqFS5x/cg+/6B8zo8jWHbJq818Qr\n",
       "KoQteO5I2PwPZVtSh6/XI82YnWB2x7ywM7Vp/3WImXfF0AuiR11n8hw9AM6amOQRKm9ovvuiv3Pr\n",
       "dYlDwO8IQiAvjhuk1KNWsPdJq9K6ToROU2rZ3xLV4xd0jbJpeq9SbqecfFY9CO7yaS4q3aPMLanj\n",
       "4dz3F/jfLuU9cE2zEmH6M9Kp0X8Y/VZCjECsYL/J6zSTBkFlDqqLqupMXarSMNZk45m4UnL2BmXH\n",
       "Acz4mrKNaeVUuQEXEOxZD/HzuPAbOCy7vN4r/j2Nda6uvNLxy/wBuvNkmN0RgloxPK9gs3h3E7jj\n",
       "iiBQm+ybfDBkQmr1+2zFPZ6uiLL1K2ozhuBmnmg9KlO3X16Vzmt7gqfq+1Q6M6U3oQ+nminZguiE\n",
       "IyvYeKbmRZhq3biganmCi6qxTcpe0xnopaJ2TVeMnlR5HJUtkPiRFI3Nb+y2HEAqLFCaJXI2enYj\n",
       "7MOAMNj2KsW2Oy13O1W4/7iNbiuVBGFVyWerQsevOjHouY8qBF6iUgzC4PskTgRp/jIGrB0k9ozn\n",
       "jrq5dO3MyXDTjW9x//nwx08mQMmuFYTDZyvDO1v8kmRDbrvZk8jSNBum90nSKpS9z2Z3eoJXShqb\n",
       "AwlqvrSXYToQaQVWsKnAvszqAIevB7/tCf2qHRrMSquKr6O6CqoDmGbJDkaJUEy75/8n1ZdZPHts\n",
       "X2iXWi23u6l02KTwWTUJVopV3qgwd5YH59ntLFmRpVWXVrCR/N/zcN8F5VbTe2fzeb1M82T3hCXe\n",
       "mvVMUGbHPj1QUslBWPn988n/pOo1VbT6aK1ZZlTmU/nnk/34cN3kf5D1hC3vG/jnY8N5oBwxyQp2\n",
       "PumtGU+UFkRXENSQU0j+V/1fLqts7/sLzOhWWnqlOAR4yAqWzmuW/r2vo6LWjVFTIESySL7fG5OH\n",
       "5SkTWjcuqFqeMLt6+Kx1yITkBy6U2DR1fnONKNfpKdL4aGy/hiSi9XNHN7e5shOVSdcgqO0SddzJ\n",
       "wGasdldz6oRAr/fOafb621uXB5tVHiiXHxwdud7b5GQr2DQr2BVcNKJythln4XHXfwfu/fvvufJR\n",
       "uOX6kApj5D4r8cxxMG2JxPDwBE2CH/wI+pW0SMkvNKyIPlxnIHdcBlc/RDoZq1lFqonEpfyuVFl6\n",
       "AMjzPCv5EFvBbuLrnuWcHz8vmYF2gCr7ZEfWqXtT5q6Z82RmnY6/V2Eni6rC9PdpCvCH0tlvM34F\n",
       "0yoCb5e/d2dOSSJw30aWD74NM6MWOziuZDN170DzJIP3QcDNZqXNuvkBhp8qLWDeBjYCUFHlNB8v\n",
       "HgqTBqXVZo+Qjcbe/9XVyRA33KZ/NyXPRivYZN7eKqxy3918/RjaKU15Frb6HYmAvNeMLwgCdzcV\n",
       "Zez7gxCJ4t1Nv+S1fTALLutRvZj+8C+nkmzAvmGUJxITrGBfxdXTM4QAvIdVvrlFb9hf9Hq8iCKx\n",
       "fSrmWCgL9qkgEL59SXs0OztwLU/YZ5OmMgaZ2J3KCN+dSGVElehK++nBEQHgz29fkqp7G2HgSg8C\n",
       "kwmz2EQFcwJwLkMuKs+MX8+JcPTwmZ/z/kbVEZ3/UaGhKnv+Lf/fYLBvEiwXo06/vVXZrvPR2uVV\n",
       "y8UjqBAkBZtlxsuM3RJeLXuBR9YE3oPcOIAJIRz4gBETGHYIvJWfaRsgGvoHUjmjvzh1/GhlC842\n",
       "y2T67ToxbxmazOA/ji7+61C5Cfe31U2a5Xky35WoSsuS3kj9K2ol+Lv3r5CyTVnBxhO+C08xs+t1\n",
       "wDlmNdJvTFo2eBlOGZB39b7MeUWYebOScHjRrMIBZ/d0Pf57YlA3P3JmnGiwInCwihpLolK7+iG4\n",
       "K/11Z0/Car3S3vvepnn7BqFS0Fd+n676z3787quKFWWK8qRueq/kf5rsAyx/j5YYE3au//O/qzJl\n",
       "QIVAt4KlQ4yNz1xLnH3Sq8BkFfdwpu6VVrBKwTbbV1QVxE2OoyRGS5xUo86WEsMkXpXK0a7rabso\n",
       "oKI6xJ3qD1CZFA3SqQf6joUdjoe3tknK7iOsirKU7A4SBajIQQPBIyztaTeV3u8m9gSYvOw4njvy\n",
       "Gd7d9FOGHRRUG33f/B8hMsXfCTvaM28iswfyxcPC/pbLng46/PPfhf+e3Icvl05sN+V7fLge3HA7\n",
       "jN0cSAWb3fRPcEhGuzWzW+XMcfKAkG5jQmnv5pVVfQuMSB1/SjZXz82V+0itYFPjjNPI/LDziKq5\n",
       "dPSEEalrd8fDZFDLjYOYYXJ8dsKdZrwMBKl7yQuQl6Oqkgo3bTPGmpX3GOXM8hOeSx2Pp1aK+i/7\n",
       "n2RGhW3KCnaeFWxTM94xy/9NmiFG77ILf/gS3tkCqr09n86cZ1dcCe9lzhM7XRACm/0pDPKzOmUn\n",
       "Gkn0k5dCuSAIMaxgt1nB3iWsvHbhq55rc/ONkGPTie/lq7zyeG28zeyc+xlbwSYnNiEufzpZiScO\n",
       "M1WZls0Yb1bxHc5SK6TEgzll2dBLOR3MEVTSckiPIr2G9CrSsbG8CWkc0rD42inV5mSk0UijkLav\n",
       "vumCo2GCSiHR198J4WbWJETxXSNTpw9hhrebGd8kGkPrabsIMYPaxvdg8f0kjhXf+Qus9Eii8P4L\n",
       "U/vlDSIhurRYjqBrPyPneqUhZJejYdkXwvHsjpO498Lv8M//LsFdl4YBfc1bvgm8b8YxZhUZXQN7\n",
       "ZiIDjNnxYO7+B4yL4fwmLZdcmUhQZV7ImO1LSYx4Yw+48jHIeiOlbWYjfrR/3ACbsBl/eRv+XvKt\n",
       "uN+sKo9QcrEkrGOensof66iKFWA2m+u8ZDvNE24fxL/Vg96sDt9jRpf0JuK7MzXCvqQtzgiz9s9X\n",
       "hBD9Is2RpDazmrFZjb5dUqM8Ib0yHEL+HizYe7/am57nzERmdINg07slc+1M0k4nVp152AxFNVm5\n",
       "rGA/i6uIwYQgrLfwt1HJ6utDqids6xMH+IwqFyvYDCvYvXbWpFd4bZ9eWff+GoydY40czJgZ30/y\n",
       "3X+UsPm7/nsUrNakpWoiawWbs3dw10/zclnNAI7DbC1CnM6jkNYgeMCej9l68RVWxNKahMnbmoRx\n",
       "+iKkhsmTRq6ohgBj4kxvBsFAnfVeOwC41Sy4hpqVVhz1tG31qJiafQQNQa43A+NybJ6PnboS3T7b\n",
       "KK5m0uH8L5RYneZdUFOkJu5DC+1Ie3fNjj4U2/1mNzIqmAoGlezoyWrhoNwnGTOtYHtYwT7nngv/\n",
       "w7VZLQ/HEiYgWY81uO2aBzIlrzGzC6nUC3n9+33mPLHPJCuqLbhoxGhmlTSb08yqVE8/JnjzJdSK\n",
       "0LEJYZBZwqzCdRqCM0niNVbV3s6YcYf9ftrPCSrZnpQ9LUv3VlG3s1WcdE/rC0l8uOQexsXkedVl\n",
       "n2Uc3sxqKlFVrR4Hz3FWsFezYZqA561gc56Z135GMrB+kFWNmTErqlN/Qtlhp/57h3iKF1jBzD5Z\n",
       "fWXCZG8nqiOPQBCSzb6PnA3CedwJrDTHWnUQV1u7pIqy2YbT9KOZiBlWsAkEt/orY9FdtepW0Glq\n",
       "9WTK7EPMhsfjKbFfifo97/u0B3ADZjMwG0uYHA/JqbdAaKSgWpbK5fs4Kr2lICSI6yfxqMQLUil2\n",
       "Vz1tFwXKM6cQTbsUWFVFlV2F0rHQEl44IgxKS/wPKj3ALgPOps9YWDGbxqiC8Pn1L5kglrBHm4zq\n",
       "wT3w/R//VEUl3kyVnlmTS/sfk4lEntNEpcv3xFUeYUw52HYcGN+3gh0DfLui7hfLQWZvSmqP0GyC\n",
       "t9QfqKain6mJTjgv2OM2Ye3VKKf+qHR3D23uMuN0gj1wicyqLl3vaTO2Tu1tSl8bR9nbLs+5IunP\n",
       "VCvYlETtFwVEYosqTcTMJDNerGpfnvn/KHttbkg5KKRZg2Cr7GAFWxADzlSaiZBuxtVmnFfrer2Y\n",
       "8RMzhlvBsva41wjCfqmcZnPD3YS4jjW3f8wjHQmu6tmVYAkr2EQrWLMhj6xgy1HeL/e75uqmaF7o\n",
       "SoMJv5VnYskxSCOQLkdKBqtlqNx71tAxek5hc+aHev6xHQlL9G0ILtFPSzxTZ9sSkpri4VAzGzo3\n",
       "bRcUEu2jR1o+nSYTow0kThTBAeLLJVbisdPe4rV9YMrSM/nFKh2YOBgmR8ellR6CT76RngF1ZPDQ\n",
       "waUIAcFN+EkqvbkgcRnvUppQJgP/2IpaZ04K4YjWuXYIcBNh9nQU6c2iy5f2d+xHMApXrUrMqHBX\n",
       "M+MuQHn7u6xgpqK2IqxQLrfz3/0Z52drAWEmOzNXHRme8UpMCtedVBifnBVCYpfJelWm75W1icwt\n",
       "idDMulfPibmybDe3WpofrGB5K5J5v5/lZshtNA8TVKQXAIUoXOZro7xZfkiy+SWq8y6bY8X6GBXv\n",
       "WTNUlqQtSfZibVGrFiD1IKxEf4HZFKSLKZsXfkdITHlIjdYLWpiXaKSgep9ksAwsR6UEhjDr/8SM\n",
       "acA0iccJM4xxdbQtYWZNC6LD80K0FW0PXCbRyYwZURCF2fvkAcOZ0X1den4AUwa+Gvc7lGftf/ok\n",
       "7DJ/bxPAOjD09DMY2lQOc7zV6dizx5jKw1Mv9t89nYZ9iBnPx425swmu5WX9/sFhMZHaOV+5Y/fr\n",
       "ytQ8KmpbVr1nBUbvDDCGg7Yob8gMnl+XUQBVDpe1go5C8ELMi6eWLPXyN98CZnNW90aD91fAY83U\n",
       "mU6++mKBYcakeXzGM5nzFxZAdxZLrGDbxcNdmq3YxrCCPcUcvntxAj8UQEUVcn8tUkeCzfJazG6P\n",
       "DT9KXb+MsnoxO74PIruBeQHSSNXfC8CqEoNjvLV9qY5DdwewWUyR3Y3gjTOyzratg45TrqM8M/pB\n",
       "9PCbDVF188Jhd9DvTTjs29Ckb1JtSE/N5AVDi6dXfOe6VmjE7gR68eZ2ZenSpNEQBmMzZsQBE5j9\n",
       "WDpoa0K1CkPw7NHpgof44S7J+/kOg0tBEarjn5XZutaF2KdqdVnBPiaoIcc2c982jxXsYcqx2V6w\n",
       "gs2Lc4fjzB+SCPu1RmJ2Qao8/bv/PsnWjjAW7YfUCWlFghkn7VG6QGmYoIq6/qMJbtkjgRvNeF3i\n",
       "MClsQDNjFEFV8jJhw9qlZoys1bZRfZ0vTun53VRSs+lkDZqPNWVdS8uGmxcPbT61xmt7v5I6a0+v\n",
       "986n20d9o1dYwt0qaj0VdRyAigr37/pZOrRK1oMszQju/3N1EMv+r8Ap3ZIZKlawD6vqlG1wNVe7\n",
       "zWEFO80K1txqbHEhUe3mbeZ2nIXBpgTb51YZV/Q/Ir2MNIKgNAxp781GEkwFIwmmgCMxa5jqT3O6\n",
       "t8SfCLrJaQShsg5wnFnuHp+FiiQz+CNWIxZZo59f1JrAa/znDHj8NGj/1Tac1rXSw6HJjqZJedlU\n",
       "l6bJ2lN2a4bgpPDr0tk+e33AmrcuQ9Atv8NnK11Av7eCOuyD9UMOnMB1BE+yzsB0/jbqbo75Rily\n",
       "QdZmI/EgUBZChmLw11qsbgWrMMDHrQXTCIbZ0bWcEJz6UFFXA4dYwWoFHHWcBYKKMprArMqW22qp\n",
       "Z0W1fVQn7UpQ06xMxnW2hcnbfb9AkbizRtDXYGfp8jlseCEZIfUcweNoO267El6oCCjxZyvYR1Qn\n",
       "MExsS0EvfN9fklAwQ4Eb6TO2nLW01/sXUY4ckLg7B918SkgR9rJkSbu7J2q5JNDlX8lsyswKKQhe\n",
       "eVHd+LoLqfnHCnagCynHyaceQZU4XOwK3BI34jVsiTcPNHRWEAVUleePiirvMO/+EaxZsW/yn1aw\n",
       "jWiy1YA9GPETuPuSm1KhS5JVVBJ8dYXozZWEtwlhlSpznA2h3eyyzu/1719EOb5bwr8z51jB8la+\n",
       "JxI85cYQbWlWsGmE7K6nUMd+HcdxnIVFPYLqLolRhFhSj8RU2DVDi7QALbV8DcLm5QNmsM61sGJF\n",
       "yLckvMy3UmUXclcMuTWrQ1J+OkCyaz2zG/8gZndMe9VU8mhxLHDjHPqYG54mbrqcSlDjlmKoWcFe\n",
       "sYJlNwPWCt/iOI6zUJijoIrxvjYBNohBLr+kdUWJaClBFdR2D59dHYmgYMnG02nx7wwzHud/USN3\n",
       "xeMHSlydbVdxD+OquJG0cpX0WMxEMbX/tEy8uOp7FKy5+GGYMbVGPLMk2GVHFtCOfMdxWg31RbBo\n",
       "RcxRUEl0J2wA/UcsWoZsZIGWZaELqrhP6kqeOn5cRVSJCd8cR6WLZnfC6nMwEFR5Fw9PYuQlUTiy\n",
       "Qj+7otm71B7gsdOO5tzx6SgF3QlC8weEcP7tCBseV2AeiaFZOlnBZqb2XzmO0zbIBsdu9dSz4fcK\n",
       "gh0jCePzAWHncnY/UJtDYhDLvBCid8/qhETXuDk5DN5vb0MMvgn3XXA2z/6ivxmHSPSMMcS6AxeZ\n",
       "pTz7JlRFTOmbOa8QDFFQvKOiAF5ndsdn0ukTYnI+qFx5HTcv7zfzXDfsO07bpDX5GNRFPTaqlc34\n",
       "I9G7LgaUbE00bkW1xP8u4ecbwtrXQtgjVRmUb9KgEDLfdCLPHtMdOFjiacoZNrtTGUw1m7sIqtNC\n",
       "fI8c5w1CdIdvETwv5zlgqOM4iz1tUlBNj/HUAJBYmeZD5ixsGiKotM/eHTlm9ZASot1MCBEk+lXs\n",
       "N/porfHAj6xp9rmptN7fgZK34GFUplVIgr4mAUePMKMiPpcZj5pVr1ZjMNNZZnxqVpHR13Ecp01T\n",
       "j6BqImz0HSRxPfAfaFWJDBuzolrrlvL+rPUvhWWf+5yVH0iH3O+GtR9IDAZrVpWJF0LYoXRm0kRo\n",
       "JTriZp0dHMdxGsAit6Kao43KjAclXiKuFIBjs+kUWpgFJqiik8S+VrDKlLDLvgCHbvRtUkngrGDT\n",
       "Ysz2i2metNdc4mG3Q/zbmlamjuMsHrSsoJL6AYMwe3mOdSM1V1RJRl2JDQi5esbH1/IS689nVxck\n",
       "C0RQqahewJLADTGwbG2aSo4GH5BvLyqVmTEgdZz9grREKgTHcZyFi/QYUq8opF4ELkOpnHxzoDnV\n",
       "3/Hx73nxdW58JedtjS9IQhclqeOb7CnGbdQ1p25HiZ0JrvppG9Rt8W/Wky/LkfHv583WchzHWfC0\n",
       "xIqqN2aTgD2BqzEbAmxbb+Oaqj8zDo1/t5zfHjaY+V5RRZVfHp/bpc98paKeoaz6TLgn/k179dXj\n",
       "7r+UGZ9I3NzKVKiO4ywetISgah9ThuwDnDq3/ahnw+9RUnmFINFXKq0IWgMLQvVXK4tnsqdpE879\n",
       "YOe8CjEpX637VIUwSoSTCynHcVqIlhBUZxDSNr2J2XNIKwOj621cj9ffz80qbC4TgZ83U39hM1+C\n",
       "SkV1ICRwTPgbAM8fASEQL1Yws8kD7+N3X0Gx2UDhSVDZU4DHzdyrz3EcB7ObMVsbsyPi+ZuY/aDe\n",
       "5vWoqtpJtEtC9ki0J8SAay3M74rqiuQgydskcSJ5gXdndT4R6AecXONe/wdsYcaZwJnz2S/HcZxG\n",
       "sPBXVNLqhKwMAzBbC2ltYHfMfl9P83pWVA8A/5LYRmJb4F+EfVWthV/OZ/sf5ZQl2YRPSBeacS7w\n",
       "ZDy9OKbmSF9/2awiYrrjOE5royVUf5cCv6WcP/AVYP96G9ezojqJoOo7Ip4/BFw2Fx1cVEi7pCd5\n",
       "n57MqZdElZiWc81xHMepphtmz5L4rZkZUt3xROtJ8zHLjIvN2Cu+LjFj1rz3t/VQCof06j5v0mR7\n",
       "Za+b8UxO2YdJ88b2znEcpyG0xIrqY6RVSmfSXoR9uXVRj9ffahK3SIyUeDu+3qrn5hI7SoySGC1V\n",
       "h12S2FLiC4lh8XVa6tpYiZdj+XPZtguUu/5vZeDKVMlLMEe3/M/mcN1xHKc1Ui2opOWQHkV6DelV\n",
       "pGNjeT+kh5D+h/QgUp9Um5ORRiONQtp+Ds88GrgEWB3pA0KGhyOab1Km3jQfBeB8YCvgIKB9cw2g\n",
       "5HTxd8KmrveB5yXuNCvZfxIeM6uIh5dgwJZmDRQIxhim9+rM9N6lTLoSBxAicUyq3ZCVgHEN65fj\n",
       "OM7CZQZwHGbDkXoALyI9BPwUeAizc5BOAn4D/AZpTWBfYE1gWeBhpNWwGvnrzN4Eton3FmaT56Zz\n",
       "9ThTdDXjYUBmjDWjCdiljnZDgDGxzQyCE0ZeZuDmVGiNVa+JVXj8tGy69+sIoZSm5rQAwIy343ty\n",
       "HMdZ1KheUZl9iNnweDyF4FC2LCGo9lWx1lWENEQQxvIbMJuB2VhgDGHMz0f6JVIvQoCEC5BeQtqh\n",
       "Zv0M9Qiqr+LqaIzE0RJ7Ul+MumUJqTESxsWyNAZsIjFC4l6JNTPXHpZ4QQpRMhYkKmp1AF7dLyl6\n",
       "KWlNjUoAACAASURBVFOlpqByHMdZhGneRiUNBtYDngWWxmxCvDIBWDoeL0OlVilvfE9zcAyhtD1h\n",
       "i8+BwNn1drge1d8vgG7AscDvgF7AT+poV4/B7iVgOTOmSuwE3A6sFq9tasZ4iaWAhyRGmfFE3k2e\n",
       "kkZsGuLsDTWzoXU8FxLPxUmlz7a3RP/UdRdUjuO0CSRtSWJ3X5uNmqnYA7gV+AVmk0teepB46jU3\n",
       "rjd3LbnRLsA1mL1ace850OyKKq6k9jVjshnvmXGQGXvmecPl8D6QVqstR8auE+87NR7fRwj22i+e\n",
       "j49/PyYIoZrLyk1gbYN75kJIAWwW/pQ+rJUJ2XMTXFA5jtMmMLOhZtZkZk3sWWP8ljoShNQ1mN0e\n",
       "SycgDYjXB1IO3J0d3wfFslq8iPQgsDPwQFQD5tuzcmhWUEU39M2kebIVvQCsKjFYohPB8HZnuoLE\n",
       "0sm9JYYQ7GCfSXST6BnLuxOWi6/M4Xlz7xn4xaA34lFi2EtHSq+OTOE4jrPok+f1J+ByYCRmF6Su\n",
       "3ElZg/YTgtYrKd8PqRPSisCqND8GH0yI6PNtzL4kRDf6ab0drkf1Nxy4Q+JmyqsMM+PfzTUyY6bE\n",
       "0YTIFu2By814XeKweP0SYC/gCImZ8d6JwWgA8O+4MuwAXGfGg/W+qTmhonoCcNMtNxAyGP+bjDoz\n",
       "J3eU4zhOWyBvbNuUEKXnZaRhsexkgh3pJqRDCBqnfcIdbCTSTcBIYCZwJGbNjZkbAyMwm4L0Y2B9\n",
       "4IJm6leg5u8NUml/UUVFs/qlYaOQZJlO5a78VNQ+wPtWsCfjeRNQ4NwPzmTKwN6EUElJpInvA7dl\n",
       "wyM5juO0BVTU+TRxnNUYLxvzUL0CrB1fVxJ8BPbBbIt6mteTiv6g+ehea+FGABXVGfgHYSX3OlMG\n",
       "dgHGmvGVxCnAH4DXXEg5jtOGaQlt0czojPE94ELMLourtLqYo6CSytHFIwZgxsFz189WweaU9aLD\n",
       "CB6M/4vnfyQIKo/h5zhOW6YlBNVkpN8S1IvfRZqrLBz12KjuofzGuhJUYx/MbS9bCTuljgcQ9gVM\n",
       "geA4Em1i06ubOY7jOPPBvsABhP1UHyItD/yp3sb1qP5uSZ9LXE9+VPFWiYpKb04+PnV8OGE19Uaq\n",
       "bEB0h3ccx2mrLPwVldl4pOuADZF2BZ7D7Op6m9cTmSLLasBS89CupZhSozyJ5VdyQzdjQo26juM4\n",
       "bYWWSJy4DyHSxd4Ez8HnkPaut3k9NqoplN+YEdRlVZHQFwG2I+TSApjNlY9+Eo/Pb6H+OI7jLC6c\n",
       "CmyIWdgwLC0FPALcXE/jevJR9TCjZ3z1MmNVM26dnx63AK9ZwR4GtgCwgrVn7JbnAnhwWcdxFjMe\n",
       "aYFnCirMKp8yF0HH68lH9X2JPqnzPlIpgm7rQupfcVpU4hixF4AV7HHKnibzm8LecRxnkcMK9kAL\n",
       "PPZ+Quikg5B+CtwL3Fdv43o2/I4wY51M2XAz1p2X3i5Iqjb8wgWYHVe6HjP4WqF6Y5vEicBPzPhm\n",
       "g7vpOI7TqpBkC3nDr4A9CTFWDXgCs9vqbV6Pe3rem5lj4sQWou4gh4T3flejOuI4juNEworo1via\n",
       "a+oRVC9KnA9cSBBaRwEvzsvDFgKlBZaKc4wh34OQxMtxHMdpBFLaGS+LYdarntvU455+DCFN8Y2E\n",
       "LL1fEYRVayS9oupICJZY6z32oLbruuM4jjO/mPXArGeNV11CCurb8DuFRccdPS25uwDTrFDTCLcU\n",
       "85IaxHEcx1mo1OP193DG66+fREt4jdRDekXVlebj9vWnnATMcRzHaaXUo/pb0ozPkxMzPgOWblyX\n",
       "5ov0CrELzSc/7A8eLslxHKe1U4+gmiWxQnIiMZi5865bmJyAlATM7Urzgqo7bqNyHMdp9dTj9XcK\n",
       "8ITEYwSvv82Bnze0V/PHwPi3K8GZohZzUg06juM4rYB6nCnul/g2QTgNB26nnJK+NXMAsGYz111Q\n",
       "OY7jLALUE5T2UOBYYDlCssHvAE8DWze2a/PNM8B7zVx3QeU4jrMIUI+N6hfAEELK9q2A9YAv6rm5\n",
       "xI4SoyRGS9Uu7hJbSnwhMSy+Tq23bR20o4b7uUQ7oBPN27Acx3EWD6R/Ik1AeiVV1oQ0DmlYfO2U\n",
       "unYy0mikUUjbN7p79diovjJjmgQSXcwYJbH6nBpJ/9/emYfbVZT5+v1lJgOEQAwBAomQINDIEIgM\n",
       "ApGmEWwlODF0q1xERRDbVlsE7u1muH19cLZtGhs1ItIKIjLEgSEoQRwwpoWAkjSkJTZTAjITGRL4\n",
       "3T+q9tnr7LP3PuckZ589fe/z1LNq1VdVq6r22vWtmhkJXAgcDjwE/EZikc2KCq+32hy9kWHrMZba\n",
       "p/VulvPVjCOZgyAIWo1LgH8FiocZGvgCdu+jkKTdSCf27gZsB9yMNAe7YZPsBtKiekBiS9LY1GKJ\n",
       "RcDqAYSbB6yyWZ2P0rgCWFDFX7WtjgYatjqSqT89PdZQBUEQlLBvA56sIqlWPy8ALsdej70aWEWq\n",
       "sxvGQM6jeqvNkzbnAv8IfB0GdMzHdvQeI3owu/WKHjhQYrnEj6WeyQ8DCdsf9VpUt0B5yn0QBEFQ\n",
       "lQ8jLUdaiFTa+GFbUp1cYmPq50ExkK6/HmyWDMb7APz8Fphh82eJo0ittjmDSVMd6rWoQkkFQdA1\n",
       "SJoPzB9ksK8A52f7/wU+D5xcw29Dh1EGpagGyUOkmYIlZtBbC2PzbMF+vcRFElOyv7phK1gOvc/M\n",
       "IrWoYrJEEARdj+0lUG5oSDpnAIHKwyPS1ykfi1RZt2+f3RrGQMaoNpZlwGyJmRJjSINvi4oeJKZJ\n",
       "qQ9UYh6gvEVTv2ELHEBfJQVpwkStrj/oPWgYBEEQFJGmF+7eCpRmBC4CjkcagzQLmE2DN/huWIvK\n",
       "ZoPE6cCNpIMWF9qskDglyy8mHRF/qsQG0iLi4+uFrfGg25EeobwjBQA3X8oxh5/IN2skbwPwvk3M\n",
       "YhAEQWcgXQ4cCmyN9ABwDjAfaS9St979kOpu7HuQrgTuIdWlp9HfUfGbmrwGx99Qeo5Tlj5Ems7e\n",
       "i7cex33XXOE5vcMwBlhnM3q40hkEQdBKDPtR9JtII7v+hpOqO0ycu4RqB3NNIE72DYIgaBs6RVFV\n",
       "bRbuubbqcSTjaY+9CoMgCAI6R1HVpnzsR4loUQVBELQRnaKo6g20Ta+4n0ScQxUEQdA2dIOiqmRL\n",
       "4IlGJSQIgiAYWhq54LdVWdzsBARBEAQDpxtbVEEQBEEb0VWKqrQLRhAEQdA+dJWiIm1U+6IdCisI\n",
       "gqBd6BRFNVBixl8QBEGb0SmKaqD5+CywVSMTEgRBEAwtnaKoBtqV956GpiIIgiAYcjpFUT3Qv5cg\n",
       "CIKgHekMRWX/dIA+r29oOoIgCIIhp9sW/N4L3NTsRARBEAQDpzNaVANnHPVP/Q2CIAhajE5SVOtr\n",
       "SiS/Wn84lXRC5QvDlqIgCIJgk+kkRXVCPeGeLH9vtkaLKgiCoI3oHEVlf7+eeApPPJ2tuw5DaoIg\n",
       "CIIhoqGKSuJIiZUS90l8so6//SQ2SLy94LZa4i6JOySWbmpaJrCutCPFNpsaVxAEQUchfQNpLdLd\n",
       "BbcpSIuR7kW6CWlyQXYW0n1IK5GOaHTyGqaoJEYCFwJHArsBJ0h9WzPZ36eBGypEBubb7G0zb7DP\n",
       "XzOhdxffKDYsyNbvDTauIAiCDucSUl1d5ExgMfYc4Cf5HqTdgONI9fqRwEVIDW30NDLyecAqm9U2\n",
       "64ErgAVV/H0YuAp4rIpsozePfW5M70XAX+DjAHfbMT09CIKgF/ZtwJMVrkcDl2b7pcAx2b4AuBx7\n",
       "PfZqYBUMvjExGBqpqLaj944RD2a3HiS2I2X6K9mpuAu6gZsllkm8f4DPnA9sDbBiKp+pIv/nAcYT\n",
       "BEHQ7UzDXpvta4Fp2b4tqT4v0aduH2oaueB3IEdvfAk408b5rKhiC+ogm0ckpgKLJVba3FYZgaRz\n",
       "C7dLbD+OxJiX+XOV500ZTAaCIAg6AUnzSR/yG4dtpHp1ekMPr22konoImFG4n0FvLQwwF7hCST1t\n",
       "DRwlsd5mkc0jADaPSVxDalr2UVS2z610O/YdsPMT/OmN/90nTaM3KidBEARtjO0lwJLSvaRzBhBs\n",
       "LdI22GuQpgOPZvfKun377NYwGtn1twyYLTFTYgxp8G1R0YPNq21m2cwijVOdarNIYrzEJACJCcAR\n",
       "wN0MAJ0nfe8v4MTlrKoi/s6mZCgIgqCLWAScmO0nAtcW3I9HGoM0C5gNmz4zux4Na1HZbJA4HbgR\n",
       "GAkstFkhcUqWX1wn+DbA1bmlNQr49iAmQYwDXtzl8ao7UKwbaPqDIAi6Buly4FBga6QHgH8CLgCu\n",
       "RDoZWA0cC4B9D9KVwD3ABuA07IZ2/anB8TcUSbbda2agztNUYIXP5bVUNEeFR9iN7UsNgiBodarV\n",
       "na1M5+xMUWYi6bj5VyoFoaSCIAjaj05UVFOA6VRRVEEQBEH70YmK6p+BMTR4umQQBEEwPHSiotqQ\n",
       "ryVFVV5t3eBtPoIgCIKhpxMr7muA71Pu+ntzQbbb8CcnCIIg2BQ6UVFNANaQJlQAvDKalw7N9gGt\n",
       "xQqCIAhah05UVGnWn/1Svn9lA6PHNzNBQRAEwcbTiYpqAr0X9j4FhKIKgiBoUzpRUf1vysfST8a+\n",
       "F/hGj1Q6pBmJCoIgCDaOTlRUUDqE0S4dP/8fBdnVw56aIAiCYKPpCEWl83SAztPDBaf1FV6eKXof\n",
       "hiQFQRAEQ0RHKCrgENJuFJDGp75aIR9XsMdC4CAIgjaiUxTVmIL9Geizc3pRUcXWSkEQBG1Epyiq\n",
       "8wv2cfRVVJsV7NGiCoIgaCPaXlHpPJ1YsI+huqIqTk+PFlUQBEEb0faKCvhmwb45qRvw+Qo/Ewv2\n",
       "bRqdoCAIgmDo6ARFVWRr4Hmf48pW04RfM29hMxIUBEEQbBqdpqj2o29rCmDCLzjotuFOTBAEQbDp\n",
       "dIqi+n2+nk8NRbUZz6+r4h4EQRC0OA1VVBJHSqyUuE/ik3X87SexQeLtgw2bmZWvM6muqHbdgqef\n",
       "67mTYtFvEARBEWk10l1IdyAtzW5TkBYj3Yt0E9LkZiStYYpKYiRwIXAk6RyoEyR2reHv05S2PRpE\n",
       "2ALFWX298iSlnSjG8NKLBefidPUgCIIgLd2Zj7039rzsdiawGHsO8JN8P+w0skU1D1hls9pmPXAF\n",
       "sKCKvw8DVwGPbUTYMhvGXJltsyskYwFex6+Li37XRasqCIKgD5X14tHApdl+KXDM8CYn0UhFtR3w\n",
       "QOH+wezWg8R2JAX0lexUWozbb9g+rHvVsTUkE4E/zeDByjGq2EU9CIKgjIGbkZYhvT+7TcNem+1r\n",
       "gWnNSNioBsY9kB0gvgScaePcRVfS5gPfPeIW4FbOY98XzmF3yqNVZSaRTvutnLK+BGkm9h8H/Kwg\n",
       "CII2RNJ8YH4/3g7CfgRpKrAYaWUvqW2kpuzs00hF9RAwo3A/g9QyKjIXuCJ3wm0NHCWxfoBhE2/g\n",
       "K9zq83jmh+cw6y0ASIywexRTOvEX7q8Sun6LUhoPrMOObsIgCNoW20uAJaV7SedU8fRIvj6GdA1p\n",
       "CGYt0jbYa5CmA48OR3oraWTX3zJgtsRMiTHAccCiogebV9vMsplFGqc61WbRQMIWuAF4Hy9NSHcv\n",
       "jYc0+6/EJJKyeSgrnG8VZAchXVc1VulHwBf7zWWMdQVB0O5I45EmZfsE4AjgblK9W9qm7kTg2mYk\n",
       "r2EtKpsNEqcDNwIjgYU2KyROyfKLBxu2hvfngTMZmY+gWvhLgJWUd1T/DrBjwX/xbKrLemzS5sCW\n",
       "ha7AN1F5rpU0DrtyH8FXkN6JfVWt/ARBELQ404BrSN/do4BvY9+EtAy4EulkYDVQay5AQ2lk1x82\n",
       "1wPXV7hVVVA2J/UXtgYvAK/mgQPg1n9cx9o9JwCjC/IdK/xv6BODNIKktI5GeiP2TVlyPzAHaUdS\n",
       "d+TzSCNyX+0plL80XlOI672kH/lFpC8C9wGfwm7K+oMgCIJ+se8H9qri/gRw+LCnp4JO2JkirY96\n",
       "aRLccv69JcfS+qkq9FVU8DJpGibAjYUBw9J6qz0pr9V6BWkaqcV1QHabmR86H1gI7I+0GfD3wNuB\n",
       "LXqeJE3uWTQnzUOageQ8HlbyMyVft0f6dK2MZz+z6nY/SlsgbVk3jiAIghamExTVC5SUVe/ZgqOr\n",
       "+IW0aG2glCZ0XAc8XXBfQ1mxAZyM9B7SHESArwF/zvbDenxJc4EngSeRbgd+DfwqS3+Y/fwSeBzp\n",
       "COBHwBlZkbkQzw5I2+a7PwAfr5BvhTQn3/0U+C+knZGKa8lKfkcgzc0KbWLBfVqPf2lXpFFVwyf5\n",
       "IUifqCor+/lgPwp1GtLYOvIRvdIXBEH3YLttDWDOZQ7YVcyU5McGf6ZXWBhbLUALmEn9yF9lWFm4\n",
       "X10hP8gwrXD/hgr5upz/0v0lfZ6R5JsV3LbO16vydfPsZ7bhbMOYgt8PGGZkubLbtoYPZvu+hr0L\n",
       "v8Nehh0Mo7P8YsO+Fb/T3Gz/Qk/6ev+W2xqU7d/N6d266juTym+yYXzBbbrhjLrvGnzCMLGO/DDD\n",
       "X2X7FMPYfuKrL68eZjvD/DpyGf624f872NUwotn//SHIx5WGs5qejiYZqv2XWtg0PQGbWticyw51\n",
       "6vZX5+sH+oSHp5qgiDbVPDgAP9/pR75/P/KjDLdUcV+Tr7cb9ii4/7KK32m5QquXhjmF+w+4pEjT\n",
       "9Uf5NyrJD6wIf3aW75Lvf2FYmu2/ztfdXFaWNmyVr7/L12NyHF/K9//upJBsuN+wuPCulJ4hw6mG\n",
       "nQyfd6nCLj9jXL6+bPgnw6QsH2/4TQ4/t5Cn8wvPON/wDidF93Wnj5Z3GrbI8lU53ETDzYYDK97n\n",
       "0sfFXxm2NHzT6SNibMHP8YY9sv0Mw34Vccx1+l+U8mnD1Io4bPhV9rOPSx8JZflWhu2y/WjDlhXy\n",
       "EYaDs30bJwU8tZe/9IzPGUYZDjWMrIhjdqEsRhjGVPl/f94wuWb9UfrNkl1V8qGesqodx7/ldE7P\n",
       "ZTfTcGzFb/I3dcK/ricNtf08Z/jcRteT5Y/yXvkLRTWciU+KakZ+516sUifumq8nV/kBp+Y/dJ9A\n",
       "YVrCnNSP/N8GEMdZ/chH9CP/muEbhfuf5uuTBbdxBfv9VeKY6NTislMFXyk/Nb+Ple63FOzbFuw/\n",
       "rOJ3RoWfkvlZvu5R8YwT8rX04TM9yy/L95dXiWt2RRzP5uvbnCroSqVd/BD5veHoLH9zdvtoRVzP\n",
       "Fv6bJffF+XqTwVXkxbLYuZCG4u/6r/k63iWl3zuOUrldYJhWkF9XyMffOr1vBxumVonjxV6/b1le\n",
       "+ojbN5fpvi4qVXhvlr/OqYV3YpV6qvSMh/P10Ar597J7qS67sEJe+sg7yPBhZ8UZimo4Ew/mXMbn\n",
       "3/LAKnXNHvn67prxpC/AUtfTQsPhhr82jKzyZy2a7/UjDxNmoObcYXjG4f3IjxpAHBf1I9++H/lY\n",
       "w6115Ge4/F+sZSb0I9/CSXHW8zOuilvp4+PLTh8Xd+T7G6r4rVTatdJRSzYzh39nFdk+htOcuqmP\n",
       "ryL/eb7Ode8eg6L5kJNyHWnYvYp8ZCiq4Uw8GKxc/ptV+c0Oz9d5/cYHO7qyCyG9rGsMTxQi3dzw\n",
       "v6q8qGfn6ySnL6eRhmtrvEg/NKx3KLswYQZr/qcF0mCXW1wba1ZtYni73G09WLN7KKrhTDzFrmaX\n",
       "Jk6srPhdbt2k56Svq8lOX3I7VMjmuffgf99B/DROsMZwsmGD4aMF2UjDHwt/vhtd+QJVf9Euzc+e\n",
       "aHiphp//4/SV+I4a8v+Xr/11sdUzn8rXa/rxd8sQ/CnDhAkzRCYU1XAmnqqK6lbw7YXf5IZmp7Nf\n",
       "k5TZFKem/GYVsrFOrbS/zvKd3Hfgd3+nGW9nOPWl71eQKctPzAXyVVe+pL3HVkqtwHcX5EcZVjgp\n",
       "3Kez/LGCvLIr5ZF8lfuOXdjlsYcfOY2v7G24sMafapRTt9WUGvIFThMH5tf5Y37UcJfhb2rItzA8\n",
       "4zQLsZr8Y/n64zrPONxposnpddJgp66lWnFcX0dml8cbzqwhf6fhsX7ieCFfX64h/0O+VpskM1jz\n",
       "8yGII0wDTCiq4Uw8VRXVD8CvLfwmVzc7nS1hktIY3Y+ftzgphvGuVIZJvqUrZ3H1lo80HJXtu9fw\n",
       "s4+TYjvYsG3BfYTTLKkDDe93agl+qyLsYTkN/+00DX2NS9Plk3yqU//8h52U8/0uddOWy+BXhlOc\n",
       "ZhqeZnih4hkvG/7kNBHj9vwS7V+Qf7/HrTz7sdhKnpXdbnaxe6j3M0ovZ3G85pCC/HPZ7VKnCR0v\n",
       "GI6rKGcXTMn/5CrPeNjw22y/riD/eIWfeuksmjc7fbhMdmnJQF8zMv/GlbM1S2asU0u+1njTp5w+\n",
       "zHZyeTJE32ekNNYaC5rn0vteXV6c1bm0hp9tDLvUicOGY3I6a8lf6/QBVG08yi7PXK01Jv4v+bqw\n",
       "zjPeYFjr3rNxi6bk3jN5LBTVcCa+uqL6dsHecx8mTMuY1IIeV0c+wrBVP3HMcpoWXf3jIw3GF2ex\n",
       "jevlN32QHFGozHcxnFARxw6GPZ0mOIwxvMvFNVTlaewjckX7Dhc/HJKfk5yWKkxwWgKwvEL+esOb\n",
       "nGbf7Wj4tIvr3JKffbP8VU4zLxdUyOcZXuM0m/F0w6cq5G9zmhTx705T2xcaphTkh+TK4i9ypX6R\n",
       "4ZGKOOz0gWSXZ0eqIJ+e0zgl5/kZl5ZA9M7HvKx8fmZYWSFfluP9UE6LDe8qyEsfBj91+oB52vD7\n",
       "gryo7H5nuNPwUs87l67fb0dFJduNXVHcQCTlMudFm3ESBv7B5vMSo4GXSBvavq+pCQ2CIOiPtHPL\n",
       "KOz1+T6dpTeYSjrtWPM89pP9PMpuo+OLGrop7TBS2npnAmk3dWzW5w17tqgRJgiCoHVICml94f7Z\n",
       "jYjj4SFMUcvQKYrqJAC7Z3+9In3P/A2CIAjahk5QVAfY3F5H/nQdWRAEQdDidMLu6c/1Iz+sH3kQ\n",
       "BEHQwnSCouqvH/fOYUlFEARB0BC6QVE9NSypCIIgCBpCQxWVxJESKyXuk/hkFfkCieUSd0j8p1Tu\n",
       "ppNYLXFXli2t85hqEyhKnATUP9AvCIIgAOlIpJVI9yH1qa+bScPWUUmMBP4LOBx4CPgNcILNioKf\n",
       "CTbrsn0P4BqbnfP9/cBcmydqP0MGj7J5uSGZaCMkzbe9pNnpaAWiLMpEWZSJsijTZx2VVLW+xl5R\n",
       "PYbhpZEtqnnAKpvVNuuBK4AFRQ8lJZWZCPypIo5+F6SFkuphfrMT0ELMb3YCWoj5zU5ACzG/2Qlo\n",
       "YeYBq7BX5wXHferrZtJIRbUd8EDh/sHs1guJYyRWANcDf1cQGbhZYpnE+xuYziAIgm5nQPV1s2jk\n",
       "OqoB9SnaXAtcK3EwcBmwSxYdZPOIxFRgscRKm9salNYgCIJupqX30mukonoImFG4n0HS0lWxuU1i\n",
       "lMRWNo/bPJLdH5O4htQ07aOo0jhVACDpnGanoVWIsigTZVEmyqImg6qvh5tGKqplwGyJmcDDwHHA\n",
       "CUUPEjsBf7CxxD4ANo9LjAdG2jwrMQE4Ajiv8gHttKliEARBC7MMmI00kxr1dTNpmKKy2SBxOnAj\n",
       "MJK0i/kKiVOy/GLg7cB7JNaTdpg4PgffBrg6byo7Cvi2zU2NSmsQBEFXY29A6lVft8qMP2jg9PQg\n",
       "CIIgGAradmcKSUdKWinpPrXY4rShQNIMSbdI+r2k30n6u+w+RdJiSfdKuknS5EKYs3J5rJR0RMF9\n",
       "rqS7s+xfmpGfoUDSSEl3SPpBvu/KspA0WdJVklZIukfS67q4LM7K/5G7JX1H0thuKQtJ35C0VtLd\n",
       "Bbchy3suy+9m99sl7Th8uaug2Sc3bowhNU1XATOB0aT9/HZtdrqGOI/bAHtl+0TSYrxdgc8AZ2T3\n",
       "TwIXZPtuuRxG53JZRbnFvBSYl+0/Bo5sdv42skw+BnwbWJTvu7IsgEuB92b7KNKZa11XFjk/fwDG\n",
       "5vvvAid2S1kABwN7A3cX3IYs78BpwEXZfhxwRbPy2q4tqryY2KvdgovThgLba2zfme3PAStI6xqO\n",
       "JlVU5Osx2b4AuNz2eturSS/i6yRNBybZLm1D9a1CmLZB0vbAm4CvU14I3nVlIWkL4GDb3wCwvcH2\n",
       "03RhWQDPkA4aHC9pFDCeNBGgK8rC9m1A5Um+Q5n3YlzfB/5yyDMxQNpVUbX04rShRmkmzt7Ar4Fp\n",
       "ttdm0VpgWrZvS+/ppKUyqXR/iPYsqy+S9m18peDWjWUxC3hM0iWSfivpa5Im0IVlYfsJ4PPA/5AU\n",
       "1FO2F9OFZVFgKPPeU8/a3gA8LWlKg9Jdl3ZVVF0zA0TSRNLXzEdccTS1U5u848tC0puBR23fQY1t\n",
       "tbqlLEhdffuQumT2AdYBZxY9dEtZSNoJ+HtSV9a2wERJ7yr66ZayqEYn5b1dFVVLL04bKiSNJimp\n",
       "y2xfm53XStomy6cDj2b3yjLZnlQmD2V70f2hRqa7ARwIHC3pfuBy4DBJl9GdZfEg8KDt3+T7q0iK\n",
       "a00XlsW+wC9tP56/+K8GDqA7y6LEUPwnHiyE2SHHNQrYIrdih512VVR5MbFmShpDGuhb1OQ0DSmS\n",
       "BCwE7rH9pYJoEWnAmHy9tuB+vKQxkmYBs4GlttcAz+SZYQLeXQjTFtg+2/YM27NIa+1+avvddGdZ\n",
       "rAEekDQnOx0O/B74AV1WFsBKYH9Jm+U8HA7cQ3eWRYmh+E9cVyWudwA/GY4MVKVZszg21QBHkWbC\n",
       "rQLOanZ6GpC/15PGY+4E7sjmSGAKcDNwL3ATMLkQ5uxcHiuBNxbc5wJ3Z9mXm523TSyXQynP+uvK\n",
       "sgD2JB3DsJzUitiii8viDJKivps08D+6W8qC1LvwMPASaSzppKHMOzAWuBK4D7gdmNmsvMaC3yAI\n",
       "gqCladeuvyAIgqBLCEUVBEEQtDShqIIgCIKWJhRVEARB0NKEogqCIAhamlBUQRAEQUsTiiro+QTs\n",
       "SAAAAmhJREFUaiT9Il93lDSkJ5pKOrvas4IgGByxjioIAEnzgY/bfssgwoxy2rqnlvxZ25OGIn1B\n",
       "0M1EiyroaiQ9l60XAAcrHcz4EUkjJH1W0lJJyyV9IPufL+k2SdcBv8tu10papnTA5fuz2wXAZjm+\n",
       "y4rPUuKz+bC6uyQdW4h7iaTvKR2K+B+FdF6gdEDgckmfHa7yCYJWYFSzExAETabUpfBJ4B9KLaqs\n",
       "mJ6yPU/SWODnkm7KfvcGdrf9x3x/ku0nJW0GLJV0le0zJX3I9t5VnvU20jZIrwWmAr+R9LMs24t0\n",
       "yN0jwC8kHUTa8uYY26/Jadt8aIsgCFqbaFEFQaLy+JAjgPdIuoO0z9kUYOcsW1pQUgAfkXQn8CvS\n",
       "DtWz+3nW64HvOPEocCuwH0mRLbX9sFOf/J3AjsBTwAuSFkp6K/D8RucyCNqQUFRBUJvTbe+dzU62\n",
       "b87u60oe8tjWXwL7296LtHnwuH7iNX0VY6m19WLB7WVgtO2XSadaXwW8GbhhYzITBO1KKKogSDwL\n",
       "FCc+3Aicls/hQdIcSeOrhNsceNL2C5JeA+xfkK0vha/gNuC4PA42FTgEWEqNQyHzCb6TbV8PfIzU\n",
       "bRgEXUOMUQXdTqklsxx4OXfhXQJ8mXRy7G/zOT2PAm/N/otTZW8APijpHtKxM78qyL4K3CXpP53O\n",
       "zzKA7WskHZCfaeATth+VtCt9T2Q1SYFeJ2kcSZl9dEhyHgRtQkxPD4IgCFqa6PoLgiAIWppQVEEQ\n",
       "BEFLE4oqCIIgaGlCUQVBEAQtTSiqIAiCoKUJRRUEQRC0NKGogiAIgpYmFFUQBEHQ0vx/14yYb5ns\n",
       "3AUAAAAASUVORK5CYII=\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104744450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def smoothen_data(data, smooth_window=100):\n",
    "    smooth = []\n",
    "    for i in xrange(len(data)-smooth_window):\n",
    "        smooth.append(np.mean(data[i:i+smooth_window]))\n",
    "\n",
    "    for i in xrange(len(data)-smooth_window, len(data)):\n",
    "        smooth.append(np.mean(data[i:len(data)]))\n",
    "    return smooth\n",
    "   \n",
    "smooth_accs_train = smoothen_data(train_accuracies)\n",
    "smooth_accs_test = smoothen_data(test_accuracies)\n",
    "                    \n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "train_accs_line, = ax1.plot(xrange(len(smooth_accs_train)), smooth_accs_train, 'b-', label='train accuracies')\n",
    "test_accs_line, = ax1.plot(xrange(len(smooth_accs_test)), smooth_accs_test, 'g-', label='test accuracies')\n",
    "ax1.set_ylabel('accuracies', color='b')\n",
    "ax1.set_xlabel('iterations')\n",
    "for tl in ax1.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "losses_line, = ax2.plot(xrange(len(losses)), losses, 'r-', label='losses')\n",
    "ax2.set_ylabel('losses', color='r')\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('r')\n",
    "\n",
    "plt.legend(handles=[losses_line, train_accs_line, test_accs_line],bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=2, mode=\"expand\")\n",
    "# plt.legend([losses_line, train_accs_line, test_accs_line], ['losses', 'train accuracies', 'test accuracies'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
