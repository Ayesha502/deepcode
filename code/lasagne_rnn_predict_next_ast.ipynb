{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Knowledge Tracing\n",
    "Authors: Lisa Wang, Angela Sy\n",
    "\n",
    "Task: Predict what the student is going to code next.\n",
    "\n",
    "Input: For each of the N students, we have a time series of Abstract Syntax Trees (ASTs), which represent the student's code at that time step.\n",
    "- input shape (num_students, num_timesteps, num_asts)\n",
    "    - num_timesteps is the max sequence length of asts that we are taking into account.\n",
    "    - num_asts is the total number of asts for that problem.\n",
    "\n",
    "Output: At each timestep, we are predicting the next AST. \n",
    "- Output shape (num_students, num_timesteps). The values will be in the range (0, num_asts).\n",
    "\n",
    "The truth matrix contains the desired output for a given input, and is used to compute the loss as well as train/val/test accuracies.\n",
    "- Truth shape (num_students, num_timesteps)\n",
    "\n",
    "\n",
    "There are few ways to calculate the loss and perform the training:\n",
    "    1. We concatenate all trajectories (compare to comparing fragments of text to a bigger corpus) and use a sliding window, very similar to predicting the next character.\n",
    "    \n",
    "    2. We could train the model on each trajectory individually. We use a sliding window, e.g. of 3 ASTs.\n",
    "    \n",
    "Current Issues:\n",
    "    1. AST IDs are not consistent across different HOCs. Hence, we can only train and run this model on each HOC individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as Tensor\n",
    "import lasagne\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# allows plots to show inline in ipython notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import our own modules\n",
    "import utils\n",
    "import model_predict_ast as model\n",
    "import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each trajectory matrix corresponds to one hoc exercise and is\n",
    "# its own data set. Mixing data sets currently does not make much\n",
    "# sense since the AST IDs don't persist betweeen different hoc's.\n",
    "TRAJ_MAP = {\n",
    "    'hoc1': '../processed_data/traj_matrix_1.npy',\n",
    "    'hoc2': '../processed_data/traj_matrix_2.npy',\n",
    "    'hoc3': '../processed_data/traj_matrix_3.npy',\n",
    "    'hoc4': '../processed_data/traj_matrix_4.npy',\n",
    "    'hoc5': '../processed_data/traj_matrix_5.npy',\n",
    "    'hoc6': '../processed_data/traj_matrix_6.npy',\n",
    "    'hoc7': '../processed_data/traj_matrix_7.npy',\n",
    "    'hoc8': '../processed_data/traj_matrix_8.npy',\n",
    "    'hoc9': '../processed_data/traj_matrix_9.npy' \n",
    "}\n",
    "\n",
    "TRAJ_MAP_PREFIX = '../processed_data/traj_matrix_'\n",
    "TRAJ_MAP_SUFFIX = '.npy'\n",
    "\n",
    "HOC_NUM = str(7)\n",
    "DATA_SET = 'hoc' + HOC_NUM\n",
    "# if DATA_SZ = -1, use entire data set\n",
    "# For DATA_SZ, powers of 2 work best for performance.\n",
    "DATA_SZ = -1\n",
    "\n",
    "# DATA_SZ = -1\n",
    "\n",
    "AST_MAP_FILE = '../processed_data/map_ast_row_' + HOC_NUM + '.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ast_id_to_row_map = pickle.load(open( AST_MAP_FILE, \"rb\" ))\n",
    "# print ast_id_to_row_map[-1]\n",
    "\n",
    "row_to_ast_id_map = {v: k for k, v in ast_id_to_row_map.items()}\n",
    "# print row_to_ast_id_map[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4195, 7, 432)\n"
     ]
    }
   ],
   "source": [
    "# trajectories matrix for a single hoc exercise\n",
    "# shape (num_traj, max_traj_len, num_asts)\n",
    "# Note that ast_index = 0 corresponds to the <END> token,\n",
    "# marking that the student has already finished.\n",
    "# The <END> token does not correspond to an AST.\n",
    "traj_mat = np.load(TRAJ_MAP[DATA_SET])\n",
    "print traj_mat.shape\n",
    "# print traj_mat[:10, :, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4195, 7, 432)\n"
     ]
    }
   ],
   "source": [
    "# if DATA_SZ specified, reduce matrix. \n",
    "# Useful to create smaller data sets for testing purposes.\n",
    "if DATA_SZ != -1:\n",
    "    traj_mat = traj_mat[:DATA_SZ]\n",
    "print traj_mat.shape\n",
    "# print traj_mat_sm[:, :, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shuffle the first dimension of the matrix\n",
    "np.random.shuffle(traj_mat)\n",
    "# print traj_mat_sm[:, :, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_traj, max_traj_len, num_asts = traj_mat.shape\n",
    "# Split data into train, val, test\n",
    "# TODO: Replace with kfold validation in the future\n",
    "# perhaps we can use sklearn kfold?\n",
    "\n",
    "train_mat = traj_mat[0:7*num_traj/8,:]\n",
    "val_mat =  traj_mat[7*num_traj/8: 15*num_traj/16 ,:]\n",
    "test_mat = traj_mat[15*num_traj/16:num_traj,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing network inputs and targets...\n",
      "(3670, 6, 432)\n",
      "(3670, 6)\n",
      "(262, 6, 432)\n",
      "(263, 6, 432)\n",
      "[[ 103.   42.   -1.   -1.   -1.   -1.]\n",
      " [   1.    0.   -1.   -1.   -1.   -1.]\n",
      " [   4.    1.   11.    3.    0.   -1.]\n",
      " [   8.   -1.   -1.   -1.   -1.   -1.]\n",
      " [  16.   45.   -1.   -1.   -1.   -1.]\n",
      " [   2.   10.    0.   -1.   -1.   -1.]\n",
      " [   1.    3.    4.    1.    0.   -1.]\n",
      " [   1.    3.    9.    0.   -1.   -1.]\n",
      " [   3.   33.   42.   -1.   -1.   -1.]\n",
      " [   4.   12.   -1.   -1.   -1.   -1.]]\n",
      "[[ 42.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  0.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  1.  11.   3.   0.  -1.  -1.]\n",
      " [ -1.  -1.  -1.  -1.  -1.  -1.]\n",
      " [ 45.  -1.  -1.  -1.  -1.  -1.]\n",
      " [ 10.   0.  -1.  -1.  -1.  -1.]\n",
      " [  3.   4.   1.   0.  -1.  -1.]\n",
      " [  3.   9.   0.  -1.  -1.  -1.]\n",
      " [ 33.  42.  -1.  -1.  -1.  -1.]\n",
      " [ 12.  -1.  -1.  -1.  -1.  -1.]]\n",
      "6\n",
      "Inputs and targets done!\n"
     ]
    }
   ],
   "source": [
    "print('Preparing network inputs and targets...')\n",
    "# X_train, y_train = utils.prepare_traj_data_for_rnn(train_data)\n",
    "# X_val, y_val = utils.prepare_traj_data_for_rnn(val_data)\n",
    "# X_test, y_test = utils.prepare_traj_data_for_rnn(test_data)\n",
    "\n",
    "train_data = utils.prepare_traj_data_for_rnn(train_mat)\n",
    "val_data = utils.prepare_traj_data_for_rnn(val_mat)\n",
    "test_data = utils.prepare_traj_data_for_rnn(test_mat)\n",
    "\n",
    "\n",
    "X_train, y_train = train_data\n",
    "X_val, y_val = val_data\n",
    "X_test, y_test = test_data\n",
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_val.shape\n",
    "print X_test.shape\n",
    "num_train, num_timesteps, num_asts = X_train.shape\n",
    "\n",
    "X_train_ast_ids, y_train_ast_ids = utils.convert_data_to_ast_ids(train_data, row_to_ast_id_map)\n",
    "print X_train_ast_ids[:10]\n",
    "print y_train_ast_ids[:10]\n",
    "\n",
    "print num_timesteps\n",
    "print (\"Inputs and targets done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 200 # size of hidden layer of neurons\n",
    "learning_rate = 1e-2\n",
    "lr_decay = 1.0\n",
    "reg_strength = 1e-2\n",
    "grad_clip = 10\n",
    "batchsize = 32\n",
    "num_epochs = 2\n",
    "dropout_p = 0.2\n",
    "num_lstm_layers = 2\n",
    "theano.config.optimizer='fast_compile'\n",
    "theano.config.exception_verbosity='high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Compiling done!\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "train_loss_acc, compute_loss_acc, probs = model.create_model(num_timesteps, num_asts, hidden_size, learning_rate, grad_clip, dropout_p, num_lstm_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "  Epoch 0 \tbatch 1 \tloss 6.07154025508 \ttrain acc 0.00 \tval acc 64.45 \n",
      "  Epoch 0 \tbatch 2 \tloss 5.99923625596 \ttrain acc 65.10 \tval acc 65.17 \n",
      "  Epoch 0 \tbatch 3 \tloss 5.48470692779 \ttrain acc 65.62 \tval acc 65.17 \n",
      "  Epoch 0 \tbatch 4 \tloss 3.78036060032 \ttrain acc 66.15 \tval acc 65.17 \n",
      "  Epoch 0 \tbatch 5 \tloss 2.35152975175 \ttrain acc 62.50 \tval acc 65.17 \n",
      "  Epoch 0 \tbatch 6 \tloss 1.68422071387 \ttrain acc 66.67 \tval acc 65.17 \n",
      "  Epoch 0 \tbatch 7 \tloss 1.55897584048 \ttrain acc 72.92 \tval acc 65.17 \n",
      "  Epoch 0 \tbatch 8 \tloss 1.83013854068 \ttrain acc 69.79 \tval acc 65.17 \n",
      "  Epoch 0 \tbatch 9 \tloss 2.13903497033 \ttrain acc 64.58 \tval acc 65.17 \n",
      "  Epoch 0 \tbatch 10 \tloss 2.15576826638 \ttrain acc 61.98 \tval acc 65.17 \n",
      "  Epoch 0 \tbatch 11 \tloss 1.79637686514 \ttrain acc 68.75 \tval acc 65.23 \n",
      "  Epoch 0 \tbatch 12 \tloss 1.87349463054 \ttrain acc 64.06 \tval acc 65.30 \n",
      "  Epoch 0 \tbatch 13 \tloss 1.92350743189 \ttrain acc 60.94 \tval acc 65.36 \n",
      "  Epoch 0 \tbatch 14 \tloss 1.77123985656 \ttrain acc 63.54 \tval acc 65.36 \n",
      "  Epoch 0 \tbatch 15 \tloss 2.02093806185 \ttrain acc 61.98 \tval acc 65.30 \n",
      "  Epoch 0 \tbatch 16 \tloss 2.00312186062 \ttrain acc 64.06 \tval acc 65.56 \n",
      "  Epoch 0 \tbatch 17 \tloss 1.59355477762 \ttrain acc 63.02 \tval acc 64.52 \n",
      "  Epoch 0 \tbatch 18 \tloss 1.43176837979 \ttrain acc 69.79 \tval acc 66.15 \n",
      "  Epoch 0 \tbatch 19 \tloss 1.76812523553 \ttrain acc 65.10 \tval acc 65.49 \n",
      "  Epoch 0 \tbatch 20 \tloss 1.602760392 \ttrain acc 68.23 \tval acc 64.84 \n",
      "  Epoch 0 \tbatch 21 \tloss 1.82775762236 \ttrain acc 61.98 \tval acc 63.54 \n",
      "  Epoch 0 \tbatch 22 \tloss 1.89021197701 \ttrain acc 61.46 \tval acc 64.97 \n",
      "  Epoch 0 \tbatch 23 \tloss 1.52514622561 \ttrain acc 65.10 \tval acc 66.67 \n",
      "  Epoch 0 \tbatch 24 \tloss 1.49080888836 \ttrain acc 69.79 \tval acc 65.89 \n",
      "  Epoch 0 \tbatch 25 \tloss 1.5073204962 \ttrain acc 69.27 \tval acc 66.15 \n",
      "  Epoch 0 \tbatch 26 \tloss 1.70646098958 \ttrain acc 68.75 \tval acc 66.21 \n",
      "  Epoch 0 \tbatch 27 \tloss 1.677176225 \ttrain acc 60.94 \tval acc 66.28 \n",
      "  Epoch 0 \tbatch 28 \tloss 1.49214208361 \ttrain acc 67.19 \tval acc 66.15 \n",
      "  Epoch 0 \tbatch 29 \tloss 1.47646245747 \ttrain acc 67.71 \tval acc 65.17 \n",
      "  Epoch 0 \tbatch 30 \tloss 1.58193321375 \ttrain acc 63.54 \tval acc 66.47 \n",
      "  Epoch 0 \tbatch 31 \tloss 1.54017139966 \ttrain acc 65.62 \tval acc 66.86 \n",
      "  Epoch 0 \tbatch 32 \tloss 1.34785698705 \ttrain acc 68.75 \tval acc 66.41 \n",
      "  Epoch 0 \tbatch 33 \tloss 1.9544992747 \ttrain acc 63.02 \tval acc 66.80 \n",
      "  Epoch 0 \tbatch 34 \tloss 1.42344986027 \ttrain acc 66.67 \tval acc 67.19 \n",
      "  Epoch 0 \tbatch 35 \tloss 1.61630322641 \ttrain acc 66.67 \tval acc 66.67 \n",
      "  Epoch 0 \tbatch 36 \tloss 1.58211614244 \ttrain acc 66.15 \tval acc 66.93 \n",
      "  Epoch 0 \tbatch 37 \tloss 1.31332738481 \ttrain acc 70.31 \tval acc 67.12 \n",
      "  Epoch 0 \tbatch 38 \tloss 1.57813480769 \ttrain acc 64.06 \tval acc 66.93 \n",
      "  Epoch 0 \tbatch 39 \tloss 1.43916959342 \ttrain acc 66.15 \tval acc 67.32 \n",
      "  Epoch 0 \tbatch 40 \tloss 1.63930509069 \ttrain acc 60.42 \tval acc 67.19 \n",
      "  Epoch 0 \tbatch 41 \tloss 1.35771028549 \ttrain acc 69.79 \tval acc 67.58 \n",
      "  Epoch 0 \tbatch 42 \tloss 1.34471095698 \ttrain acc 69.27 \tval acc 66.47 \n",
      "  Epoch 0 \tbatch 43 \tloss 1.55673905303 \ttrain acc 63.54 \tval acc 66.93 \n",
      "  Epoch 0 \tbatch 44 \tloss 1.24969110794 \ttrain acc 70.31 \tval acc 67.45 \n",
      "  Epoch 0 \tbatch 45 \tloss 1.34001904372 \ttrain acc 66.15 \tval acc 67.64 \n",
      "  Epoch 0 \tbatch 46 \tloss 1.50235347841 \ttrain acc 69.79 \tval acc 67.71 \n",
      "  Epoch 0 \tbatch 47 \tloss 1.35793756826 \ttrain acc 67.19 \tval acc 67.90 \n",
      "  Epoch 0 \tbatch 48 \tloss 1.31428550533 \ttrain acc 68.75 \tval acc 68.42 \n",
      "  Epoch 0 \tbatch 49 \tloss 1.64623104824 \ttrain acc 66.67 \tval acc 68.10 \n",
      "  Epoch 0 \tbatch 50 \tloss 1.37614933747 \ttrain acc 68.75 \tval acc 68.62 \n",
      "  Epoch 0 \tbatch 51 \tloss 1.4111272564 \ttrain acc 70.31 \tval acc 68.23 \n",
      "  Epoch 0 \tbatch 52 \tloss 1.45490993391 \ttrain acc 66.15 \tval acc 67.38 \n",
      "  Epoch 0 \tbatch 53 \tloss 1.16110008991 \ttrain acc 70.31 \tval acc 68.23 \n",
      "  Epoch 0 \tbatch 54 \tloss 1.58101980458 \ttrain acc 68.75 \tval acc 69.21 \n",
      "  Epoch 0 \tbatch 55 \tloss 1.24223324697 \ttrain acc 71.88 \tval acc 69.34 \n",
      "  Epoch 0 \tbatch 56 \tloss 1.38258252261 \ttrain acc 69.27 \tval acc 69.21 \n",
      "  Epoch 0 \tbatch 57 \tloss 1.32674061024 \ttrain acc 69.27 \tval acc 68.82 \n",
      "  Epoch 0 \tbatch 58 \tloss 1.36773252195 \ttrain acc 68.23 \tval acc 68.75 \n",
      "  Epoch 0 \tbatch 59 \tloss 1.36819435075 \ttrain acc 67.19 \tval acc 69.86 \n",
      "  Epoch 0 \tbatch 60 \tloss 1.46701146899 \ttrain acc 67.19 \tval acc 70.44 \n",
      "  Epoch 0 \tbatch 61 \tloss 1.37452922839 \ttrain acc 69.79 \tval acc 70.12 \n",
      "  Epoch 0 \tbatch 62 \tloss 1.46889055211 \ttrain acc 66.67 \tval acc 70.12 \n",
      "  Epoch 0 \tbatch 63 \tloss 1.30732803321 \ttrain acc 69.27 \tval acc 70.12 \n",
      "  Epoch 0 \tbatch 64 \tloss 1.28193711027 \ttrain acc 71.35 \tval acc 70.31 \n",
      "  Epoch 0 \tbatch 65 \tloss 1.51016971052 \ttrain acc 66.67 \tval acc 70.18 \n",
      "  Epoch 0 \tbatch 66 \tloss 1.21278174874 \ttrain acc 70.31 \tval acc 70.05 \n",
      "  Epoch 0 \tbatch 67 \tloss 1.54265017437 \ttrain acc 67.71 \tval acc 71.16 \n",
      "  Epoch 0 \tbatch 68 \tloss 1.0754172227 \ttrain acc 73.44 \tval acc 71.03 \n",
      "  Epoch 0 \tbatch 69 \tloss 1.28583329605 \ttrain acc 71.88 \tval acc 71.29 \n",
      "  Epoch 0 \tbatch 70 \tloss 1.20595605002 \ttrain acc 70.31 \tval acc 71.81 \n",
      "  Epoch 0 \tbatch 71 \tloss 1.42428037211 \ttrain acc 69.79 \tval acc 71.55 \n",
      "  Epoch 0 \tbatch 72 \tloss 1.4380320432 \ttrain acc 71.35 \tval acc 71.09 \n",
      "  Epoch 0 \tbatch 73 \tloss 1.45500097268 \ttrain acc 69.79 \tval acc 71.68 \n",
      "  Epoch 0 \tbatch 74 \tloss 1.4116180692 \ttrain acc 68.75 \tval acc 72.33 \n",
      "  Epoch 0 \tbatch 75 \tloss 1.32777280077 \ttrain acc 73.96 \tval acc 72.72 \n",
      "  Epoch 0 \tbatch 76 \tloss 1.15395083796 \ttrain acc 74.48 \tval acc 72.59 \n",
      "  Epoch 0 \tbatch 77 \tloss 1.17842128096 \ttrain acc 74.48 \tval acc 72.85 \n",
      "  Epoch 0 \tbatch 78 \tloss 1.45481266119 \ttrain acc 68.75 \tval acc 72.98 \n",
      "  Epoch 0 \tbatch 79 \tloss 1.24883297201 \ttrain acc 76.04 \tval acc 72.72 \n",
      "  Epoch 0 \tbatch 80 \tloss 1.24117514567 \ttrain acc 74.48 \tval acc 72.59 \n",
      "  Epoch 0 \tbatch 81 \tloss 1.14826696843 \ttrain acc 73.44 \tval acc 72.72 \n",
      "  Epoch 0 \tbatch 82 \tloss 1.48027028379 \ttrain acc 68.23 \tval acc 72.59 \n",
      "  Epoch 0 \tbatch 83 \tloss 1.54393417636 \ttrain acc 69.27 \tval acc 72.85 \n",
      "  Epoch 0 \tbatch 84 \tloss 1.34815822863 \ttrain acc 69.79 \tval acc 72.66 \n",
      "  Epoch 0 \tbatch 85 \tloss 1.43840846778 \ttrain acc 70.31 \tval acc 72.46 \n",
      "  Epoch 0 \tbatch 86 \tloss 1.37561825955 \ttrain acc 68.75 \tval acc 72.27 \n",
      "  Epoch 0 \tbatch 87 \tloss 1.24905759551 \ttrain acc 73.44 \tval acc 72.20 \n",
      "  Epoch 0 \tbatch 88 \tloss 1.1595817145 \ttrain acc 74.48 \tval acc 72.01 \n",
      "  Epoch 0 \tbatch 89 \tloss 1.30098164732 \ttrain acc 68.23 \tval acc 72.27 \n",
      "  Epoch 0 \tbatch 90 \tloss 1.34277558266 \ttrain acc 71.35 \tval acc 72.72 \n",
      "  Epoch 0 \tbatch 91 \tloss 1.31148324358 \ttrain acc 71.88 \tval acc 72.85 \n",
      "  Epoch 0 \tbatch 92 \tloss 1.21366087213 \ttrain acc 72.92 \tval acc 72.98 \n",
      "  Epoch 0 \tbatch 93 \tloss 1.16702830066 \ttrain acc 72.40 \tval acc 72.66 \n",
      "  Epoch 0 \tbatch 94 \tloss 1.38252000611 \ttrain acc 66.67 \tval acc 73.11 \n",
      "  Epoch 0 \tbatch 95 \tloss 1.26513420567 \ttrain acc 76.56 \tval acc 73.05 \n",
      "  Epoch 0 \tbatch 96 \tloss 1.1993629954 \ttrain acc 75.00 \tval acc 72.92 \n",
      "  Epoch 0 \tbatch 97 \tloss 1.34284110857 \ttrain acc 69.79 \tval acc 72.59 \n",
      "  Epoch 0 \tbatch 98 \tloss 1.20952760731 \ttrain acc 73.44 \tval acc 72.14 \n",
      "  Epoch 0 \tbatch 99 \tloss 1.20363613627 \ttrain acc 71.35 \tval acc 73.11 \n",
      "  Epoch 0 \tbatch 100 \tloss 1.0095646124 \ttrain acc 76.04 \tval acc 72.72 \n",
      "  Epoch 0 \tbatch 101 \tloss 1.31652562666 \ttrain acc 72.40 \tval acc 72.98 \n",
      "  Epoch 0 \tbatch 102 \tloss 1.02400252135 \ttrain acc 78.12 \tval acc 72.79 \n",
      "  Epoch 0 \tbatch 103 \tloss 1.06774939416 \ttrain acc 76.04 \tval acc 73.05 \n",
      "  Epoch 0 \tbatch 104 \tloss 1.00338564023 \ttrain acc 77.08 \tval acc 72.98 \n",
      "  Epoch 0 \tbatch 105 \tloss 1.23907926087 \ttrain acc 73.96 \tval acc 73.05 \n",
      "  Epoch 0 \tbatch 106 \tloss 1.19423129305 \ttrain acc 76.04 \tval acc 73.05 \n",
      "  Epoch 0 \tbatch 107 \tloss 1.33170514896 \ttrain acc 73.44 \tval acc 73.11 \n",
      "  Epoch 0 \tbatch 108 \tloss 1.13604715786 \ttrain acc 72.92 \tval acc 73.05 \n",
      "  Epoch 0 \tbatch 109 \tloss 1.02134040192 \ttrain acc 75.00 \tval acc 72.79 \n",
      "  Epoch 0 \tbatch 110 \tloss 1.17965352369 \ttrain acc 75.00 \tval acc 72.92 \n",
      "  Epoch 0 \tbatch 111 \tloss 1.25644837864 \ttrain acc 72.40 \tval acc 72.85 \n",
      "  Epoch 0 \tbatch 112 \tloss 1.14167957813 \ttrain acc 73.96 \tval acc 73.18 \n",
      "  Epoch 0 \tbatch 113 \tloss 1.02238615843 \ttrain acc 76.56 \tval acc 72.98 \n",
      "  Epoch 0 \tbatch 114 \tloss 1.35889396256 \ttrain acc 68.23 \tval acc 72.79 \n",
      "Epoch 1 of 2 took 61.209s\n",
      "  training loss:\t\t1.568798\n",
      "  training accuracy:\t\t68.60 %\n",
      "  validation loss:\t\t1.241714\n",
      "  validation accuracy:\t\t72.92 %\n",
      "  Epoch 1 \tbatch 1 \tloss 1.14969151991 \ttrain acc 75.00 \tval acc 72.85 \n",
      "  Epoch 1 \tbatch 2 \tloss 1.26745711509 \ttrain acc 70.83 \tval acc 72.59 \n",
      "  Epoch 1 \tbatch 3 \tloss 1.3015358052 \ttrain acc 72.40 \tval acc 72.92 \n",
      "  Epoch 1 \tbatch 4 \tloss 1.1852433019 \ttrain acc 72.40 \tval acc 72.79 \n",
      "  Epoch 1 \tbatch 5 \tloss 1.38244117386 \ttrain acc 69.79 \tval acc 72.98 \n",
      "  Epoch 1 \tbatch 6 \tloss 1.15230389805 \ttrain acc 73.44 \tval acc 72.72 \n",
      "  Epoch 1 \tbatch 7 \tloss 1.19097226954 \ttrain acc 74.48 \tval acc 72.85 \n",
      "  Epoch 1 \tbatch 8 \tloss 1.06563862226 \ttrain acc 75.00 \tval acc 72.53 \n",
      "  Epoch 1 \tbatch 9 \tloss 1.25856843804 \ttrain acc 71.88 \tval acc 72.85 \n",
      "  Epoch 1 \tbatch 10 \tloss 1.30224342467 \ttrain acc 74.48 \tval acc 72.72 \n",
      "  Epoch 1 \tbatch 11 \tloss 1.15958885437 \ttrain acc 75.00 \tval acc 72.85 \n",
      "  Epoch 1 \tbatch 12 \tloss 1.20432091189 \ttrain acc 71.88 \tval acc 72.92 \n",
      "  Epoch 1 \tbatch 13 \tloss 1.2889566578 \ttrain acc 70.31 \tval acc 72.66 \n",
      "  Epoch 1 \tbatch 14 \tloss 1.25933626949 \ttrain acc 67.71 \tval acc 72.98 \n",
      "  Epoch 1 \tbatch 15 \tloss 1.32137051155 \ttrain acc 68.75 \tval acc 72.92 \n",
      "  Epoch 1 \tbatch 16 \tloss 1.38532260044 \ttrain acc 72.40 \tval acc 72.98 \n",
      "  Epoch 1 \tbatch 17 \tloss 1.23874984138 \ttrain acc 70.83 \tval acc 73.24 \n",
      "  Epoch 1 \tbatch 18 \tloss 1.11932030553 \ttrain acc 76.56 \tval acc 72.40 \n",
      "  Epoch 1 \tbatch 19 \tloss 1.24587220438 \ttrain acc 73.44 \tval acc 72.66 \n",
      "  Epoch 1 \tbatch 20 \tloss 1.16452443958 \ttrain acc 71.35 \tval acc 72.33 \n",
      "  Epoch 1 \tbatch 21 \tloss 1.46089189548 \ttrain acc 69.27 \tval acc 72.79 \n",
      "  Epoch 1 \tbatch 22 \tloss 1.4186113412 \ttrain acc 70.31 \tval acc 72.79 \n",
      "  Epoch 1 \tbatch 23 \tloss 1.06393801578 \ttrain acc 77.08 \tval acc 72.66 \n",
      "  Epoch 1 \tbatch 24 \tloss 1.17595850675 \ttrain acc 72.92 \tval acc 72.98 \n",
      "  Epoch 1 \tbatch 25 \tloss 1.13058114488 \ttrain acc 75.52 \tval acc 72.92 \n",
      "  Epoch 1 \tbatch 26 \tloss 1.26341725149 \ttrain acc 75.52 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 27 \tloss 1.2312769248 \ttrain acc 71.35 \tval acc 73.24 \n",
      "  Epoch 1 \tbatch 28 \tloss 1.23324565363 \ttrain acc 69.79 \tval acc 72.92 \n",
      "  Epoch 1 \tbatch 29 \tloss 1.113747547 \ttrain acc 74.48 \tval acc 72.98 \n",
      "  Epoch 1 \tbatch 30 \tloss 1.13295271256 \ttrain acc 77.60 \tval acc 73.37 \n",
      "  Epoch 1 \tbatch 31 \tloss 1.20139121398 \ttrain acc 72.40 \tval acc 73.37 \n",
      "  Epoch 1 \tbatch 32 \tloss 1.14144733402 \ttrain acc 75.00 \tval acc 73.37 \n",
      "  Epoch 1 \tbatch 33 \tloss 1.39062143495 \ttrain acc 68.75 \tval acc 72.92 \n",
      "  Epoch 1 \tbatch 34 \tloss 1.15151795933 \ttrain acc 71.88 \tval acc 72.66 \n",
      "  Epoch 1 \tbatch 35 \tloss 1.2772090846 \ttrain acc 71.88 \tval acc 72.72 \n",
      "  Epoch 1 \tbatch 36 \tloss 1.24582276009 \ttrain acc 72.40 \tval acc 72.79 \n",
      "  Epoch 1 \tbatch 37 \tloss 1.06991988222 \ttrain acc 75.00 \tval acc 73.37 \n",
      "  Epoch 1 \tbatch 38 \tloss 1.32895114921 \ttrain acc 70.31 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 39 \tloss 1.1804509743 \ttrain acc 73.44 \tval acc 73.24 \n",
      "  Epoch 1 \tbatch 40 \tloss 1.34625350805 \ttrain acc 68.23 \tval acc 73.44 \n",
      "  Epoch 1 \tbatch 41 \tloss 1.09277797392 \ttrain acc 76.56 \tval acc 73.11 \n",
      "  Epoch 1 \tbatch 42 \tloss 1.13916210545 \ttrain acc 73.96 \tval acc 72.98 \n",
      "  Epoch 1 \tbatch 43 \tloss 1.28557430424 \ttrain acc 70.31 \tval acc 73.05 \n",
      "  Epoch 1 \tbatch 44 \tloss 0.999451637942 \ttrain acc 73.44 \tval acc 73.05 \n",
      "  Epoch 1 \tbatch 45 \tloss 1.10424613307 \ttrain acc 72.40 \tval acc 73.50 \n",
      "  Epoch 1 \tbatch 46 \tloss 1.62716778089 \ttrain acc 69.79 \tval acc 73.37 \n",
      "  Epoch 1 \tbatch 47 \tloss 1.14052214645 \ttrain acc 72.92 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 48 \tloss 1.14537225003 \ttrain acc 72.92 \tval acc 73.11 \n",
      "  Epoch 1 \tbatch 49 \tloss 1.43294673218 \ttrain acc 69.79 \tval acc 72.85 \n",
      "  Epoch 1 \tbatch 50 \tloss 1.24715828752 \ttrain acc 72.92 \tval acc 73.44 \n",
      "  Epoch 1 \tbatch 51 \tloss 1.24460082967 \ttrain acc 73.44 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 52 \tloss 1.25479609757 \ttrain acc 72.92 \tval acc 73.44 \n",
      "  Epoch 1 \tbatch 53 \tloss 1.07413186209 \ttrain acc 76.56 \tval acc 73.57 \n",
      "  Epoch 1 \tbatch 54 \tloss 1.3444026068 \ttrain acc 73.44 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 55 \tloss 1.03195983076 \ttrain acc 76.56 \tval acc 72.53 \n",
      "  Epoch 1 \tbatch 56 \tloss 1.22297583573 \ttrain acc 72.92 \tval acc 72.79 \n",
      "  Epoch 1 \tbatch 57 \tloss 1.21132846823 \ttrain acc 71.88 \tval acc 73.18 \n",
      "  Epoch 1 \tbatch 58 \tloss 1.19862067189 \ttrain acc 72.40 \tval acc 73.11 \n",
      "  Epoch 1 \tbatch 59 \tloss 1.21416125648 \ttrain acc 71.35 \tval acc 72.85 \n",
      "  Epoch 1 \tbatch 60 \tloss 1.25417843533 \ttrain acc 71.35 \tval acc 72.92 \n",
      "  Epoch 1 \tbatch 61 \tloss 1.21920222523 \ttrain acc 72.40 \tval acc 73.05 \n",
      "  Epoch 1 \tbatch 62 \tloss 1.27313405097 \ttrain acc 70.31 \tval acc 73.63 \n",
      "  Epoch 1 \tbatch 63 \tloss 1.15386710028 \ttrain acc 73.44 \tval acc 73.50 \n",
      "  Epoch 1 \tbatch 64 \tloss 1.15152336097 \ttrain acc 73.44 \tval acc 73.89 \n",
      "  Epoch 1 \tbatch 65 \tloss 1.29979042953 \ttrain acc 70.31 \tval acc 73.70 \n",
      "  Epoch 1 \tbatch 66 \tloss 1.09770816336 \ttrain acc 75.00 \tval acc 72.92 \n",
      "  Epoch 1 \tbatch 67 \tloss 1.30351263478 \ttrain acc 69.79 \tval acc 73.18 \n",
      "  Epoch 1 \tbatch 68 \tloss 1.02218190386 \ttrain acc 75.52 \tval acc 73.24 \n",
      "  Epoch 1 \tbatch 69 \tloss 1.1571111302 \ttrain acc 75.52 \tval acc 73.50 \n",
      "  Epoch 1 \tbatch 70 \tloss 1.00787130952 \ttrain acc 76.04 \tval acc 73.70 \n",
      "  Epoch 1 \tbatch 71 \tloss 1.27190004187 \ttrain acc 71.88 \tval acc 73.83 \n",
      "  Epoch 1 \tbatch 72 \tloss 1.25431852671 \ttrain acc 73.44 \tval acc 74.22 \n",
      "  Epoch 1 \tbatch 73 \tloss 1.19076530771 \ttrain acc 74.48 \tval acc 74.09 \n",
      "  Epoch 1 \tbatch 74 \tloss 1.26237291571 \ttrain acc 71.88 \tval acc 72.46 \n",
      "  Epoch 1 \tbatch 75 \tloss 1.2805780663 \ttrain acc 72.40 \tval acc 72.98 \n",
      "  Epoch 1 \tbatch 76 \tloss 1.01946230343 \ttrain acc 76.04 \tval acc 73.57 \n",
      "  Epoch 1 \tbatch 77 \tloss 1.03067483643 \ttrain acc 73.44 \tval acc 73.44 \n",
      "  Epoch 1 \tbatch 78 \tloss 1.35753093556 \ttrain acc 68.75 \tval acc 72.98 \n",
      "  Epoch 1 \tbatch 79 \tloss 1.09876773873 \ttrain acc 78.12 \tval acc 73.05 \n",
      "  Epoch 1 \tbatch 80 \tloss 1.10577265473 \ttrain acc 77.60 \tval acc 73.44 \n",
      "  Epoch 1 \tbatch 81 \tloss 1.0328117424 \ttrain acc 73.44 \tval acc 73.18 \n",
      "  Epoch 1 \tbatch 82 \tloss 1.35363913692 \ttrain acc 68.75 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 83 \tloss 1.36501805314 \ttrain acc 68.75 \tval acc 73.57 \n",
      "  Epoch 1 \tbatch 84 \tloss 1.24800715477 \ttrain acc 70.83 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 85 \tloss 1.33359519193 \ttrain acc 71.35 \tval acc 73.57 \n",
      "  Epoch 1 \tbatch 86 \tloss 1.2981363104 \ttrain acc 70.31 \tval acc 73.63 \n",
      "  Epoch 1 \tbatch 87 \tloss 1.17000019616 \ttrain acc 73.96 \tval acc 73.83 \n",
      "  Epoch 1 \tbatch 88 \tloss 1.05040274969 \ttrain acc 76.56 \tval acc 73.76 \n",
      "  Epoch 1 \tbatch 89 \tloss 1.15215751581 \ttrain acc 70.31 \tval acc 73.63 \n",
      "  Epoch 1 \tbatch 90 \tloss 1.25215644616 \ttrain acc 71.88 \tval acc 74.09 \n",
      "  Epoch 1 \tbatch 91 \tloss 1.24052772268 \ttrain acc 73.96 \tval acc 73.70 \n",
      "  Epoch 1 \tbatch 92 \tloss 1.17157848629 \ttrain acc 72.40 \tval acc 74.02 \n",
      "  Epoch 1 \tbatch 93 \tloss 1.03273000766 \ttrain acc 74.48 \tval acc 74.09 \n",
      "  Epoch 1 \tbatch 94 \tloss 1.22216015638 \ttrain acc 69.79 \tval acc 73.24 \n",
      "  Epoch 1 \tbatch 95 \tloss 1.19999340141 \ttrain acc 75.00 \tval acc 73.63 \n",
      "  Epoch 1 \tbatch 96 \tloss 1.13457274861 \ttrain acc 73.44 \tval acc 72.85 \n",
      "  Epoch 1 \tbatch 97 \tloss 1.23554852895 \ttrain acc 71.35 \tval acc 72.66 \n",
      "  Epoch 1 \tbatch 98 \tloss 1.10660239696 \ttrain acc 75.00 \tval acc 72.53 \n",
      "  Epoch 1 \tbatch 99 \tloss 1.14856527903 \ttrain acc 70.83 \tval acc 73.11 \n",
      "  Epoch 1 \tbatch 100 \tloss 0.934437974954 \ttrain acc 77.60 \tval acc 73.50 \n",
      "  Epoch 1 \tbatch 101 \tloss 1.20791570191 \ttrain acc 72.92 \tval acc 73.57 \n",
      "  Epoch 1 \tbatch 102 \tloss 0.923251537973 \ttrain acc 79.69 \tval acc 73.70 \n",
      "  Epoch 1 \tbatch 103 \tloss 1.0279376208 \ttrain acc 75.52 \tval acc 73.57 \n",
      "  Epoch 1 \tbatch 104 \tloss 0.946969895879 \ttrain acc 77.60 \tval acc 73.70 \n",
      "  Epoch 1 \tbatch 105 \tloss 1.10966249156 \ttrain acc 75.00 \tval acc 73.83 \n",
      "  Epoch 1 \tbatch 106 \tloss 1.07103654799 \ttrain acc 78.12 \tval acc 73.44 \n",
      "  Epoch 1 \tbatch 107 \tloss 1.22497287932 \ttrain acc 73.96 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 108 \tloss 1.08619135181 \ttrain acc 74.48 \tval acc 74.02 \n",
      "  Epoch 1 \tbatch 109 \tloss 0.93224355737 \ttrain acc 75.52 \tval acc 73.50 \n",
      "  Epoch 1 \tbatch 110 \tloss 1.07677963747 \ttrain acc 73.96 \tval acc 73.57 \n",
      "  Epoch 1 \tbatch 111 \tloss 1.12487666103 \ttrain acc 73.96 \tval acc 73.96 \n",
      "  Epoch 1 \tbatch 112 \tloss 1.06962009235 \ttrain acc 78.12 \tval acc 74.67 \n",
      "  Epoch 1 \tbatch 113 \tloss 0.958437736605 \ttrain acc 76.56 \tval acc 74.35 \n",
      "  Epoch 1 \tbatch 114 \tloss 1.23047856206 \ttrain acc 72.92 \tval acc 74.09 \n",
      "Epoch 2 of 2 took 54.262s\n",
      "  training loss:\t\t1.190336\n",
      "  training accuracy:\t\t73.13 %\n",
      "  validation loss:\t\t1.151322\n",
      "  validation accuracy:\t\t73.96 %\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Training!!!\n",
    "train_losses, train_accuracies, val_accuracies = model.train(train_data, val_data, train_loss_acc, compute_loss_acc, num_epochs=num_epochs, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Predicted AST IDs\n",
      "[[   1.    7.    3.    0.   -1.   -1.]\n",
      " [ 102.    0.   -1.   -1.   -1.   -1.]\n",
      " [  11.   31.   -1.   -1.   -1.   -1.]\n",
      " [   5.    1.    0.   -1.   -1.   -1.]\n",
      " [  84.    1.    2.    0.   -1.   -1.]\n",
      " [  86.   25.   -1.   -1.   -1.   -1.]\n",
      " [   1.   24.    7.   -1.   -1.   -1.]\n",
      " [   5.    0.   -1.   -1.   -1.   -1.]\n",
      " [  10.    6.   -1.   -1.   -1.   -1.]\n",
      " [   1.    4.   25.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[   1.    7.    3.    0.   -1.   -1.]\n",
      " [ 102.    0.   -1.   -1.   -1.   -1.]\n",
      " [  11.   31.   -1.   -1.   -1.   -1.]\n",
      " [   5.    1.    0.   -1.   -1.   -1.]\n",
      " [  84.    1.    2.    0.   -1.   -1.]\n",
      " [  86.   25.   -1.   -1.   -1.   -1.]\n",
      " [   1.   24.    7.   -1.   -1.   -1.]\n",
      " [   5.    0.   -1.   -1.   -1.   -1.]\n",
      " [  10.    6.   -1.   -1.   -1.   -1.]\n",
      " [   1.    4.   25.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[ 119.   -1.   -1.   -1.   -1.   -1.]\n",
      " [  61.   76.   25.   -1.   -1.   -1.]\n",
      " [   6.    5.   -1.   -1.   -1.   -1.]\n",
      " [  13.  113.   27.   -1.   -1.   -1.]\n",
      " [ 291.    0.   -1.   -1.   -1.   -1.]\n",
      " [   8.    3.   22.   32.   20.    9.]\n",
      " [   1.   17.    6.    5.    0.   -1.]\n",
      " [ 168.    2.    0.   -1.   -1.   -1.]\n",
      " [   6.   17.    0.   -1.   -1.   -1.]\n",
      " [ 119.  123.   -1.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[ 589.   -1.   -1.   -1.   -1.   -1.]\n",
      " [  61.  204.   25.   -1.   -1.   -1.]\n",
      " [   6.    5.   -1.   -1.   -1.   -1.]\n",
      " [  13.  121.   27.   -1.   -1.   -1.]\n",
      " [ 248.    0.   -1.   -1.   -1.   -1.]\n",
      " [   8.    3.   22.   32.   20.    9.]\n",
      " [   1.   17.    6.    5.    0.   -1.]\n",
      " [ 168.    2.    0.   -1.   -1.   -1.]\n",
      " [   6.   17.    0.   -1.   -1.   -1.]\n",
      " [ 414.  358.   -1.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[  65.   38.    9.   -1.   -1.   -1.]\n",
      " [  16.  113.    0.   -1.   -1.   -1.]\n",
      " [   2.   41.    0.   -1.   -1.   -1.]\n",
      " [   1.    3.   28.   14.   -1.   -1.]\n",
      " [   3.   16.    0.   -1.   -1.   -1.]\n",
      " [  21.    1.    8.    3.    0.   -1.]\n",
      " [   1.    6.    4.   -1.   -1.   -1.]\n",
      " [  32.    0.   -1.   -1.   -1.   -1.]\n",
      " [  10.    1.    3.    4.    0.   -1.]\n",
      " [   1.    4.   18.    3.    0.   -1.]]\n",
      "Truth AST IDs\n",
      "[[  65.   38.    9.   -1.   -1.   -1.]\n",
      " [  16.  113.    0.   -1.   -1.   -1.]\n",
      " [   2.   41.    0.   -1.   -1.   -1.]\n",
      " [   1.    3.   28.   14.   -1.   -1.]\n",
      " [   3.   16.    0.   -1.   -1.   -1.]\n",
      " [  21.    1.    8.    3.    0.   -1.]\n",
      " [   1.    6.    4.   -1.   -1.   -1.]\n",
      " [  32.    0.   -1.   -1.   -1.   -1.]\n",
      " [  10.    1.    3.    4.    0.   -1.]\n",
      " [   1.    4.   18.    3.    0.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[   1.   10.    0.   -1.   -1.   -1.]\n",
      " [  84.  211.   89.    0.   -1.   -1.]\n",
      " [   4.   15.    2.    0.   -1.   -1.]\n",
      " [  38.   65.   52.   -1.   -1.   -1.]\n",
      " [ 135.  112.   -1.   -1.   -1.   -1.]\n",
      " [  18.    6.    3.    0.   -1.   -1.]\n",
      " [  61.  100.   -1.   -1.   -1.   -1.]\n",
      " [  19.   17.    0.   -1.   -1.   -1.]\n",
      " [  64.    2.    0.   -1.   -1.   -1.]\n",
      " [   8.    3.    0.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[   1.   10.    0.   -1.   -1.   -1.]\n",
      " [  84.  243.   89.    0.   -1.   -1.]\n",
      " [   4.   15.    2.    0.   -1.   -1.]\n",
      " [  38.  111.   52.   -1.   -1.   -1.]\n",
      " [ 135.  184.   -1.   -1.   -1.   -1.]\n",
      " [  18.    6.    3.    0.   -1.   -1.]\n",
      " [  61.  100.   -1.   -1.   -1.   -1.]\n",
      " [  19.   17.    0.   -1.   -1.   -1.]\n",
      " [  64.    2.    0.   -1.   -1.   -1.]\n",
      " [   8.    3.    0.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[  1.   3.  33.  29.  -1.  -1.]\n",
      " [ 24.   7.   0.  -1.  -1.  -1.]\n",
      " [ 18.   3.   0.  -1.  -1.  -1.]\n",
      " [  4.  45.   0.  -1.  -1.  -1.]\n",
      " [  4.  43.  26.  -1.  -1.  -1.]\n",
      " [  1.  12.  75.  -1.  -1.  -1.]\n",
      " [  1.   4.   3.  32.   9.  -1.]\n",
      " [  4.   1.   3.   0.  -1.  -1.]\n",
      " [ 13.  23.   2.   0.  -1.  -1.]\n",
      " [  4.  52.  -1.  -1.  -1.  -1.]]\n",
      "Truth AST IDs\n",
      "[[   1.    3.   93.   29.   -1.   -1.]\n",
      " [  24.    7.    0.   -1.   -1.   -1.]\n",
      " [  18.    3.    0.   -1.   -1.   -1.]\n",
      " [   4.   45.    0.   -1.   -1.   -1.]\n",
      " [   4.   43.   26.   -1.   -1.   -1.]\n",
      " [   1.   12.  178.   -1.   -1.   -1.]\n",
      " [   1.    4.    3.   32.    9.   -1.]\n",
      " [   4.    1.    3.    0.   -1.   -1.]\n",
      " [  13.   23.    2.    0.   -1.   -1.]\n",
      " [   4.   52.   -1.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[  75.    0.   -1.   -1.   -1.   -1.]\n",
      " [   8.    2.    8.    0.   -1.   -1.]\n",
      " [   6.   73.   19.    0.   -1.   -1.]\n",
      " [   1.    4.    1.    0.   -1.   -1.]\n",
      " [  44.    3.    0.   -1.   -1.   -1.]\n",
      " [ 211.   14.   -1.   -1.   -1.   -1.]\n",
      " [  40.   19.   -1.   -1.   -1.   -1.]\n",
      " [   2.  108.    0.   -1.   -1.   -1.]\n",
      " [ 162.    0.   -1.   -1.   -1.   -1.]\n",
      " [  16.   28.   14.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[ 220.    0.   -1.   -1.   -1.   -1.]\n",
      " [   8.    2.    8.    0.   -1.   -1.]\n",
      " [   6.   73.   19.    0.   -1.   -1.]\n",
      " [   1.    4.    1.    0.   -1.   -1.]\n",
      " [  44.    3.    0.   -1.   -1.   -1.]\n",
      " [ 113.   14.   -1.   -1.   -1.   -1.]\n",
      " [  40.   19.   -1.   -1.   -1.   -1.]\n",
      " [   2.  262.    0.   -1.   -1.   -1.]\n",
      " [ 162.    0.   -1.   -1.   -1.   -1.]\n",
      " [  16.   28.   14.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[  39.   12.   26.   -1.   -1.   -1.]\n",
      " [   4.   10.    1.   12.    0.   -1.]\n",
      " [  85.   20.    9.   -1.   -1.   -1.]\n",
      " [  17.   34.    0.   -1.   -1.   -1.]\n",
      " [  63.    3.   -1.   -1.   -1.   -1.]\n",
      " [  13.    2.    1.    0.   -1.   -1.]\n",
      " [  13.   39.   12.   26.   -1.   -1.]\n",
      " [ 133.    0.   -1.   -1.   -1.   -1.]\n",
      " [  66.   -1.   -1.   -1.   -1.   -1.]\n",
      " [ 139.    8.    3.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[  39.   12.   26.   -1.   -1.   -1.]\n",
      " [   4.   10.    1.   12.    0.   -1.]\n",
      " [  85.   20.    9.   -1.   -1.   -1.]\n",
      " [  17.   34.    0.   -1.   -1.   -1.]\n",
      " [  63.    3.   -1.   -1.   -1.   -1.]\n",
      " [  13.    2.    1.    0.   -1.   -1.]\n",
      " [  13.   39.   12.   26.   -1.   -1.]\n",
      " [ 133.    0.   -1.   -1.   -1.   -1.]\n",
      " [  66.   -1.   -1.   -1.   -1.   -1.]\n",
      " [ 175.    8.    3.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[  34.   14.   -1.   -1.   -1.   -1.]\n",
      " [   3.   12.  178.   -1.   -1.   -1.]\n",
      " [   8.    3.   24.    7.   42.   -1.]\n",
      " [  81.    0.   -1.   -1.   -1.   -1.]\n",
      " [ 117.    9.   -1.   -1.   -1.   -1.]\n",
      " [   4.    2.    0.   -1.   -1.   -1.]\n",
      " [   4.   10.    1.    4.    1.    0.]\n",
      " [  57.    0.   -1.   -1.   -1.   -1.]\n",
      " [  44.   73.    0.   -1.   -1.   -1.]\n",
      " [   1.   52.   -1.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[  34.   14.   -1.   -1.   -1.   -1.]\n",
      " [   3.   12.  178.   -1.   -1.   -1.]\n",
      " [   8.    3.   24.    7.   42.   -1.]\n",
      " [  81.    0.   -1.   -1.   -1.   -1.]\n",
      " [ 117.    9.   -1.   -1.   -1.   -1.]\n",
      " [   4.    2.    0.   -1.   -1.   -1.]\n",
      " [   4.   10.    1.    4.    1.    0.]\n",
      " [  57.    0.   -1.   -1.   -1.   -1.]\n",
      " [  44.   73.    0.   -1.   -1.   -1.]\n",
      " [   1.   52.   -1.   -1.   -1.   -1.]]\n",
      "Final results:\n",
      "  test loss:\t\t\t0.229070\n",
      "  test accuracy:\t\t96.48 %\n"
     ]
    }
   ],
   "source": [
    "model.check_accuracy(test_data, compute_loss_acc, row_to_ast_id_map, dataset_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Final results:\n",
      "  test loss:\t\t\t1.138466\n",
      "  test raw accuracy:\t\t73.76 %\n",
      "  test corrected accuracy:\t49.45 %\n",
      "X AST IDs\n",
      "[[  10.   15.    0.   -1.   -1.   -1.]\n",
      " [ 128.    2.    0.   -1.   -1.   -1.]\n",
      " [  21.    6.    5.    0.   -1.   -1.]\n",
      " [ 114.    3.    0.   -1.   -1.   -1.]\n",
      " [  34.    7.    0.   -1.   -1.   -1.]\n",
      " [   1.   11.    4.    1.    0.   -1.]\n",
      " [   1.   10.   -1.   -1.   -1.   -1.]\n",
      " [  31.    3.   -1.   -1.   -1.   -1.]\n",
      " [   8.    3.   24.    7.    0.   -1.]\n",
      " [   3.   65.    9.   -1.   -1.   -1.]]\n",
      "Predicted AST IDs\n",
      "[[ 3.  3. -1. -1. -1. -1.]\n",
      " [ 0.  0. -1. -1. -1. -1.]\n",
      " [ 1.  0.  0. -1. -1. -1.]\n",
      " [-1.  0. -1. -1. -1. -1.]\n",
      " [ 0.  0. -1. -1. -1. -1.]\n",
      " [ 4.  3.  0.  0. -1. -1.]\n",
      " [ 4.  3. -1. -1. -1. -1.]\n",
      " [ 3.  0. -1. -1. -1. -1.]\n",
      " [ 3.  0.  0.  0. -1. -1.]\n",
      " [ 1.  9. -1. -1. -1. -1.]]\n",
      "Truth AST IDs\n",
      "[[ 15.   0.  -1.  -1.  -1.  -1.]\n",
      " [  2.   0.  -1.  -1.  -1.  -1.]\n",
      " [  6.   5.   0.  -1.  -1.  -1.]\n",
      " [  3.   0.  -1.  -1.  -1.  -1.]\n",
      " [  7.   0.  -1.  -1.  -1.  -1.]\n",
      " [ 11.   4.   1.   0.  -1.  -1.]\n",
      " [ 10.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  3.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  3.  24.   7.   0.  -1.  -1.]\n",
      " [ 65.   9.  -1.  -1.  -1.  -1.]]\n"
     ]
    }
   ],
   "source": [
    "X_ast_ids, predicted_ast_ids, truth_ast_ids, loss, raw_acc, corrected_acc = model.check_accuracy(test_data, compute_loss_acc, row_to_ast_id_map, dataset_name='test')\n",
    "print(\"X AST IDs\")\n",
    "print X_ast_ids[:10,:]\n",
    "print(\"Predicted AST IDs\")\n",
    "print predicted_ast_ids[:10,:]\n",
    "print (\"Truth AST IDs\")\n",
    "print truth_ast_ids[:10, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAawAAAE+CAYAAADRZiTkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJCkoYYmISFL4f1sI13DoJJNYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c27ced0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss and the accuracies for both training and validation sets for each epoch\n",
    "visualize.plot_loss_acc(DATA_SET + '_train', train_losses, train_accuracies, val_accuracies, learning_rate, reg_strength, num_epochs, num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432\n",
      "0\n",
      "-1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
