{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Knowledge Tracing\n",
    "Authors: Lisa Wang, Angela Sy\n",
    "\n",
    "### Task: Predict what the student is going to code next.\n",
    "\n",
    "Input: For each of the N students, we have a time series of Abstract Syntax Trees (ASTs), which represent the student's code at that time step.\n",
    "- input shape (num_students, num_timesteps, num_asts)\n",
    "    - num_timesteps is the max sequence length of asts that we are taking into account.\n",
    "    - num_asts is the total number of asts for that problem.\n",
    "\n",
    "Output: At each timestep, we are predicting the next AST.\n",
    "- Output shape (num_students, num_timesteps, num_asts). (one-hot encoding)\n",
    "\n",
    "The truth matrix contains the desired output for a given input, and is used to compute the loss as well as train/val/test accuracies.\n",
    "- Truth shape (num_students, num_timesteps) Values are in range (0, num_asts)\n",
    "\n",
    "Accuracy:\n",
    "- Raw Accuracy: For all predictions at all timesteps, we get the percentage of predictions we got correct.\n",
    "- Corrected Accuracy: Since many trajectories contain fewer asts than max_traj_len, we fill the empty asts with our dummy ast token at row 0. However, predicting on end tokens is to simple of a task and might bias our results. The corrected accuracy ignores all predictions on the end token.\n",
    "\n",
    "### Current Issues:\n",
    "    1. AST IDs are not consistent across different HOCs. Hence, we can only train and run this model on each HOC individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as Tensor\n",
    "import lasagne\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# allows plots to show inline in ipython notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import our own modules\n",
    "import utils\n",
    "import model_predict_ast as model\n",
    "import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each trajectory matrix corresponds to one hoc exercise and is\n",
    "# its own data set. Mixing data sets currently does not make much\n",
    "# sense since the AST IDs don't persist betweeen different hoc's.\n",
    "TRAJ_MAP = {\n",
    "    'hoc1': '../processed_data/traj_matrix_1.npy',\n",
    "    'hoc2': '../processed_data/traj_matrix_2.npy',\n",
    "    'hoc3': '../processed_data/traj_matrix_3.npy',\n",
    "    'hoc4': '../processed_data/traj_matrix_4.npy',\n",
    "    'hoc5': '../processed_data/traj_matrix_5.npy',\n",
    "    'hoc6': '../processed_data/traj_matrix_6.npy',\n",
    "    'hoc7': '../processed_data/traj_matrix_7.npy',\n",
    "    'hoc8': '../processed_data/traj_matrix_8.npy',\n",
    "    'hoc9': '../processed_data/traj_matrix_9.npy' \n",
    "}\n",
    "\n",
    "TRAJ_MAP_PREFIX = '../processed_data/traj_matrix_'\n",
    "TRAJ_MAP_SUFFIX = '.npy'\n",
    "\n",
    "HOC_NUM = str(7)\n",
    "DATA_SET = 'hoc' + HOC_NUM\n",
    "# if DATA_SZ = -1, use entire data set\n",
    "# For DATA_SZ, powers of 2 work best for performance.\n",
    "DATA_SZ = -1\n",
    "AST_MAP_FILE = '../processed_data/map_ast_row_' + HOC_NUM + '.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load AST ID to Row Map\n",
    "ast_id_to_row_map = pickle.load(open( AST_MAP_FILE, \"rb\" ))\n",
    "# Create Row to AST ID Map by inverting the previous one\n",
    "row_to_ast_id_map = {v: k for k, v in ast_id_to_row_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4195, 7, 432)\n"
     ]
    }
   ],
   "source": [
    "# trajectories matrix for a single hoc exercise\n",
    "# shape (num_traj, max_traj_len, num_asts)\n",
    "# Note that ast_index = 0 corresponds to the <END> token,\n",
    "# marking that the student has already finished.\n",
    "# The <END> token does not correspond to an AST.\n",
    "traj_mat = np.load(TRAJ_MAP[DATA_SET])\n",
    "print traj_mat.shape\n",
    "# print traj_mat[:10, :, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4195, 7, 432)\n"
     ]
    }
   ],
   "source": [
    "# if DATA_SZ specified, reduce matrix. \n",
    "# Useful to create smaller data sets for testing purposes.\n",
    "if DATA_SZ != -1:\n",
    "    traj_mat = traj_mat[:DATA_SZ]\n",
    "print traj_mat.shape\n",
    "# print traj_mat_sm[:, :, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shuffle the first dimension of the matrix\n",
    "np.random.shuffle(traj_mat)\n",
    "# print traj_mat_sm[:, :, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_traj, max_traj_len, num_asts = traj_mat.shape\n",
    "# Split data into train, val, test\n",
    "# TODO: Replace with kfold validation in the future\n",
    "# perhaps we can use sklearn kfold?\n",
    "\n",
    "train_mat = traj_mat[0:7*num_traj/8,:]\n",
    "val_mat =  traj_mat[7*num_traj/8: 15*num_traj/16 ,:]\n",
    "test_mat = traj_mat[15*num_traj/16:num_traj,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing network inputs and targets...\n",
      "(3670, 6, 432)\n",
      "(3670, 6)\n",
      "(262, 6, 432)\n",
      "(263, 6, 432)\n",
      "[[   2.   43.    0.    0.    0.    0.]\n",
      " [  51.    3.    0.    0.    0.    0.]\n",
      " [  36.    2.   27.   28.    0.    0.]\n",
      " [  59.    1.    3.    0.    0.    0.]\n",
      " [   2.  322.    0.    0.    0.    0.]\n",
      " [   2.   27.    3.    0.    0.    0.]\n",
      " [  37.  108.    3.    0.    0.    0.]\n",
      " [   1.    3.    0.    0.    0.    0.]\n",
      " [ 219.   43.    0.    0.    0.    0.]\n",
      " [  30.   34.    3.    0.    0.    0.]]\n",
      "[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "[[  8.   3.  25.  -1.  -1.  -1.]\n",
      " [  4.  12.   0.  -1.  -1.  -1.]\n",
      " [  4.  11.   3.   7.   9.  -1.]\n",
      " [  1.   8.   1.   0.  -1.  -1.]\n",
      " [  1.   3.  77.  -1.  -1.  -1.]\n",
      " [  8.   3.   7.   0.  -1.  -1.]\n",
      " [  1.   2.  59.   0.  -1.  -1.]\n",
      " [ 12.   1.   0.  -1.  -1.  -1.]\n",
      " [ 61.  86.  25.  -1.  -1.  -1.]\n",
      " [  1.   5.  19.   0.  -1.  -1.]]\n",
      "[[  3.  25.  -1.  -1.  -1.  -1.]\n",
      " [ 12.   0.  -1.  -1.  -1.  -1.]\n",
      " [ 11.   3.   7.   9.  -1.  -1.]\n",
      " [  8.   1.   0.  -1.  -1.  -1.]\n",
      " [  3.  77.  -1.  -1.  -1.  -1.]\n",
      " [  3.   7.   0.  -1.  -1.  -1.]\n",
      " [  2.  59.   0.  -1.  -1.  -1.]\n",
      " [  1.   0.  -1.  -1.  -1.  -1.]\n",
      " [ 86.  25.  -1.  -1.  -1.  -1.]\n",
      " [  5.  19.   0.  -1.  -1.  -1.]]\n",
      "6\n",
      "Inputs and targets done!\n"
     ]
    }
   ],
   "source": [
    "print('Preparing network inputs and targets...')\n",
    "# X_train, y_train = utils.prepare_traj_data_for_rnn(train_data)\n",
    "# X_val, y_val = utils.prepare_traj_data_for_rnn(val_data)\n",
    "# X_test, y_test = utils.prepare_traj_data_for_rnn(test_data)\n",
    "\n",
    "train_data = utils.prepare_traj_data_for_rnn(train_mat)\n",
    "val_data = utils.prepare_traj_data_for_rnn(val_mat)\n",
    "test_data = utils.prepare_traj_data_for_rnn(test_mat)\n",
    "\n",
    "\n",
    "X_train, y_train = train_data\n",
    "X_val, y_val = val_data\n",
    "X_test, y_test = test_data\n",
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_val.shape\n",
    "print X_test.shape\n",
    "num_train, num_timesteps, num_asts = X_train.shape\n",
    "\n",
    "print y_train[:10]\n",
    "print X_train[:10,:, :10]\n",
    "\n",
    "X_train_ast_ids, y_train_ast_ids = utils.convert_data_to_ast_ids(train_data, row_to_ast_id_map)\n",
    "print X_train_ast_ids[:10]\n",
    "print y_train_ast_ids[:10]\n",
    "\n",
    "print num_timesteps\n",
    "print (\"Inputs and targets done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 256 # size of hidden layer of neurons\n",
    "learning_rate = 2e-2\n",
    "lr_decay = 0.995\n",
    "reg_strength = 1e-2\n",
    "grad_clip = 10\n",
    "batchsize = 32\n",
    "num_epochs = 10\n",
    "dropout_p = 0.2\n",
    "num_lstm_layers = 2\n",
    "theano.config.optimizer='fast_compile'\n",
    "theano.config.exception_verbosity='high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Compiling done!\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "train_loss_acc, compute_loss_acc, probs = model.create_model(num_timesteps, num_asts, hidden_size, learning_rate, grad_clip, dropout_p, num_lstm_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Total training iterations: 1140\n",
      "Ep 0 \titer 1  \tloss 5.71710, train acc 64.06, train corr acc 36.11, val acc 66.16, val corr acc 33.00\n",
      "Ep 0 \titer 2  \tloss 2.10251, train acc 68.75, train corr acc 34.78, val acc 66.16, val corr acc 33.00\n",
      "Ep 0 \titer 3  \tloss 1.99258, train acc 65.62, train corr acc 32.65, val acc 66.09, val corr acc 32.87\n",
      "Ep 0 \titer 4  \tloss 2.22885, train acc 65.62, train corr acc 32.65, val acc 66.16, val corr acc 33.00\n",
      "Ep 0 \titer 5  \tloss 2.32056, train acc 64.58, train corr acc 32.00, val acc 66.28, val corr acc 33.25\n",
      "Ep 0 \titer 6  \tloss 2.17487, train acc 59.90, train corr acc 29.36, val acc 66.67, val corr acc 34.01\n",
      "Ep 0 \titer 7  \tloss 2.22112, train acc 67.19, train corr acc 36.36, val acc 61.32, val corr acc 25.57\n",
      "Ep 0 \titer 8  \tloss 1.81876, train acc 64.06, train corr acc 29.90, val acc 65.84, val corr acc 32.37\n",
      "Ep 0 \titer 9  \tloss 1.53966, train acc 63.02, train corr acc 30.39, val acc 66.98, val corr acc 34.63\n",
      "Ep 0 \titer 10  \tloss 1.84297, train acc 64.06, train corr acc 36.11, val acc 66.98, val corr acc 34.63\n",
      "Ep 0 \titer 11  \tloss 1.68237, train acc 64.58, train corr acc 33.98, val acc 67.05, val corr acc 35.01\n",
      "Ep 0 \titer 12  \tloss 1.51641, train acc 68.75, train corr acc 37.23, val acc 65.71, val corr acc 34.13\n",
      "Ep 0 \titer 13  \tloss 1.49005, train acc 66.67, train corr acc 30.00, val acc 66.09, val corr acc 34.63\n",
      "Ep 0 \titer 14  \tloss 1.54207, train acc 68.23, train corr acc 38.54, val acc 68.07, val corr acc 36.90\n",
      "Ep 0 \titer 15  \tloss 1.95962, train acc 65.10, train corr acc 39.09, val acc 67.88, val corr acc 36.52\n",
      "Ep 0 \titer 16  \tloss 1.40798, train acc 69.27, train corr acc 37.23, val acc 67.94, val corr acc 36.65\n",
      "Ep 0 \titer 17  \tloss 1.46333, train acc 64.06, train corr acc 33.01, val acc 67.49, val corr acc 35.77\n",
      "Ep 0 \titer 18  \tloss 1.60890, train acc 64.06, train corr acc 33.65, val acc 67.05, val corr acc 35.14\n",
      "Ep 0 \titer 19  \tloss 1.51129, train acc 65.62, train corr acc 32.29, val acc 66.98, val corr acc 34.63\n",
      "Ep 0 \titer 20  \tloss 1.68883, train acc 67.19, train corr acc 34.38, val acc 67.18, val corr acc 35.01\n",
      "Ep 0 \titer 21  \tloss 1.46056, train acc 72.40, train corr acc 38.37, val acc 67.37, val corr acc 35.39\n",
      "Ep 0 \titer 22  \tloss 1.85491, train acc 64.58, train corr acc 34.62, val acc 66.67, val corr acc 34.01\n",
      "Ep 0 \titer 23  \tloss 1.33307, train acc 69.79, train corr acc 31.76, val acc 65.84, val corr acc 33.00\n",
      "Ep 0 \titer 24  \tloss 1.44986, train acc 66.67, train corr acc 36.63, val acc 66.48, val corr acc 34.13\n",
      "Ep 0 \titer 25  \tloss 1.63830, train acc 66.15, train corr acc 34.02, val acc 66.35, val corr acc 33.38\n",
      "Ep 0 \titer 26  \tloss 1.42909, train acc 67.19, train corr acc 35.71, val acc 66.41, val corr acc 33.50\n",
      "Ep 0 \titer 27  \tloss 1.72074, train acc 63.02, train corr acc 31.07, val acc 66.60, val corr acc 33.88\n",
      "Ep 0 \titer 28  \tloss 1.47576, train acc 68.23, train corr acc 38.38, val acc 66.92, val corr acc 34.63\n",
      "Ep 0 \titer 29  \tloss 1.55528, train acc 66.67, train corr acc 35.35, val acc 67.18, val corr acc 35.01\n",
      "Ep 0 \titer 30  \tloss 1.68219, train acc 66.67, train corr acc 37.25, val acc 67.37, val corr acc 35.77\n",
      "Ep 0 \titer 31  \tloss 1.49045, train acc 63.02, train corr acc 26.04, val acc 67.49, val corr acc 35.89\n",
      "Ep 0 \titer 32  \tloss 1.51654, train acc 70.31, train corr acc 38.71, val acc 67.37, val corr acc 35.39\n",
      "Ep 0 \titer 33  \tloss 1.21158, train acc 69.79, train corr acc 36.96, val acc 67.30, val corr acc 35.26\n",
      "Ep 0 \titer 34  \tloss 1.51233, train acc 65.62, train corr acc 32.65, val acc 67.88, val corr acc 36.40\n",
      "Ep 0 \titer 35  \tloss 1.72461, train acc 61.46, train corr acc 30.84, val acc 68.26, val corr acc 37.28\n",
      "Ep 0 \titer 36  \tloss 1.50663, train acc 64.58, train corr acc 33.98, val acc 68.26, val corr acc 37.78\n",
      "Ep 0 \titer 37  \tloss 1.63599, train acc 66.67, train corr acc 38.83, val acc 68.89, val corr acc 38.54\n",
      "Ep 0 \titer 38  \tloss 1.64662, train acc 68.23, train corr acc 40.20, val acc 69.53, val corr acc 39.80\n",
      "Ep 0 \titer 39  \tloss 1.25628, train acc 71.88, train corr acc 39.33, val acc 68.58, val corr acc 37.78\n",
      "Ep 0 \titer 40  \tloss 1.54467, train acc 66.15, train corr acc 35.00, val acc 67.43, val corr acc 35.52\n",
      "Ep 0 \titer 41  \tloss 1.62610, train acc 63.02, train corr acc 30.39, val acc 68.45, val corr acc 37.66\n",
      "Ep 0 \titer 42  \tloss 1.61127, train acc 65.10, train corr acc 36.79, val acc 69.34, val corr acc 39.55\n",
      "Ep 0 \titer 43  \tloss 1.40020, train acc 65.10, train corr acc 30.93, val acc 69.40, val corr acc 39.67\n",
      "Ep 0 \titer 44  \tloss 1.45773, train acc 69.27, train corr acc 43.56, val acc 67.75, val corr acc 37.15\n",
      "Ep 0 \titer 45  \tloss 1.46557, train acc 66.15, train corr acc 37.86, val acc 69.02, val corr acc 38.92\n",
      "Ep 0 \titer 46  \tloss 1.49650, train acc 64.06, train corr acc 33.98, val acc 69.02, val corr acc 38.79\n",
      "Ep 0 \titer 47  \tloss 1.28222, train acc 70.83, train corr acc 41.67, val acc 69.02, val corr acc 38.79\n",
      "Ep 0 \titer 48  \tloss 1.41531, train acc 64.58, train corr acc 34.62, val acc 69.91, val corr acc 40.43\n",
      "Ep 0 \titer 49  \tloss 1.37821, train acc 68.23, train corr acc 37.11, val acc 71.44, val corr acc 43.45\n",
      "Ep 0 \titer 50  \tloss 1.26039, train acc 68.75, train corr acc 39.39, val acc 71.56, val corr acc 43.70\n",
      "Ep 0 \titer 51  \tloss 1.48167, train acc 69.27, train corr acc 43.27, val acc 71.56, val corr acc 43.83\n",
      "Ep 0 \titer 52  \tloss 1.34194, train acc 69.79, train corr acc 47.27, val acc 72.14, val corr acc 45.09\n",
      "Ep 0 \titer 53  \tloss 1.50245, train acc 69.27, train corr acc 40.21, val acc 71.88, val corr acc 44.46\n",
      "Ep 0 \titer 54  \tloss 1.29130, train acc 72.92, train corr acc 43.48, val acc 72.20, val corr acc 44.96\n",
      "Ep 0 \titer 55  \tloss 1.34686, train acc 72.40, train corr acc 46.46, val acc 72.07, val corr acc 44.71\n",
      "Ep 0 \titer 56  \tloss 1.24665, train acc 72.40, train corr acc 43.62, val acc 72.07, val corr acc 44.84\n",
      "Ep 0 \titer 57  \tloss 1.38602, train acc 70.83, train corr acc 41.05, val acc 72.46, val corr acc 45.47\n",
      "Ep 0 \titer 58  \tloss 1.22579, train acc 69.79, train corr acc 41.41, val acc 72.39, val corr acc 45.34\n",
      "Ep 0 \titer 59  \tloss 1.19821, train acc 72.40, train corr acc 41.76, val acc 72.20, val corr acc 44.96\n",
      "Ep 0 \titer 60  \tloss 1.15518, train acc 74.48, train corr acc 43.68, val acc 72.33, val corr acc 45.21\n",
      "Ep 0 \titer 61  \tloss 1.13985, train acc 72.40, train corr acc 40.45, val acc 72.46, val corr acc 45.47\n",
      "Ep 0 \titer 62  \tloss 1.38632, train acc 71.88, train corr acc 46.53, val acc 73.35, val corr acc 47.23\n",
      "Ep 0 \titer 63  \tloss 1.29661, train acc 69.79, train corr acc 44.23, val acc 72.84, val corr acc 46.22\n",
      "Ep 0 \titer 64  \tloss 1.29360, train acc 71.35, train corr acc 46.08, val acc 72.58, val corr acc 45.72\n",
      "Ep 0 \titer 65  \tloss 1.33166, train acc 69.79, train corr acc 41.41, val acc 72.33, val corr acc 45.21\n",
      "Ep 0 \titer 66  \tloss 1.25252, train acc 71.88, train corr acc 42.55, val acc 73.16, val corr acc 46.85\n",
      "Ep 0 \titer 67  \tloss 1.23463, train acc 73.44, train corr acc 46.88, val acc 73.16, val corr acc 46.85\n",
      "Ep 0 \titer 68  \tloss 1.48964, train acc 69.79, train corr acc 40.21, val acc 73.66, val corr acc 47.86\n",
      "Ep 0 \titer 69  \tloss 1.27441, train acc 72.92, train corr acc 44.68, val acc 73.41, val corr acc 47.36\n",
      "Ep 0 \titer 70  \tloss 1.42173, train acc 68.75, train corr acc 43.93, val acc 73.22, val corr acc 46.98\n",
      "Ep 0 \titer 71  \tloss 1.20738, train acc 72.40, train corr acc 43.62, val acc 73.41, val corr acc 47.36\n",
      "Ep 0 \titer 72  \tloss 1.44573, train acc 69.27, train corr acc 42.72, val acc 73.79, val corr acc 48.11\n",
      "Ep 0 \titer 73  \tloss 1.30053, train acc 70.31, train corr acc 41.84, val acc 74.11, val corr acc 48.74\n",
      "Ep 0 \titer 74  \tloss 1.16379, train acc 72.92, train corr acc 43.48, val acc 74.17, val corr acc 48.87\n",
      "Ep 0 \titer 75  \tloss 1.36013, train acc 69.79, train corr acc 43.69, val acc 73.85, val corr acc 48.24\n",
      "Ep 0 \titer 76  \tloss 1.37382, train acc 72.40, train corr acc 48.04, val acc 73.79, val corr acc 48.11\n",
      "Ep 0 \titer 77  \tloss 1.27145, train acc 72.40, train corr acc 50.47, val acc 73.60, val corr acc 47.73\n",
      "Ep 0 \titer 78  \tloss 1.34103, train acc 71.88, train corr acc 45.45, val acc 73.41, val corr acc 47.36\n",
      "Ep 0 \titer 79  \tloss 1.19507, train acc 75.00, train corr acc 49.47, val acc 73.35, val corr acc 47.23\n",
      "Ep 0 \titer 80  \tloss 1.12867, train acc 75.52, train corr acc 50.00, val acc 73.60, val corr acc 47.73\n",
      "Ep 0 \titer 81  \tloss 1.39910, train acc 68.23, train corr acc 46.96, val acc 73.85, val corr acc 48.24\n",
      "Ep 0 \titer 82  \tloss 1.31795, train acc 70.83, train corr acc 43.43, val acc 74.11, val corr acc 48.74\n",
      "Ep 0 \titer 83  \tloss 1.29200, train acc 71.35, train corr acc 46.60, val acc 73.79, val corr acc 48.11\n",
      "Ep 0 \titer 84  \tloss 1.34308, train acc 68.23, train corr acc 37.76, val acc 73.73, val corr acc 47.98\n",
      "Ep 0 \titer 85  \tloss 1.18349, train acc 71.88, train corr acc 44.33, val acc 73.73, val corr acc 47.98\n",
      "Ep 0 \titer 86  \tloss 1.29198, train acc 70.83, train corr acc 44.00, val acc 73.60, val corr acc 47.73\n",
      "Ep 0 \titer 87  \tloss 1.38776, train acc 70.83, train corr acc 45.63, val acc 73.92, val corr acc 48.36\n",
      "Ep 0 \titer 88  \tloss 1.21837, train acc 75.00, train corr acc 48.94, val acc 74.24, val corr acc 48.99\n",
      "Ep 0 \titer 89  \tloss 1.28715, train acc 72.92, train corr acc 48.51, val acc 74.30, val corr acc 49.12\n",
      "Ep 0 \titer 90  \tloss 1.22932, train acc 69.79, train corr acc 45.28, val acc 74.05, val corr acc 48.61\n",
      "Ep 0 \titer 91  \tloss 1.17630, train acc 71.88, train corr acc 44.33, val acc 73.54, val corr acc 47.61\n",
      "Ep 0 \titer 92  \tloss 1.39332, train acc 66.15, train corr acc 34.34, val acc 73.54, val corr acc 47.61\n",
      "Ep 0 \titer 93  \tloss 1.28053, train acc 71.35, train corr acc 48.60, val acc 73.85, val corr acc 48.24\n",
      "Ep 0 \titer 94  \tloss 1.06262, train acc 76.56, train corr acc 53.61, val acc 73.85, val corr acc 48.24\n",
      "Ep 0 \titer 95  \tloss 1.20960, train acc 75.52, train corr acc 50.00, val acc 74.05, val corr acc 48.61\n",
      "Ep 0 \titer 96  \tloss 1.39729, train acc 71.35, train corr acc 46.60, val acc 73.98, val corr acc 48.49\n",
      "Ep 0 \titer 97  \tloss 1.13355, train acc 77.08, train corr acc 51.11, val acc 73.92, val corr acc 48.36\n",
      "Ep 0 \titer 98  \tloss 1.09098, train acc 72.40, train corr acc 44.79, val acc 73.60, val corr acc 47.73\n",
      "Ep 0 \titer 99  \tloss 1.31632, train acc 70.83, train corr acc 44.00, val acc 73.85, val corr acc 48.24\n",
      "Ep 0 \titer 100  \tloss 1.30562, train acc 72.92, train corr acc 46.39, val acc 74.17, val corr acc 48.87\n",
      "Ep 0 \titer 101  \tloss 1.23310, train acc 71.35, train corr acc 43.88, val acc 74.11, val corr acc 48.74\n",
      "Ep 0 \titer 102  \tloss 1.31976, train acc 69.27, train corr acc 45.87, val acc 74.24, val corr acc 48.99\n",
      "Ep 0 \titer 103  \tloss 1.03902, train acc 76.04, train corr acc 47.13, val acc 74.24, val corr acc 48.99\n",
      "Ep 0 \titer 104  \tloss 1.19667, train acc 75.52, train corr acc 47.19, val acc 73.98, val corr acc 48.49\n",
      "Ep 0 \titer 105  \tloss 1.18353, train acc 73.96, train corr acc 48.98, val acc 73.79, val corr acc 48.11\n",
      "Ep 0 \titer 106  \tloss 1.15941, train acc 74.48, train corr acc 51.49, val acc 73.85, val corr acc 48.24\n",
      "Ep 0 \titer 107  \tloss 1.07329, train acc 77.08, train corr acc 48.84, val acc 74.17, val corr acc 48.87\n",
      "Ep 0 \titer 108  \tloss 1.16734, train acc 75.00, train corr acc 50.52, val acc 74.68, val corr acc 49.87\n",
      "Ep 0 \titer 109  \tloss 0.90802, train acc 77.08, train corr acc 53.68, val acc 74.75, val corr acc 50.00\n",
      "Ep 0 \titer 110  \tloss 1.24284, train acc 76.04, train corr acc 48.89, val acc 74.43, val corr acc 49.37\n",
      "Ep 0 \titer 111  \tloss 1.01184, train acc 75.00, train corr acc 50.00, val acc 74.62, val corr acc 49.75\n",
      "Ep 0 \titer 112  \tloss 1.16887, train acc 75.00, train corr acc 51.02, val acc 74.62, val corr acc 49.75\n",
      "Ep 0 \titer 113  \tloss 1.24945, train acc 68.75, train corr acc 43.93, val acc 74.30, val corr acc 49.12\n",
      "Ep 0 \titer 114  \tloss 1.13801, train acc 75.52, train corr acc 50.00, val acc 74.43, val corr acc 49.37\n",
      "\n",
      "Epoch 1 of 10 took 60.709s\n",
      "  training loss:\t\t1.450534\n",
      "  training raw accuracy:\t\t69.63 %\n",
      "  training corrected acc:\t\t41.02 %\n",
      "  validation loss:\t\t1.153610\n",
      "  validation raw accuracy:\t\t74.62 %\n",
      "  validation corrected acc:\t\t49.75 % \n",
      "\n",
      "Ep 1 \titer 115  \tloss 1.21821, train acc 70.31, train corr acc 47.22, val acc 74.55, val corr acc 49.62\n",
      "Ep 1 \titer 116  \tloss 1.07130, train acc 78.12, train corr acc 54.35, val acc 74.30, val corr acc 49.12\n",
      "Ep 1 \titer 117  \tloss 1.14522, train acc 74.48, train corr acc 50.00, val acc 73.85, val corr acc 48.24\n",
      "Ep 1 \titer 118  \tloss 1.24076, train acc 73.44, train corr acc 47.96, val acc 74.49, val corr acc 49.50\n",
      "Ep 1 \titer 119  \tloss 1.22774, train acc 71.35, train corr acc 45.00, val acc 73.73, val corr acc 47.98\n",
      "Ep 1 \titer 120  \tloss 1.26471, train acc 71.88, train corr acc 50.46, val acc 73.92, val corr acc 48.36\n",
      "Ep 1 \titer 121  \tloss 1.31911, train acc 71.35, train corr acc 44.44, val acc 74.24, val corr acc 48.99\n",
      "Ep 1 \titer 122  \tloss 1.17978, train acc 72.40, train corr acc 45.36, val acc 74.49, val corr acc 49.50\n",
      "Ep 1 \titer 123  \tloss 1.11042, train acc 74.48, train corr acc 51.96, val acc 74.43, val corr acc 49.37\n",
      "Ep 1 \titer 124  \tloss 1.30651, train acc 69.27, train corr acc 45.37, val acc 74.11, val corr acc 48.74\n",
      "Ep 1 \titer 125  \tloss 1.14684, train acc 71.88, train corr acc 47.57, val acc 74.62, val corr acc 49.75\n",
      "Ep 1 \titer 126  \tloss 1.08985, train acc 73.96, train corr acc 46.81, val acc 74.49, val corr acc 49.50\n",
      "Ep 1 \titer 127  \tloss 1.19452, train acc 74.48, train corr acc 45.56, val acc 74.81, val corr acc 50.13\n",
      "Ep 1 \titer 128  \tloss 1.20800, train acc 74.48, train corr acc 48.96, val acc 74.81, val corr acc 50.13\n",
      "Ep 1 \titer 129  \tloss 1.36970, train acc 68.75, train corr acc 45.45, val acc 74.24, val corr acc 48.99\n",
      "Ep 1 \titer 130  \tloss 1.02273, train acc 77.60, train corr acc 54.26, val acc 74.11, val corr acc 48.74\n",
      "Ep 1 \titer 131  \tloss 1.15961, train acc 72.92, train corr acc 49.51, val acc 73.98, val corr acc 48.49\n",
      "Ep 1 \titer 132  \tloss 1.29755, train acc 69.79, train corr acc 44.23, val acc 73.54, val corr acc 47.61\n",
      "Ep 1 \titer 133  \tloss 1.21548, train acc 69.79, train corr acc 39.58, val acc 73.28, val corr acc 47.10\n",
      "Ep 1 \titer 134  \tloss 0.97605, train acc 76.56, train corr acc 53.12, val acc 74.05, val corr acc 48.61\n",
      "Ep 1 \titer 135  \tloss 0.94084, train acc 78.12, train corr acc 51.16, val acc 73.54, val corr acc 47.61\n",
      "Ep 1 \titer 136  \tloss 1.31884, train acc 72.92, train corr acc 50.00, val acc 74.05, val corr acc 48.61\n",
      "Ep 1 \titer 137  \tloss 0.99270, train acc 76.56, train corr acc 47.06, val acc 74.43, val corr acc 49.37\n",
      "Ep 1 \titer 138  \tloss 1.07621, train acc 76.04, train corr acc 54.46, val acc 73.73, val corr acc 47.98\n",
      "Ep 1 \titer 139  \tloss 1.16740, train acc 75.52, train corr acc 51.55, val acc 73.79, val corr acc 48.11\n",
      "Ep 1 \titer 140  \tloss 1.10371, train acc 76.04, train corr acc 53.06, val acc 73.92, val corr acc 48.36\n",
      "Ep 1 \titer 141  \tloss 1.25681, train acc 70.31, train corr acc 44.66, val acc 73.92, val corr acc 48.36\n",
      "Ep 1 \titer 142  \tloss 1.21888, train acc 70.83, train corr acc 43.43, val acc 74.36, val corr acc 49.24\n",
      "Ep 1 \titer 143  \tloss 1.17084, train acc 74.48, train corr acc 50.51, val acc 73.54, val corr acc 47.61\n",
      "Ep 1 \titer 144  \tloss 1.32855, train acc 74.48, train corr acc 51.96, val acc 73.66, val corr acc 47.86\n",
      "Ep 1 \titer 145  \tloss 1.17416, train acc 71.35, train corr acc 42.71, val acc 73.54, val corr acc 47.61\n",
      "Ep 1 \titer 146  \tloss 1.11903, train acc 75.52, train corr acc 49.46, val acc 73.79, val corr acc 48.11\n",
      "Ep 1 \titer 147  \tloss 0.95007, train acc 76.04, train corr acc 50.00, val acc 73.60, val corr acc 47.73\n",
      "Ep 1 \titer 148  \tloss 1.13200, train acc 71.88, train corr acc 44.90, val acc 74.43, val corr acc 49.37\n",
      "Ep 1 \titer 149  \tloss 1.20518, train acc 71.88, train corr acc 49.53, val acc 74.11, val corr acc 48.74\n",
      "Ep 1 \titer 150  \tloss 1.28367, train acc 71.35, train corr acc 46.60, val acc 74.62, val corr acc 49.75\n",
      "Ep 1 \titer 151  \tloss 1.25314, train acc 73.96, train corr acc 51.46, val acc 74.87, val corr acc 50.25\n",
      "Ep 1 \titer 152  \tloss 1.29108, train acc 71.88, train corr acc 47.06, val acc 74.55, val corr acc 49.62\n",
      "Ep 1 \titer 153  \tloss 1.02241, train acc 77.08, train corr acc 50.56, val acc 73.92, val corr acc 48.36\n",
      "Ep 1 \titer 154  \tloss 1.23573, train acc 71.88, train corr acc 46.00, val acc 74.36, val corr acc 49.24\n",
      "Ep 1 \titer 155  \tloss 1.21324, train acc 72.92, train corr acc 49.02, val acc 74.43, val corr acc 49.37\n",
      "Ep 1 \titer 156  \tloss 1.31463, train acc 70.83, train corr acc 47.17, val acc 74.55, val corr acc 49.62\n",
      "Ep 1 \titer 157  \tloss 1.09969, train acc 73.44, train corr acc 47.42, val acc 74.81, val corr acc 50.13\n",
      "Ep 1 \titer 158  \tloss 1.16325, train acc 73.96, train corr acc 50.50, val acc 75.13, val corr acc 50.76\n",
      "Ep 1 \titer 159  \tloss 1.16477, train acc 73.44, train corr acc 50.49, val acc 75.13, val corr acc 50.76\n",
      "Ep 1 \titer 160  \tloss 1.14790, train acc 72.40, train corr acc 48.54, val acc 75.06, val corr acc 50.63\n",
      "Ep 1 \titer 161  \tloss 1.02739, train acc 77.08, train corr acc 54.17, val acc 74.68, val corr acc 49.87\n",
      "Ep 1 \titer 162  \tloss 1.17567, train acc 72.40, train corr acc 49.04, val acc 74.43, val corr acc 49.37\n",
      "Ep 1 \titer 163  \tloss 1.20579, train acc 73.96, train corr acc 48.45, val acc 73.92, val corr acc 48.36\n",
      "Ep 1 \titer 164  \tloss 1.01789, train acc 76.04, train corr acc 53.54, val acc 74.55, val corr acc 49.62\n",
      "Ep 1 \titer 165  \tloss 1.27700, train acc 73.44, train corr acc 50.96, val acc 75.00, val corr acc 50.50\n",
      "Ep 1 \titer 166  \tloss 1.13655, train acc 69.79, train corr acc 47.27, val acc 74.36, val corr acc 49.24\n",
      "Ep 1 \titer 167  \tloss 1.32721, train acc 70.31, train corr acc 41.24, val acc 74.55, val corr acc 49.62\n",
      "Ep 1 \titer 168  \tloss 0.99592, train acc 78.65, train corr acc 55.43, val acc 75.13, val corr acc 50.76\n",
      "Ep 1 \titer 169  \tloss 1.08021, train acc 76.04, train corr acc 53.54, val acc 74.68, val corr acc 49.87\n",
      "Ep 1 \titer 170  \tloss 1.01746, train acc 77.08, train corr acc 53.19, val acc 75.00, val corr acc 50.50\n",
      "Ep 1 \titer 171  \tloss 1.22537, train acc 72.40, train corr acc 44.21, val acc 75.13, val corr acc 50.76\n",
      "Ep 1 \titer 172  \tloss 1.04267, train acc 74.48, train corr acc 50.51, val acc 74.87, val corr acc 50.25\n",
      "Ep 1 \titer 173  \tloss 0.98479, train acc 78.12, train corr acc 53.85, val acc 75.25, val corr acc 51.01\n",
      "Ep 1 \titer 174  \tloss 1.04116, train acc 77.60, train corr acc 50.57, val acc 75.13, val corr acc 50.76\n",
      "Ep 1 \titer 175  \tloss 0.97085, train acc 78.65, train corr acc 53.93, val acc 75.00, val corr acc 50.50\n",
      "Ep 1 \titer 176  \tloss 1.07607, train acc 77.08, train corr acc 56.44, val acc 75.19, val corr acc 50.88\n",
      "Ep 1 \titer 177  \tloss 1.09112, train acc 74.48, train corr acc 52.88, val acc 75.25, val corr acc 51.01\n",
      "Ep 1 \titer 178  \tloss 1.02954, train acc 78.65, train corr acc 59.80, val acc 75.25, val corr acc 51.01\n",
      "Ep 1 \titer 179  \tloss 1.06137, train acc 73.44, train corr acc 48.48, val acc 75.70, val corr acc 51.89\n",
      "Ep 1 \titer 180  \tloss 0.99033, train acc 77.60, train corr acc 54.26, val acc 75.51, val corr acc 51.51\n",
      "Ep 1 \titer 181  \tloss 1.02652, train acc 76.04, train corr acc 52.08, val acc 75.70, val corr acc 51.89\n",
      "Ep 1 \titer 182  \tloss 1.19331, train acc 72.92, train corr acc 46.39, val acc 75.25, val corr acc 51.01\n",
      "Ep 1 \titer 183  \tloss 1.16621, train acc 74.48, train corr acc 47.87, val acc 75.32, val corr acc 51.13\n",
      "Ep 1 \titer 184  \tloss 1.13082, train acc 72.92, train corr acc 51.40, val acc 75.51, val corr acc 51.51\n",
      "Ep 1 \titer 185  \tloss 1.01736, train acc 76.56, train corr acc 52.13, val acc 75.45, val corr acc 51.39\n",
      "Ep 1 \titer 186  \tloss 1.17912, train acc 72.92, train corr acc 49.51, val acc 74.81, val corr acc 50.13\n",
      "Ep 1 \titer 187  \tloss 1.10181, train acc 73.96, train corr acc 48.98, val acc 75.32, val corr acc 51.13\n",
      "Ep 1 \titer 188  \tloss 1.02623, train acc 74.48, train corr acc 46.74, val acc 76.15, val corr acc 52.77\n",
      "Ep 1 \titer 189  \tloss 1.19690, train acc 72.92, train corr acc 49.51, val acc 76.15, val corr acc 52.77\n",
      "Ep 1 \titer 190  \tloss 1.20052, train acc 74.48, train corr acc 51.96, val acc 75.64, val corr acc 51.76\n",
      "Ep 1 \titer 191  \tloss 1.10674, train acc 74.48, train corr acc 54.21, val acc 75.95, val corr acc 52.39\n",
      "Ep 1 \titer 192  \tloss 1.13590, train acc 73.96, train corr acc 49.49, val acc 76.02, val corr acc 52.52\n",
      "Ep 1 \titer 193  \tloss 1.13444, train acc 76.04, train corr acc 51.58, val acc 75.64, val corr acc 51.76\n",
      "Ep 1 \titer 194  \tloss 0.97526, train acc 77.60, train corr acc 54.26, val acc 75.51, val corr acc 51.51\n",
      "Ep 1 \titer 195  \tloss 1.13940, train acc 73.44, train corr acc 55.65, val acc 76.21, val corr acc 52.90\n",
      "Ep 1 \titer 196  \tloss 1.12052, train acc 73.44, train corr acc 48.48, val acc 75.06, val corr acc 50.63\n",
      "Ep 1 \titer 197  \tloss 1.08281, train acc 75.00, train corr acc 53.40, val acc 75.83, val corr acc 52.14\n",
      "Ep 1 \titer 198  \tloss 1.21242, train acc 69.79, train corr acc 40.82, val acc 75.25, val corr acc 51.01\n",
      "Ep 1 \titer 199  \tloss 1.08214, train acc 74.48, train corr acc 49.48, val acc 75.57, val corr acc 51.64\n",
      "Ep 1 \titer 200  \tloss 1.09684, train acc 72.92, train corr acc 48.00, val acc 76.08, val corr acc 52.64\n",
      "Ep 1 \titer 201  \tloss 1.16420, train acc 72.92, train corr acc 49.51, val acc 76.40, val corr acc 53.27\n",
      "Ep 1 \titer 202  \tloss 1.05528, train acc 75.52, train corr acc 50.00, val acc 76.02, val corr acc 52.52\n",
      "Ep 1 \titer 203  \tloss 1.08451, train acc 73.96, train corr acc 50.50, val acc 76.02, val corr acc 52.52\n",
      "Ep 1 \titer 204  \tloss 1.08989, train acc 72.92, train corr acc 50.94, val acc 75.89, val corr acc 52.27\n",
      "Ep 1 \titer 205  \tloss 1.01631, train acc 76.04, train corr acc 52.58, val acc 76.08, val corr acc 52.64\n",
      "Ep 1 \titer 206  \tloss 1.19930, train acc 70.31, train corr acc 42.42, val acc 75.95, val corr acc 52.39\n",
      "Ep 1 \titer 207  \tloss 1.13002, train acc 72.40, train corr acc 50.47, val acc 75.45, val corr acc 51.39\n",
      "Ep 1 \titer 208  \tloss 1.02771, train acc 74.48, train corr acc 49.48, val acc 76.34, val corr acc 53.15\n",
      "Ep 1 \titer 209  \tloss 1.06237, train acc 77.60, train corr acc 54.26, val acc 76.40, val corr acc 53.27\n",
      "Ep 1 \titer 210  \tloss 1.22432, train acc 70.31, train corr acc 44.66, val acc 75.95, val corr acc 52.39\n",
      "Ep 1 \titer 211  \tloss 0.97896, train acc 77.60, train corr acc 52.22, val acc 76.72, val corr acc 53.90\n",
      "Ep 1 \titer 212  \tloss 0.96777, train acc 73.96, train corr acc 47.92, val acc 76.27, val corr acc 53.02\n",
      "Ep 1 \titer 213  \tloss 1.14247, train acc 72.92, train corr acc 48.00, val acc 75.83, val corr acc 52.14\n",
      "Ep 1 \titer 214  \tloss 1.16075, train acc 73.96, train corr acc 48.45, val acc 75.51, val corr acc 51.51\n",
      "Ep 1 \titer 215  \tloss 1.02857, train acc 75.52, train corr acc 52.04, val acc 75.06, val corr acc 50.63\n",
      "Ep 1 \titer 216  \tloss 1.19377, train acc 68.75, train corr acc 44.95, val acc 75.25, val corr acc 51.01\n",
      "Ep 1 \titer 217  \tloss 0.91010, train acc 77.08, train corr acc 49.43, val acc 75.06, val corr acc 50.63\n",
      "Ep 1 \titer 218  \tloss 1.03154, train acc 77.60, train corr acc 51.69, val acc 75.00, val corr acc 50.50\n",
      "Ep 1 \titer 219  \tloss 1.07188, train acc 76.04, train corr acc 53.06, val acc 74.87, val corr acc 50.25\n",
      "Ep 1 \titer 220  \tloss 1.05119, train acc 76.04, train corr acc 54.46, val acc 75.32, val corr acc 51.13\n",
      "Ep 1 \titer 221  \tloss 0.92839, train acc 79.69, train corr acc 54.65, val acc 75.45, val corr acc 51.39\n",
      "Ep 1 \titer 222  \tloss 1.01216, train acc 75.00, train corr acc 50.52, val acc 75.64, val corr acc 51.76\n",
      "Ep 1 \titer 223  \tloss 0.82281, train acc 82.81, train corr acc 65.26, val acc 75.64, val corr acc 51.76\n",
      "Ep 1 \titer 224  \tloss 1.08109, train acc 73.44, train corr acc 43.33, val acc 75.64, val corr acc 51.76\n",
      "Ep 1 \titer 225  \tloss 0.92474, train acc 76.04, train corr acc 52.08, val acc 76.08, val corr acc 52.64\n",
      "Ep 1 \titer 226  \tloss 1.01393, train acc 75.52, train corr acc 52.04, val acc 75.64, val corr acc 51.76\n",
      "Ep 1 \titer 227  \tloss 1.15829, train acc 67.19, train corr acc 41.12, val acc 75.95, val corr acc 52.39\n",
      "Ep 1 \titer 228  \tloss 0.99124, train acc 75.52, train corr acc 50.00, val acc 75.57, val corr acc 51.64\n",
      "\n",
      "Epoch 2 of 10 took 61.816s\n",
      "  training loss:\t\t1.120456\n",
      "  training raw accuracy:\t\t74.15 %\n",
      "  training corrected acc:\t\t49.70 %\n",
      "  validation loss:\t\t1.058919\n",
      "  validation raw accuracy:\t\t76.08 %\n",
      "  validation corrected acc:\t\t52.64 % \n",
      "\n",
      "Ep 2 \titer 229  \tloss 1.14781, train acc 72.40, train corr acc 50.93, val acc 75.57, val corr acc 51.64\n",
      "Ep 2 \titer 230  \tloss 0.93290, train acc 79.17, train corr acc 56.52, val acc 76.15, val corr acc 52.77\n",
      "Ep 2 \titer 231  \tloss 1.02948, train acc 76.04, train corr acc 53.06, val acc 76.08, val corr acc 52.64\n",
      "Ep 2 \titer 232  \tloss 1.16367, train acc 73.96, train corr acc 48.98, val acc 75.95, val corr acc 52.39\n",
      "Ep 2 \titer 233  \tloss 1.12254, train acc 73.44, train corr acc 49.00, val acc 75.38, val corr acc 51.26\n",
      "Ep 2 \titer 234  \tloss 1.12042, train acc 73.96, train corr acc 54.13, val acc 75.25, val corr acc 51.01\n",
      "Ep 2 \titer 235  \tloss 1.18686, train acc 72.40, train corr acc 46.46, val acc 75.70, val corr acc 51.89\n",
      "Ep 2 \titer 236  \tloss 1.07618, train acc 73.96, train corr acc 48.45, val acc 75.45, val corr acc 51.39\n",
      "Ep 2 \titer 237  \tloss 0.99296, train acc 75.52, train corr acc 53.92, val acc 75.51, val corr acc 51.51\n",
      "Ep 2 \titer 238  \tloss 1.20172, train acc 70.83, train corr acc 48.15, val acc 75.64, val corr acc 51.76\n",
      "Ep 2 \titer 239  \tloss 1.04235, train acc 73.44, train corr acc 50.49, val acc 76.02, val corr acc 52.52\n",
      "Ep 2 \titer 240  \tloss 0.99465, train acc 76.04, train corr acc 51.06, val acc 76.15, val corr acc 52.77\n",
      "Ep 2 \titer 241  \tloss 1.05511, train acc 77.08, train corr acc 51.11, val acc 76.53, val corr acc 53.53\n",
      "Ep 2 \titer 242  \tloss 1.09774, train acc 74.48, train corr acc 48.96, val acc 76.02, val corr acc 52.52\n",
      "Ep 2 \titer 243  \tloss 1.22698, train acc 71.88, train corr acc 50.91, val acc 75.76, val corr acc 52.02\n",
      "Ep 2 \titer 244  \tloss 0.96093, train acc 76.04, train corr acc 51.06, val acc 76.15, val corr acc 52.77\n",
      "Ep 2 \titer 245  \tloss 1.02385, train acc 73.96, train corr acc 51.46, val acc 75.70, val corr acc 51.89\n",
      "Ep 2 \titer 246  \tloss 1.02770, train acc 76.56, train corr acc 56.73, val acc 75.57, val corr acc 51.64\n",
      "Ep 2 \titer 247  \tloss 1.04652, train acc 72.92, train corr acc 45.83, val acc 75.57, val corr acc 51.64\n",
      "Ep 2 \titer 248  \tloss 0.92216, train acc 78.12, train corr acc 56.25, val acc 75.70, val corr acc 51.89\n",
      "Ep 2 \titer 249  \tloss 0.81908, train acc 79.17, train corr acc 53.49, val acc 75.83, val corr acc 52.14\n",
      "Ep 2 \titer 250  \tloss 1.16121, train acc 73.44, train corr acc 50.96, val acc 75.32, val corr acc 51.13\n",
      "Ep 2 \titer 251  \tloss 0.89115, train acc 78.65, train corr acc 51.76, val acc 75.51, val corr acc 51.51\n",
      "Ep 2 \titer 252  \tloss 1.00338, train acc 76.04, train corr acc 54.46, val acc 75.25, val corr acc 51.01\n",
      "Ep 2 \titer 253  \tloss 1.03568, train acc 78.12, train corr acc 56.70, val acc 75.13, val corr acc 50.76\n",
      "Ep 2 \titer 254  \tloss 1.01701, train acc 76.04, train corr acc 53.06, val acc 75.19, val corr acc 50.88\n",
      "Ep 2 \titer 255  \tloss 1.13328, train acc 71.88, train corr acc 47.57, val acc 75.32, val corr acc 51.13\n",
      "Ep 2 \titer 256  \tloss 1.06824, train acc 73.96, train corr acc 49.49, val acc 75.32, val corr acc 51.13\n",
      "Ep 2 \titer 257  \tloss 1.11114, train acc 73.44, train corr acc 48.48, val acc 75.70, val corr acc 51.89\n",
      "Ep 2 \titer 258  \tloss 1.15942, train acc 73.44, train corr acc 50.00, val acc 75.83, val corr acc 52.14\n",
      "Ep 2 \titer 259  \tloss 1.09499, train acc 73.44, train corr acc 46.88, val acc 75.76, val corr acc 52.02\n",
      "Ep 2 \titer 260  \tloss 0.96479, train acc 77.60, train corr acc 53.76, val acc 75.89, val corr acc 52.27\n",
      "Ep 2 \titer 261  \tloss 0.83814, train acc 81.77, train corr acc 61.96, val acc 75.32, val corr acc 51.13\n",
      "Ep 2 \titer 262  \tloss 0.98771, train acc 78.12, train corr acc 57.14, val acc 76.02, val corr acc 52.52\n",
      "Ep 2 \titer 263  \tloss 1.12388, train acc 77.08, train corr acc 58.88, val acc 76.02, val corr acc 52.52\n",
      "Ep 2 \titer 264  \tloss 1.14138, train acc 72.92, train corr acc 49.51, val acc 75.89, val corr acc 52.27\n",
      "Ep 2 \titer 265  \tloss 1.10178, train acc 78.12, train corr acc 59.22, val acc 76.15, val corr acc 52.77\n",
      "Ep 2 \titer 266  \tloss 1.18515, train acc 73.96, train corr acc 50.98, val acc 75.76, val corr acc 52.02\n",
      "Ep 2 \titer 267  \tloss 0.89376, train acc 79.69, train corr acc 56.18, val acc 75.89, val corr acc 52.27\n",
      "Ep 2 \titer 268  \tloss 1.07392, train acc 73.96, train corr acc 50.00, val acc 75.64, val corr acc 51.76\n",
      "Ep 2 \titer 269  \tloss 1.03614, train acc 76.04, train corr acc 54.90, val acc 75.45, val corr acc 51.39\n",
      "Ep 2 \titer 270  \tloss 1.21023, train acc 72.92, train corr acc 50.94, val acc 75.38, val corr acc 51.26\n",
      "Ep 2 \titer 271  \tloss 0.94912, train acc 76.04, train corr acc 52.58, val acc 75.89, val corr acc 52.27\n",
      "Ep 2 \titer 272  \tloss 1.05667, train acc 76.04, train corr acc 54.46, val acc 75.51, val corr acc 51.51\n",
      "Ep 2 \titer 273  \tloss 1.06534, train acc 74.48, train corr acc 52.43, val acc 75.64, val corr acc 51.76\n",
      "Ep 2 \titer 274  \tloss 1.00866, train acc 75.00, train corr acc 53.40, val acc 75.25, val corr acc 51.01\n",
      "Ep 2 \titer 275  \tloss 0.91126, train acc 77.08, train corr acc 54.17, val acc 75.32, val corr acc 51.13\n",
      "Ep 2 \titer 276  \tloss 1.07518, train acc 72.92, train corr acc 50.00, val acc 75.76, val corr acc 52.02\n",
      "Ep 2 \titer 277  \tloss 1.08107, train acc 74.48, train corr acc 49.48, val acc 75.32, val corr acc 51.13\n",
      "Ep 2 \titer 278  \tloss 0.95951, train acc 78.12, train corr acc 57.58, val acc 75.32, val corr acc 51.13\n",
      "Ep 2 \titer 279  \tloss 1.17791, train acc 73.44, train corr acc 50.96, val acc 76.02, val corr acc 52.52\n",
      "Ep 2 \titer 280  \tloss 1.03104, train acc 73.96, train corr acc 54.55, val acc 75.76, val corr acc 52.02\n",
      "Ep 2 \titer 281  \tloss 1.17087, train acc 71.88, train corr acc 44.33, val acc 75.76, val corr acc 52.02\n",
      "Ep 2 \titer 282  \tloss 0.86877, train acc 78.65, train corr acc 55.43, val acc 75.89, val corr acc 52.27\n",
      "Ep 2 \titer 283  \tloss 0.95701, train acc 77.60, train corr acc 56.57, val acc 76.27, val corr acc 53.02\n",
      "Ep 2 \titer 284  \tloss 0.91210, train acc 77.08, train corr acc 53.19, val acc 76.02, val corr acc 52.52\n",
      "Ep 2 \titer 285  \tloss 1.12700, train acc 73.44, train corr acc 46.32, val acc 75.89, val corr acc 52.27\n",
      "Ep 2 \titer 286  \tloss 0.98028, train acc 73.96, train corr acc 49.49, val acc 75.57, val corr acc 51.64\n",
      "Ep 2 \titer 287  \tloss 0.82715, train acc 80.21, train corr acc 58.24, val acc 75.38, val corr acc 51.26\n",
      "Ep 2 \titer 288  \tloss 0.92730, train acc 78.12, train corr acc 51.72, val acc 75.00, val corr acc 50.50\n",
      "Ep 2 \titer 289  \tloss 0.87003, train acc 78.65, train corr acc 53.93, val acc 75.32, val corr acc 51.13\n",
      "Ep 2 \titer 290  \tloss 0.95608, train acc 76.04, train corr acc 54.46, val acc 75.00, val corr acc 50.50\n",
      "Ep 2 \titer 291  \tloss 0.96691, train acc 73.96, train corr acc 51.92, val acc 74.68, val corr acc 49.87\n",
      "Ep 2 \titer 292  \tloss 0.91612, train acc 79.17, train corr acc 60.78, val acc 74.49, val corr acc 49.50\n",
      "Ep 2 \titer 293  \tloss 0.93794, train acc 77.08, train corr acc 55.56, val acc 75.06, val corr acc 50.63\n",
      "Ep 2 \titer 294  \tloss 0.91916, train acc 77.08, train corr acc 53.19, val acc 74.36, val corr acc 49.24\n",
      "Ep 2 \titer 295  \tloss 0.96798, train acc 76.04, train corr acc 52.08, val acc 75.89, val corr acc 52.27\n",
      "Ep 2 \titer 296  \tloss 1.06311, train acc 75.52, train corr acc 51.55, val acc 75.70, val corr acc 51.89\n",
      "Ep 2 \titer 297  \tloss 1.06385, train acc 75.00, train corr acc 48.94, val acc 75.70, val corr acc 51.89\n",
      "Ep 2 \titer 298  \tloss 1.00776, train acc 72.92, train corr acc 51.40, val acc 75.64, val corr acc 51.76\n",
      "Ep 2 \titer 299  \tloss 0.95378, train acc 78.65, train corr acc 56.38, val acc 75.32, val corr acc 51.13\n",
      "Ep 2 \titer 300  \tloss 1.05184, train acc 75.52, train corr acc 54.37, val acc 75.83, val corr acc 52.14\n",
      "Ep 2 \titer 301  \tloss 0.99069, train acc 77.60, train corr acc 56.12, val acc 75.06, val corr acc 50.63\n",
      "Ep 2 \titer 302  \tloss 0.96057, train acc 76.56, train corr acc 51.09, val acc 75.76, val corr acc 52.02\n",
      "Ep 2 \titer 303  \tloss 1.07053, train acc 73.96, train corr acc 51.46, val acc 75.45, val corr acc 51.39\n",
      "Ep 2 \titer 304  \tloss 1.13281, train acc 72.92, train corr acc 49.02, val acc 74.81, val corr acc 50.13\n",
      "Ep 2 \titer 305  \tloss 1.00651, train acc 76.04, train corr acc 57.01, val acc 75.00, val corr acc 50.50\n",
      "Ep 2 \titer 306  \tloss 1.84207, train acc 70.83, train corr acc 47.47, val acc 75.89, val corr acc 52.27\n",
      "Ep 2 \titer 307  \tloss 1.09927, train acc 74.48, train corr acc 48.42, val acc 75.00, val corr acc 50.50\n",
      "Ep 2 \titer 308  \tloss 0.97514, train acc 78.12, train corr acc 55.32, val acc 74.87, val corr acc 50.25\n",
      "Ep 2 \titer 309  \tloss 1.16239, train acc 70.31, train corr acc 50.43, val acc 74.94, val corr acc 50.38\n",
      "Ep 2 \titer 310  \tloss 1.10091, train acc 71.35, train corr acc 44.44, val acc 74.17, val corr acc 48.87\n",
      "Ep 2 \titer 311  \tloss 1.07775, train acc 75.52, train corr acc 54.37, val acc 74.49, val corr acc 49.50\n",
      "Ep 2 \titer 312  \tloss 1.13636, train acc 72.40, train corr acc 45.92, val acc 75.19, val corr acc 50.88\n",
      "Ep 2 \titer 313  \tloss 1.02109, train acc 75.00, train corr acc 50.52, val acc 74.75, val corr acc 50.00\n",
      "Ep 2 \titer 314  \tloss 1.11122, train acc 70.31, train corr acc 43.00, val acc 75.38, val corr acc 51.26\n",
      "Ep 2 \titer 315  \tloss 1.15580, train acc 73.96, train corr acc 51.46, val acc 74.87, val corr acc 50.25\n",
      "Ep 2 \titer 316  \tloss 1.03466, train acc 75.52, train corr acc 50.00, val acc 74.62, val corr acc 49.75\n",
      "Ep 2 \titer 317  \tloss 1.12819, train acc 73.44, train corr acc 49.50, val acc 75.57, val corr acc 51.64\n",
      "Ep 2 \titer 318  \tloss 1.08163, train acc 71.35, train corr acc 48.11, val acc 75.32, val corr acc 51.13\n",
      "Ep 2 \titer 319  \tloss 1.08043, train acc 75.00, train corr acc 50.52, val acc 74.87, val corr acc 50.25\n",
      "Ep 2 \titer 320  \tloss 1.19170, train acc 70.83, train corr acc 43.43, val acc 73.79, val corr acc 48.11\n",
      "Ep 2 \titer 321  \tloss 1.16896, train acc 73.96, train corr acc 53.27, val acc 74.81, val corr acc 50.13\n",
      "Ep 2 \titer 322  \tloss 1.06316, train acc 76.04, train corr acc 52.58, val acc 75.25, val corr acc 51.01\n",
      "Ep 2 \titer 323  \tloss 1.15618, train acc 73.96, train corr acc 46.81, val acc 74.75, val corr acc 50.00\n",
      "Ep 2 \titer 324  \tloss 1.24838, train acc 69.79, train corr acc 43.69, val acc 75.06, val corr acc 50.63\n",
      "Ep 2 \titer 325  \tloss 1.03665, train acc 78.12, train corr acc 53.33, val acc 74.87, val corr acc 50.25\n",
      "Ep 2 \titer 326  \tloss 1.02413, train acc 73.96, train corr acc 47.92, val acc 74.17, val corr acc 48.87\n",
      "Ep 2 \titer 327  \tloss 1.15628, train acc 73.44, train corr acc 49.00, val acc 74.55, val corr acc 49.62\n",
      "Ep 2 \titer 328  \tloss 1.16751, train acc 73.96, train corr acc 48.45, val acc 71.88, val corr acc 44.33\n",
      "Ep 2 \titer 329  \tloss 1.22960, train acc 72.92, train corr acc 46.94, val acc 72.14, val corr acc 45.47\n",
      "Ep 2 \titer 330  \tloss 1.39318, train acc 66.67, train corr acc 41.28, val acc 71.82, val corr acc 44.21\n",
      "Ep 2 \titer 331  \tloss 1.08809, train acc 74.48, train corr acc 43.68, val acc 72.71, val corr acc 45.97\n",
      "Ep 2 \titer 332  \tloss 1.17465, train acc 72.92, train corr acc 41.57, val acc 73.09, val corr acc 46.85\n",
      "Ep 2 \titer 333  \tloss 1.16191, train acc 73.96, train corr acc 48.98, val acc 73.85, val corr acc 48.24\n",
      "Ep 2 \titer 334  \tloss 1.14569, train acc 73.96, train corr acc 50.50, val acc 73.79, val corr acc 48.24\n",
      "Ep 2 \titer 335  \tloss 1.07215, train acc 78.12, train corr acc 51.16, val acc 73.54, val corr acc 47.86\n",
      "Ep 2 \titer 336  \tloss 1.18714, train acc 73.44, train corr acc 47.42, val acc 74.24, val corr acc 49.12\n",
      "Ep 2 \titer 337  \tloss 0.96826, train acc 78.12, train corr acc 55.79, val acc 73.66, val corr acc 47.86\n",
      "Ep 2 \titer 338  \tloss 1.22187, train acc 75.00, train corr acc 46.67, val acc 74.30, val corr acc 49.12\n",
      "Ep 2 \titer 339  \tloss 1.02031, train acc 77.08, train corr acc 54.17, val acc 73.79, val corr acc 48.11\n",
      "Ep 2 \titer 340  \tloss 1.16219, train acc 72.40, train corr acc 45.92, val acc 73.09, val corr acc 46.73\n",
      "Ep 2 \titer 341  \tloss 1.24057, train acc 67.19, train corr acc 41.12, val acc 73.92, val corr acc 48.36\n",
      "Ep 2 \titer 342  \tloss 1.17100, train acc 73.44, train corr acc 45.74, val acc 71.69, val corr acc 43.95\n",
      "\n",
      "Epoch 3 of 10 took 68.055s\n",
      "  training loss:\t\t1.068442\n",
      "  training raw accuracy:\t\t74.90 %\n",
      "  training corrected acc:\t\t51.17 %\n",
      "  validation loss:\t\t1.198060\n",
      "  validation raw accuracy:\t\t71.69 %\n",
      "  validation corrected acc:\t\t43.95 % \n",
      "\n",
      "Ep 3 \titer 343  \tloss 1.32064, train acc 68.75, train corr acc 44.44, val acc 74.49, val corr acc 49.50\n",
      "Ep 3 \titer 344  \tloss 1.04656, train acc 75.52, train corr acc 48.91, val acc 73.35, val corr acc 47.23\n",
      "Ep 3 \titer 345  \tloss 1.15918, train acc 70.83, train corr acc 42.86, val acc 74.68, val corr acc 49.87\n",
      "Ep 3 \titer 346  \tloss 1.24581, train acc 70.31, train corr acc 41.84, val acc 73.73, val corr acc 47.98\n",
      "Ep 3 \titer 347  \tloss 1.22381, train acc 71.88, train corr acc 46.00, val acc 72.26, val corr acc 45.09\n",
      "Ep 3 \titer 348  \tloss 1.26019, train acc 68.23, train corr acc 44.04, val acc 74.17, val corr acc 48.87\n",
      "Ep 3 \titer 349  \tloss 1.30966, train acc 71.88, train corr acc 45.45, val acc 74.30, val corr acc 49.12\n",
      "Ep 3 \titer 350  \tloss 1.17404, train acc 71.88, train corr acc 44.33, val acc 73.66, val corr acc 47.86\n",
      "Ep 3 \titer 351  \tloss 1.15997, train acc 75.52, train corr acc 53.92, val acc 73.92, val corr acc 48.36\n",
      "Ep 3 \titer 352  \tloss 1.38052, train acc 69.79, train corr acc 46.30, val acc 73.66, val corr acc 47.86\n",
      "Ep 3 \titer 353  \tloss 1.20874, train acc 72.92, train corr acc 49.51, val acc 73.35, val corr acc 47.23\n",
      "Ep 3 \titer 354  \tloss 1.20632, train acc 73.44, train corr acc 45.74, val acc 73.92, val corr acc 48.36\n",
      "Ep 3 \titer 355  \tloss 1.19242, train acc 72.40, train corr acc 41.11, val acc 73.66, val corr acc 47.86\n",
      "Ep 3 \titer 356  \tloss 1.24584, train acc 72.92, train corr acc 45.83, val acc 73.54, val corr acc 47.61\n",
      "Ep 3 \titer 357  \tloss 1.37036, train acc 69.79, train corr acc 47.27, val acc 73.47, val corr acc 47.48\n",
      "Ep 3 \titer 358  \tloss 1.12132, train acc 74.48, train corr acc 47.87, val acc 73.35, val corr acc 47.23\n",
      "Ep 3 \titer 359  \tloss 1.21902, train acc 70.31, train corr acc 44.66, val acc 73.35, val corr acc 47.23\n",
      "Ep 3 \titer 360  \tloss 1.26819, train acc 70.31, train corr acc 45.19, val acc 74.05, val corr acc 48.61\n",
      "Ep 3 \titer 361  \tloss 1.16309, train acc 72.40, train corr acc 44.79, val acc 74.55, val corr acc 49.62\n",
      "Ep 3 \titer 362  \tloss 1.07993, train acc 76.04, train corr acc 52.08, val acc 73.66, val corr acc 47.86\n",
      "Ep 3 \titer 363  \tloss 0.99841, train acc 75.52, train corr acc 45.35, val acc 73.66, val corr acc 47.86\n",
      "Ep 3 \titer 364  \tloss 1.33128, train acc 70.83, train corr acc 46.15, val acc 74.49, val corr acc 49.50\n",
      "Ep 3 \titer 365  \tloss 0.99579, train acc 75.52, train corr acc 44.71, val acc 73.47, val corr acc 47.48\n",
      "Ep 3 \titer 366  \tloss 1.16521, train acc 72.92, train corr acc 48.51, val acc 73.60, val corr acc 47.73\n",
      "Ep 3 \titer 367  \tloss 1.16906, train acc 75.52, train corr acc 51.55, val acc 73.28, val corr acc 47.10\n",
      "Ep 3 \titer 368  \tloss 1.12626, train acc 75.52, train corr acc 52.04, val acc 74.24, val corr acc 48.99\n",
      "Ep 3 \titer 369  \tloss 1.22311, train acc 72.40, train corr acc 48.54, val acc 72.96, val corr acc 46.47\n",
      "Ep 3 \titer 370  \tloss 1.16119, train acc 69.79, train corr acc 41.41, val acc 73.98, val corr acc 48.49\n",
      "Ep 3 \titer 371  \tloss 1.19543, train acc 71.88, train corr acc 45.45, val acc 73.79, val corr acc 48.11\n",
      "Ep 3 \titer 372  \tloss 1.33170, train acc 71.35, train corr acc 46.08, val acc 73.92, val corr acc 48.36\n",
      "Ep 3 \titer 373  \tloss 1.17524, train acc 72.40, train corr acc 44.79, val acc 74.36, val corr acc 49.24\n",
      "Ep 3 \titer 374  \tloss 1.11435, train acc 75.52, train corr acc 49.46, val acc 73.73, val corr acc 47.98\n",
      "Ep 3 \titer 375  \tloss 0.99772, train acc 77.08, train corr acc 52.17, val acc 74.24, val corr acc 48.99\n",
      "Ep 3 \titer 376  \tloss 1.06842, train acc 72.40, train corr acc 45.92, val acc 74.62, val corr acc 49.75\n",
      "Ep 3 \titer 377  \tloss 1.22219, train acc 72.92, train corr acc 51.40, val acc 74.05, val corr acc 48.61\n",
      "Ep 3 \titer 378  \tloss 1.21623, train acc 70.31, train corr acc 44.66, val acc 74.30, val corr acc 49.12\n",
      "Ep 3 \titer 379  \tloss 1.18634, train acc 73.44, train corr acc 50.49, val acc 73.85, val corr acc 48.24\n",
      "Ep 3 \titer 380  \tloss 1.36939, train acc 69.79, train corr acc 43.14, val acc 74.11, val corr acc 48.74\n",
      "Ep 3 \titer 381  \tloss 1.02867, train acc 77.60, train corr acc 51.69, val acc 73.73, val corr acc 47.98\n",
      "Ep 3 \titer 382  \tloss 1.16843, train acc 72.92, train corr acc 48.00, val acc 73.35, val corr acc 47.23\n",
      "Ep 3 \titer 383  \tloss 1.27832, train acc 68.75, train corr acc 41.18, val acc 73.09, val corr acc 46.73\n",
      "Ep 3 \titer 384  \tloss 1.30981, train acc 69.79, train corr acc 45.28, val acc 73.41, val corr acc 47.36\n",
      "Ep 3 \titer 385  \tloss 1.09162, train acc 70.83, train corr acc 42.27, val acc 73.85, val corr acc 48.24\n",
      "Ep 3 \titer 386  \tloss 1.20311, train acc 73.44, train corr acc 49.50, val acc 74.17, val corr acc 48.87\n",
      "Ep 3 \titer 387  \tloss 1.15604, train acc 71.88, train corr acc 47.57, val acc 73.35, val corr acc 47.23\n",
      "Ep 3 \titer 388  \tloss 1.22716, train acc 70.31, train corr acc 44.66, val acc 73.35, val corr acc 47.23\n",
      "Ep 3 \titer 389  \tloss 1.05638, train acc 74.48, train corr acc 48.96, val acc 73.79, val corr acc 48.11\n",
      "Ep 3 \titer 390  \tloss 1.15518, train acc 73.96, train corr acc 51.92, val acc 73.41, val corr acc 47.36\n",
      "Ep 3 \titer 391  \tloss 1.17975, train acc 71.88, train corr acc 44.33, val acc 74.30, val corr acc 49.12\n",
      "Ep 3 \titer 392  \tloss 1.06306, train acc 73.44, train corr acc 48.48, val acc 74.11, val corr acc 48.74\n",
      "Ep 3 \titer 393  \tloss 1.28654, train acc 73.96, train corr acc 51.92, val acc 73.98, val corr acc 48.49\n",
      "Ep 3 \titer 394  \tloss 1.10821, train acc 73.96, train corr acc 54.55, val acc 73.54, val corr acc 47.61\n",
      "Ep 3 \titer 395  \tloss 1.33540, train acc 68.75, train corr acc 38.14, val acc 73.98, val corr acc 48.49\n",
      "Ep 3 \titer 396  \tloss 1.09427, train acc 73.44, train corr acc 44.57, val acc 75.00, val corr acc 50.50\n",
      "Ep 3 \titer 397  \tloss 1.15617, train acc 72.92, train corr acc 47.47, val acc 74.05, val corr acc 48.61\n",
      "Ep 3 \titer 398  \tloss 1.02891, train acc 75.52, train corr acc 50.00, val acc 74.11, val corr acc 48.74\n",
      "Ep 3 \titer 399  \tloss 1.27626, train acc 72.92, train corr acc 45.26, val acc 74.49, val corr acc 49.50\n",
      "Ep 3 \titer 400  \tloss 1.05839, train acc 72.40, train corr acc 46.46, val acc 74.17, val corr acc 48.87\n",
      "Ep 3 \titer 401  \tloss 1.03480, train acc 76.04, train corr acc 49.45, val acc 74.17, val corr acc 48.87\n",
      "Ep 3 \titer 402  \tloss 1.08804, train acc 74.48, train corr acc 43.68, val acc 73.92, val corr acc 48.36\n",
      "Ep 3 \titer 403  \tloss 0.98414, train acc 78.65, train corr acc 53.93, val acc 73.85, val corr acc 48.24\n",
      "Ep 3 \titer 404  \tloss 1.12403, train acc 75.00, train corr acc 52.48, val acc 73.54, val corr acc 47.61\n",
      "Ep 3 \titer 405  \tloss 1.14509, train acc 72.92, train corr acc 50.00, val acc 73.98, val corr acc 48.49\n",
      "Ep 3 \titer 406  \tloss 1.11943, train acc 73.96, train corr acc 50.98, val acc 74.24, val corr acc 48.99\n",
      "Ep 3 \titer 407  \tloss 1.12026, train acc 71.88, train corr acc 45.45, val acc 73.73, val corr acc 47.98\n",
      "Ep 3 \titer 408  \tloss 1.07723, train acc 77.08, train corr acc 53.19, val acc 74.55, val corr acc 49.62\n",
      "Ep 3 \titer 409  \tloss 1.05437, train acc 75.00, train corr acc 50.00, val acc 75.06, val corr acc 50.63\n",
      "Ep 3 \titer 410  \tloss 1.20693, train acc 72.92, train corr acc 46.39, val acc 74.94, val corr acc 50.38\n",
      "Ep 3 \titer 411  \tloss 1.13368, train acc 73.96, train corr acc 46.81, val acc 74.30, val corr acc 49.12\n",
      "Ep 3 \titer 412  \tloss 1.14383, train acc 72.92, train corr acc 51.40, val acc 74.17, val corr acc 48.87\n",
      "Ep 3 \titer 413  \tloss 1.03131, train acc 75.52, train corr acc 50.00, val acc 73.66, val corr acc 47.86\n",
      "Ep 3 \titer 414  \tloss 1.23542, train acc 69.27, train corr acc 42.72, val acc 73.92, val corr acc 48.36\n",
      "Ep 3 \titer 415  \tloss 1.15955, train acc 72.40, train corr acc 45.92, val acc 74.62, val corr acc 49.75\n",
      "Ep 3 \titer 416  \tloss 1.08811, train acc 73.44, train corr acc 44.57, val acc 74.68, val corr acc 49.87\n",
      "Ep 3 \titer 417  \tloss 1.25172, train acc 70.83, train corr acc 45.63, val acc 74.55, val corr acc 49.62\n",
      "Ep 3 \titer 418  \tloss 1.17554, train acc 72.92, train corr acc 49.02, val acc 73.41, val corr acc 47.36\n",
      "Ep 3 \titer 419  \tloss 1.08476, train acc 73.96, train corr acc 53.27, val acc 73.98, val corr acc 48.49\n",
      "Ep 3 \titer 420  \tloss 1.16112, train acc 75.52, train corr acc 52.53, val acc 74.36, val corr acc 49.24\n",
      "Ep 3 \titer 421  \tloss 1.14132, train acc 72.92, train corr acc 45.26, val acc 74.94, val corr acc 50.38\n",
      "Ep 3 \titer 422  \tloss 1.05614, train acc 78.12, train corr acc 55.32, val acc 74.75, val corr acc 50.00\n",
      "Ep 3 \titer 423  \tloss 1.20862, train acc 70.31, train corr acc 50.43, val acc 74.49, val corr acc 49.50\n",
      "Ep 3 \titer 424  \tloss 1.15755, train acc 70.83, train corr acc 43.43, val acc 74.68, val corr acc 49.87\n",
      "Ep 3 \titer 425  \tloss 1.10522, train acc 72.92, train corr acc 49.51, val acc 74.17, val corr acc 48.87\n",
      "Ep 3 \titer 426  \tloss 1.18883, train acc 70.31, train corr acc 41.84, val acc 74.55, val corr acc 49.62\n",
      "Ep 3 \titer 427  \tloss 1.07736, train acc 76.04, train corr acc 52.58, val acc 74.94, val corr acc 50.38\n",
      "Ep 3 \titer 428  \tloss 1.18098, train acc 71.88, train corr acc 46.00, val acc 75.45, val corr acc 51.39\n",
      "Ep 3 \titer 429  \tloss 1.24133, train acc 74.48, train corr acc 52.43, val acc 74.62, val corr acc 49.75\n",
      "Ep 3 \titer 430  \tloss 1.10988, train acc 73.44, train corr acc 45.74, val acc 73.73, val corr acc 47.98\n",
      "Ep 3 \titer 431  \tloss 1.19944, train acc 72.40, train corr acc 47.52, val acc 73.60, val corr acc 47.73\n",
      "Ep 3 \titer 432  \tloss 1.08996, train acc 71.35, train corr acc 48.11, val acc 73.98, val corr acc 48.49\n",
      "Ep 3 \titer 433  \tloss 1.08031, train acc 73.44, train corr acc 47.42, val acc 74.05, val corr acc 48.61\n",
      "Ep 3 \titer 434  \tloss 1.19867, train acc 73.44, train corr acc 48.48, val acc 74.36, val corr acc 49.24\n",
      "Ep 3 \titer 435  \tloss 1.21262, train acc 72.92, train corr acc 51.40, val acc 74.81, val corr acc 50.13\n",
      "Ep 3 \titer 436  \tloss 1.02601, train acc 73.96, train corr acc 48.45, val acc 75.51, val corr acc 51.51\n",
      "Ep 3 \titer 437  \tloss 1.08708, train acc 76.04, train corr acc 51.06, val acc 75.13, val corr acc 50.76\n",
      "Ep 3 \titer 438  \tloss 1.19998, train acc 69.79, train corr acc 43.69, val acc 74.55, val corr acc 49.62\n",
      "Ep 3 \titer 439  \tloss 1.05037, train acc 77.08, train corr acc 51.11, val acc 74.30, val corr acc 49.12\n",
      "Ep 3 \titer 440  \tloss 1.05070, train acc 73.44, train corr acc 46.88, val acc 74.36, val corr acc 49.24\n",
      "Ep 3 \titer 441  \tloss 1.14593, train acc 73.96, train corr acc 50.00, val acc 74.55, val corr acc 49.62\n",
      "Ep 3 \titer 442  \tloss 1.12790, train acc 74.48, train corr acc 49.48, val acc 74.68, val corr acc 49.87\n",
      "Ep 3 \titer 443  \tloss 1.04542, train acc 73.44, train corr acc 47.96, val acc 74.55, val corr acc 49.62\n",
      "Ep 3 \titer 444  \tloss 1.28954, train acc 68.23, train corr acc 44.04, val acc 74.17, val corr acc 48.87\n",
      "Ep 3 \titer 445  \tloss 0.96472, train acc 77.60, train corr acc 50.57, val acc 75.06, val corr acc 50.63\n",
      "Ep 3 \titer 446  \tloss 1.09084, train acc 76.56, train corr acc 49.44, val acc 74.62, val corr acc 49.75\n",
      "Ep 3 \titer 447  \tloss 1.07352, train acc 76.56, train corr acc 54.08, val acc 74.62, val corr acc 49.75\n",
      "Ep 3 \titer 448  \tloss 1.13322, train acc 73.96, train corr acc 50.50, val acc 75.13, val corr acc 50.76\n",
      "Ep 3 \titer 449  \tloss 1.02635, train acc 78.65, train corr acc 52.33, val acc 74.75, val corr acc 50.00\n",
      "Ep 3 \titer 450  \tloss 1.07684, train acc 75.52, train corr acc 51.55, val acc 74.87, val corr acc 50.25\n",
      "Ep 3 \titer 451  \tloss 0.88049, train acc 79.69, train corr acc 58.95, val acc 74.62, val corr acc 49.75\n",
      "Ep 3 \titer 452  \tloss 1.16224, train acc 75.00, train corr acc 46.67, val acc 74.68, val corr acc 50.00\n",
      "Ep 3 \titer 453  \tloss 1.01226, train acc 74.48, train corr acc 48.96, val acc 75.06, val corr acc 50.63\n",
      "Ep 3 \titer 454  \tloss 1.11322, train acc 72.92, train corr acc 46.94, val acc 74.87, val corr acc 50.25\n",
      "Ep 3 \titer 455  \tloss 1.19733, train acc 67.71, train corr acc 42.06, val acc 74.75, val corr acc 50.00\n",
      "Ep 3 \titer 456  \tloss 1.09396, train acc 75.52, train corr acc 50.00, val acc 74.68, val corr acc 49.87\n",
      "\n",
      "Epoch 4 of 10 took 96.779s\n",
      "  training loss:\t\t1.150703\n",
      "  training raw accuracy:\t\t73.21 %\n",
      "  training corrected acc:\t\t47.84 %\n",
      "  validation loss:\t\t1.122374\n",
      "  validation raw accuracy:\t\t74.68 %\n",
      "  validation corrected acc:\t\t49.87 % \n",
      "\n",
      "Ep 4 \titer 457  \tloss 1.20324, train acc 70.83, train corr acc 48.15, val acc 75.19, val corr acc 50.88\n",
      "Ep 4 \titer 458  \tloss 1.02561, train acc 76.04, train corr acc 50.00, val acc 74.36, val corr acc 49.24\n",
      "Ep 4 \titer 459  \tloss 1.07038, train acc 73.96, train corr acc 48.98, val acc 74.68, val corr acc 49.87\n",
      "Ep 4 \titer 460  \tloss 1.13107, train acc 72.40, train corr acc 45.92, val acc 74.49, val corr acc 49.50\n",
      "Ep 4 \titer 461  \tloss 1.14139, train acc 73.44, train corr acc 49.00, val acc 74.87, val corr acc 50.25\n",
      "Ep 4 \titer 462  \tloss 1.17965, train acc 72.40, train corr acc 51.38, val acc 75.06, val corr acc 50.63\n",
      "Ep 4 \titer 463  \tloss 1.23503, train acc 71.88, train corr acc 45.45, val acc 74.30, val corr acc 49.12\n",
      "Ep 4 \titer 464  \tloss 1.12452, train acc 75.52, train corr acc 51.55, val acc 74.24, val corr acc 48.99\n",
      "Ep 4 \titer 465  \tloss 1.01878, train acc 74.48, train corr acc 51.96, val acc 74.43, val corr acc 49.37\n",
      "Ep 4 \titer 466  \tloss 1.25774, train acc 70.31, train corr acc 47.22, val acc 73.92, val corr acc 48.36\n",
      "Ep 4 \titer 467  \tloss 1.09325, train acc 70.31, train corr acc 44.66, val acc 73.85, val corr acc 48.24\n",
      "Ep 4 \titer 468  \tloss 1.09350, train acc 75.00, train corr acc 48.94, val acc 74.87, val corr acc 50.25\n",
      "Ep 4 \titer 469  \tloss 1.11805, train acc 72.92, train corr acc 42.22, val acc 74.87, val corr acc 50.25\n",
      "Ep 4 \titer 470  \tloss 1.08089, train acc 73.44, train corr acc 46.88, val acc 73.73, val corr acc 47.98\n",
      "Ep 4 \titer 471  \tloss 1.17200, train acc 72.40, train corr acc 51.82, val acc 73.60, val corr acc 47.73\n",
      "Ep 4 \titer 472  \tloss 1.03575, train acc 75.00, train corr acc 48.94, val acc 73.73, val corr acc 47.98\n",
      "Ep 4 \titer 473  \tloss 1.13489, train acc 69.27, train corr acc 42.72, val acc 74.36, val corr acc 49.24\n",
      "Ep 4 \titer 474  \tloss 1.20865, train acc 70.83, train corr acc 46.15, val acc 74.62, val corr acc 49.75\n",
      "Ep 4 \titer 475  \tloss 1.06783, train acc 72.92, train corr acc 45.83, val acc 74.55, val corr acc 49.62\n",
      "Ep 4 \titer 476  \tloss 1.00664, train acc 77.08, train corr acc 54.17, val acc 74.62, val corr acc 49.75\n",
      "Ep 4 \titer 477  \tloss 0.94329, train acc 78.12, train corr acc 51.16, val acc 75.00, val corr acc 50.50\n",
      "Ep 4 \titer 478  \tloss 1.21940, train acc 70.83, train corr acc 46.15, val acc 74.81, val corr acc 50.13\n",
      "Ep 4 \titer 479  \tloss 0.96395, train acc 75.52, train corr acc 44.71, val acc 74.49, val corr acc 49.50\n",
      "Ep 4 \titer 480  \tloss 1.07650, train acc 75.52, train corr acc 53.47, val acc 73.47, val corr acc 47.48\n",
      "Ep 4 \titer 481  \tloss 1.05967, train acc 75.00, train corr acc 50.52, val acc 73.22, val corr acc 46.98\n",
      "Ep 4 \titer 482  \tloss 1.09420, train acc 75.52, train corr acc 52.04, val acc 74.30, val corr acc 49.12\n",
      "Ep 4 \titer 483  \tloss 1.17803, train acc 72.92, train corr acc 49.51, val acc 74.11, val corr acc 48.74\n",
      "Ep 4 \titer 484  \tloss 1.08842, train acc 72.92, train corr acc 47.47, val acc 74.17, val corr acc 48.87\n",
      "Ep 4 \titer 485  \tloss 1.13911, train acc 72.40, train corr acc 46.46, val acc 73.92, val corr acc 48.36\n",
      "Ep 4 \titer 486  \tloss 1.24963, train acc 73.96, train corr acc 50.98, val acc 73.79, val corr acc 48.11\n",
      "Ep 4 \titer 487  \tloss 1.09812, train acc 73.96, train corr acc 47.92, val acc 73.85, val corr acc 48.24\n",
      "Ep 4 \titer 488  \tloss 1.04676, train acc 77.60, train corr acc 53.76, val acc 73.85, val corr acc 48.24\n",
      "Ep 4 \titer 489  \tloss 0.92601, train acc 78.12, train corr acc 54.35, val acc 74.49, val corr acc 49.50\n",
      "Ep 4 \titer 490  \tloss 1.03433, train acc 74.48, train corr acc 50.00, val acc 74.75, val corr acc 50.00\n",
      "Ep 4 \titer 491  \tloss 1.14530, train acc 73.96, train corr acc 53.27, val acc 74.49, val corr acc 49.50\n",
      "Ep 4 \titer 492  \tloss 1.15118, train acc 70.31, train corr acc 44.66, val acc 73.98, val corr acc 48.49\n",
      "Ep 4 \titer 493  \tloss 1.15601, train acc 76.04, train corr acc 55.34, val acc 74.24, val corr acc 48.99\n",
      "Ep 4 \titer 494  \tloss 1.30658, train acc 71.88, train corr acc 47.06, val acc 74.24, val corr acc 48.99\n",
      "Ep 4 \titer 495  \tloss 1.01557, train acc 79.17, train corr acc 55.06, val acc 74.05, val corr acc 48.61\n",
      "Ep 4 \titer 496  \tloss 1.13950, train acc 71.88, train corr acc 46.00, val acc 74.43, val corr acc 49.37\n",
      "Ep 4 \titer 497  \tloss 1.18862, train acc 71.88, train corr acc 47.06, val acc 74.62, val corr acc 49.75\n",
      "Ep 4 \titer 498  \tloss 1.31027, train acc 70.83, train corr acc 47.17, val acc 74.81, val corr acc 50.13\n",
      "Ep 4 \titer 499  \tloss 1.12211, train acc 75.52, train corr acc 51.55, val acc 74.87, val corr acc 50.25\n",
      "Ep 4 \titer 500  \tloss 1.14064, train acc 75.52, train corr acc 53.47, val acc 73.73, val corr acc 47.98\n",
      "Ep 4 \titer 501  \tloss 1.15383, train acc 72.40, train corr acc 48.54, val acc 74.24, val corr acc 48.99\n",
      "Ep 4 \titer 502  \tloss 1.12880, train acc 72.92, train corr acc 49.51, val acc 74.24, val corr acc 48.99\n",
      "Ep 4 \titer 503  \tloss 1.01303, train acc 74.48, train corr acc 48.96, val acc 73.73, val corr acc 47.98\n",
      "Ep 4 \titer 504  \tloss 1.20016, train acc 71.35, train corr acc 47.12, val acc 73.79, val corr acc 48.11\n",
      "Ep 4 \titer 505  \tloss 1.13424, train acc 71.35, train corr acc 43.30, val acc 74.49, val corr acc 49.50\n",
      "Ep 4 \titer 506  \tloss 1.04976, train acc 75.52, train corr acc 52.53, val acc 74.17, val corr acc 48.87\n",
      "Ep 4 \titer 507  \tloss 1.19631, train acc 70.83, train corr acc 46.15, val acc 75.00, val corr acc 50.50\n",
      "Ep 4 \titer 508  \tloss 1.09714, train acc 73.44, train corr acc 53.64, val acc 74.68, val corr acc 49.87\n",
      "Ep 4 \titer 509  \tloss 1.25053, train acc 71.35, train corr acc 43.30, val acc 74.62, val corr acc 49.75\n",
      "Ep 4 \titer 510  \tloss 0.97789, train acc 77.60, train corr acc 53.26, val acc 74.11, val corr acc 48.74\n",
      "Ep 4 \titer 511  \tloss 1.10375, train acc 74.48, train corr acc 50.51, val acc 74.81, val corr acc 50.13\n",
      "Ep 4 \titer 512  \tloss 1.02685, train acc 75.00, train corr acc 48.94, val acc 74.49, val corr acc 49.50\n",
      "Ep 4 \titer 513  \tloss 1.17132, train acc 71.35, train corr acc 42.11, val acc 74.17, val corr acc 48.87\n",
      "Ep 4 \titer 514  \tloss 1.05293, train acc 72.40, train corr acc 46.46, val acc 74.36, val corr acc 49.24\n",
      "Ep 4 \titer 515  \tloss 0.97797, train acc 77.60, train corr acc 52.75, val acc 74.94, val corr acc 50.38\n",
      "Ep 4 \titer 516  \tloss 1.01022, train acc 76.56, train corr acc 48.28, val acc 75.00, val corr acc 50.50\n",
      "Ep 4 \titer 517  \tloss 0.95684, train acc 78.65, train corr acc 53.93, val acc 74.75, val corr acc 50.00\n",
      "Ep 4 \titer 518  \tloss 1.05891, train acc 76.04, train corr acc 54.46, val acc 75.00, val corr acc 50.50\n",
      "Ep 4 \titer 519  \tloss 1.08514, train acc 71.88, train corr acc 48.08, val acc 73.66, val corr acc 47.86\n",
      "Ep 4 \titer 520  \tloss 1.01618, train acc 78.12, train corr acc 58.82, val acc 73.60, val corr acc 47.73\n",
      "Ep 4 \titer 521  \tloss 1.04077, train acc 72.40, train corr acc 46.46, val acc 73.41, val corr acc 47.36\n",
      "Ep 4 \titer 522  \tloss 1.04291, train acc 73.96, train corr acc 46.81, val acc 73.28, val corr acc 47.10\n",
      "Ep 4 \titer 523  \tloss 1.01636, train acc 78.12, train corr acc 56.25, val acc 74.75, val corr acc 50.00\n",
      "Ep 4 \titer 524  \tloss 1.11766, train acc 75.52, train corr acc 51.55, val acc 74.81, val corr acc 50.13\n",
      "Ep 4 \titer 525  \tloss 1.09065, train acc 73.44, train corr acc 45.74, val acc 75.25, val corr acc 51.01\n",
      "Ep 4 \titer 526  \tloss 1.09986, train acc 74.48, train corr acc 54.21, val acc 75.38, val corr acc 51.26\n",
      "Ep 4 \titer 527  \tloss 0.99375, train acc 76.56, train corr acc 52.13, val acc 74.94, val corr acc 50.38\n",
      "Ep 4 \titer 528  \tloss 1.11289, train acc 70.83, train corr acc 45.63, val acc 74.36, val corr acc 49.24\n",
      "Ep 4 \titer 529  \tloss 1.11406, train acc 71.88, train corr acc 44.90, val acc 74.55, val corr acc 49.62\n",
      "Ep 4 \titer 530  \tloss 1.03571, train acc 74.48, train corr acc 46.74, val acc 74.81, val corr acc 50.13\n",
      "Ep 4 \titer 531  \tloss 1.11272, train acc 72.92, train corr acc 49.51, val acc 74.81, val corr acc 50.13\n",
      "Ep 4 \titer 532  \tloss 1.12638, train acc 72.40, train corr acc 48.04, val acc 74.11, val corr acc 48.74\n",
      "Ep 4 \titer 533  \tloss 1.04023, train acc 74.48, train corr acc 54.21, val acc 74.62, val corr acc 49.75\n",
      "Ep 4 \titer 534  \tloss 1.10008, train acc 74.48, train corr acc 50.51, val acc 74.43, val corr acc 49.37\n",
      "Ep 4 \titer 535  \tloss 1.09454, train acc 75.52, train corr acc 50.53, val acc 74.11, val corr acc 48.74\n",
      "Ep 4 \titer 536  \tloss 0.97151, train acc 76.04, train corr acc 51.06, val acc 74.68, val corr acc 49.87\n",
      "Ep 4 \titer 537  \tloss 1.07121, train acc 72.40, train corr acc 53.91, val acc 74.55, val corr acc 49.62\n",
      "Ep 4 \titer 538  \tloss 1.09888, train acc 70.83, train corr acc 43.43, val acc 74.55, val corr acc 49.62\n",
      "Ep 4 \titer 539  \tloss 1.03024, train acc 74.48, train corr acc 52.43, val acc 74.81, val corr acc 50.13\n",
      "Ep 4 \titer 540  \tloss 1.17454, train acc 71.35, train corr acc 43.88, val acc 74.68, val corr acc 49.87\n",
      "Ep 4 \titer 541  \tloss 1.00144, train acc 75.00, train corr acc 50.52, val acc 74.30, val corr acc 49.12\n",
      "Ep 4 \titer 542  \tloss 1.17650, train acc 71.35, train corr acc 45.00, val acc 74.49, val corr acc 49.50\n",
      "Ep 4 \titer 543  \tloss 1.15740, train acc 73.44, train corr acc 50.49, val acc 74.75, val corr acc 50.00\n",
      "Ep 4 \titer 544  \tloss 1.04515, train acc 76.04, train corr acc 51.06, val acc 75.06, val corr acc 50.63\n",
      "Ep 4 \titer 545  \tloss 1.08874, train acc 73.44, train corr acc 49.50, val acc 75.25, val corr acc 51.01\n",
      "Ep 4 \titer 546  \tloss 1.06235, train acc 73.44, train corr acc 51.89, val acc 75.19, val corr acc 50.88\n",
      "Ep 4 \titer 547  \tloss 0.98214, train acc 75.00, train corr acc 50.52, val acc 74.81, val corr acc 50.13\n",
      "Ep 4 \titer 548  \tloss 1.05667, train acc 73.96, train corr acc 49.49, val acc 74.81, val corr acc 50.13\n",
      "Ep 4 \titer 549  \tloss 1.15094, train acc 73.96, train corr acc 53.27, val acc 74.43, val corr acc 49.37\n",
      "Ep 4 \titer 550  \tloss 0.94620, train acc 73.96, train corr acc 48.45, val acc 75.57, val corr acc 51.64\n",
      "Ep 4 \titer 551  \tloss 1.01826, train acc 76.56, train corr acc 52.13, val acc 75.32, val corr acc 51.13\n",
      "Ep 4 \titer 552  \tloss 1.22824, train acc 71.35, train corr acc 46.60, val acc 75.19, val corr acc 50.88\n",
      "Ep 4 \titer 553  \tloss 0.99353, train acc 76.04, train corr acc 48.89, val acc 74.55, val corr acc 49.62\n",
      "Ep 4 \titer 554  \tloss 0.96660, train acc 75.52, train corr acc 51.04, val acc 75.06, val corr acc 50.63\n",
      "Ep 4 \titer 555  \tloss 1.10313, train acc 71.88, train corr acc 46.00, val acc 75.13, val corr acc 50.76\n",
      "Ep 4 \titer 556  \tloss 1.07473, train acc 74.48, train corr acc 49.48, val acc 75.25, val corr acc 51.01\n",
      "Ep 4 \titer 557  \tloss 1.03884, train acc 75.00, train corr acc 51.02, val acc 74.68, val corr acc 49.87\n",
      "Ep 4 \titer 558  \tloss 1.12173, train acc 70.83, train corr acc 48.62, val acc 74.81, val corr acc 50.13\n",
      "Ep 4 \titer 559  \tloss 0.87142, train acc 76.56, train corr acc 48.28, val acc 75.00, val corr acc 50.50\n",
      "Ep 4 \titer 560  \tloss 0.99995, train acc 77.08, train corr acc 50.56, val acc 74.94, val corr acc 50.38\n",
      "Ep 4 \titer 561  \tloss 1.02472, train acc 74.48, train corr acc 50.00, val acc 75.45, val corr acc 51.39\n",
      "Ep 4 \titer 562  \tloss 1.04545, train acc 74.48, train corr acc 51.49, val acc 75.32, val corr acc 51.13\n",
      "Ep 4 \titer 563  \tloss 0.90509, train acc 79.69, train corr acc 54.65, val acc 74.81, val corr acc 50.13\n",
      "Ep 4 \titer 564  \tloss 1.02532, train acc 77.08, train corr acc 54.64, val acc 74.87, val corr acc 50.25\n",
      "Ep 4 \titer 565  \tloss 0.84359, train acc 80.73, train corr acc 61.05, val acc 74.75, val corr acc 50.00\n",
      "Ep 4 \titer 566  \tloss 1.05370, train acc 76.56, train corr acc 50.00, val acc 75.00, val corr acc 50.50\n",
      "Ep 4 \titer 567  \tloss 0.93698, train acc 75.52, train corr acc 51.04, val acc 75.25, val corr acc 51.01\n",
      "Ep 4 \titer 568  \tloss 1.01160, train acc 74.48, train corr acc 50.00, val acc 75.19, val corr acc 50.88\n",
      "Ep 4 \titer 569  \tloss 1.15247, train acc 68.23, train corr acc 42.99, val acc 75.57, val corr acc 51.64\n",
      "Ep 4 \titer 570  \tloss 1.04590, train acc 76.04, train corr acc 51.06, val acc 75.13, val corr acc 50.76\n",
      "\n",
      "Epoch 5 of 10 took 68.762s\n",
      "  training loss:\t\t1.083049\n",
      "  training raw accuracy:\t\t74.09 %\n",
      "  training corrected acc:\t\t49.56 %\n",
      "  validation loss:\t\t1.124348\n",
      "  validation raw accuracy:\t\t75.00 %\n",
      "  validation corrected acc:\t\t50.50 % \n",
      "\n",
      "Ep 5 \titer 571  \tloss 1.17266, train acc 72.92, train corr acc 51.85, val acc 75.00, val corr acc 50.50\n",
      "Ep 5 \titer 572  \tloss 0.98027, train acc 77.60, train corr acc 53.26, val acc 73.85, val corr acc 48.24\n",
      "Ep 5 \titer 573  \tloss 1.02382, train acc 74.48, train corr acc 50.00, val acc 74.87, val corr acc 50.25\n",
      "Ep 5 \titer 574  \tloss 1.10761, train acc 71.88, train corr acc 44.90, val acc 74.55, val corr acc 49.62\n",
      "Ep 5 \titer 575  \tloss 1.13295, train acc 73.44, train corr acc 49.00, val acc 74.68, val corr acc 49.87\n",
      "Ep 5 \titer 576  \tloss 1.17113, train acc 72.40, train corr acc 51.38, val acc 74.11, val corr acc 48.74\n",
      "Ep 5 \titer 577  \tloss 1.15523, train acc 71.88, train corr acc 45.45, val acc 74.05, val corr acc 48.61\n",
      "Ep 5 \titer 578  \tloss 1.11330, train acc 72.92, train corr acc 46.39, val acc 73.98, val corr acc 48.49\n",
      "Ep 5 \titer 579  \tloss 1.02772, train acc 72.92, train corr acc 49.02, val acc 74.49, val corr acc 49.50\n",
      "Ep 5 \titer 580  \tloss 1.23937, train acc 71.35, train corr acc 49.07, val acc 74.43, val corr acc 49.37\n",
      "Ep 5 \titer 581  \tloss 1.05475, train acc 74.48, train corr acc 52.43, val acc 75.00, val corr acc 50.50\n",
      "Ep 5 \titer 582  \tloss 1.03898, train acc 75.52, train corr acc 50.00, val acc 74.81, val corr acc 50.13\n",
      "Ep 5 \titer 583  \tloss 1.01885, train acc 75.00, train corr acc 46.67, val acc 75.00, val corr acc 50.50\n",
      "Ep 5 \titer 584  \tloss 1.06522, train acc 75.00, train corr acc 50.00, val acc 75.32, val corr acc 51.13\n",
      "Ep 5 \titer 585  \tloss 1.17345, train acc 71.88, train corr acc 50.91, val acc 75.38, val corr acc 51.26\n",
      "Ep 5 \titer 586  \tloss 0.94608, train acc 76.56, train corr acc 52.13, val acc 75.76, val corr acc 52.02\n",
      "Ep 5 \titer 587  \tloss 1.04883, train acc 72.92, train corr acc 49.51, val acc 75.25, val corr acc 51.01\n",
      "Ep 5 \titer 588  \tloss 1.08462, train acc 73.44, train corr acc 50.96, val acc 75.06, val corr acc 50.63\n",
      "Ep 5 \titer 589  \tloss 1.03148, train acc 73.44, train corr acc 46.88, val acc 75.19, val corr acc 50.88\n",
      "Ep 5 \titer 590  \tloss 0.94136, train acc 77.60, train corr acc 55.21, val acc 74.43, val corr acc 49.37\n",
      "Ep 5 \titer 591  \tloss 0.90089, train acc 79.17, train corr acc 53.49, val acc 74.68, val corr acc 49.87\n",
      "Ep 5 \titer 592  \tloss 1.13117, train acc 71.35, train corr acc 47.12, val acc 74.62, val corr acc 49.75\n",
      "Ep 5 \titer 593  \tloss 0.90375, train acc 79.69, train corr acc 54.12, val acc 74.81, val corr acc 50.13\n",
      "Ep 5 \titer 594  \tloss 1.02964, train acc 73.96, train corr acc 50.50, val acc 74.62, val corr acc 49.75\n",
      "Ep 5 \titer 595  \tloss 1.03408, train acc 73.44, train corr acc 47.42, val acc 74.55, val corr acc 49.62\n",
      "Ep 5 \titer 596  \tloss 1.07647, train acc 76.04, train corr acc 53.06, val acc 74.43, val corr acc 49.37\n",
      "Ep 5 \titer 597  \tloss 1.11262, train acc 72.92, train corr acc 49.51, val acc 74.17, val corr acc 48.87\n",
      "Ep 5 \titer 598  \tloss 1.06907, train acc 73.44, train corr acc 48.48, val acc 74.36, val corr acc 49.24\n",
      "Ep 5 \titer 599  \tloss 1.11594, train acc 73.96, train corr acc 49.49, val acc 74.05, val corr acc 48.61\n",
      "Ep 5 \titer 600  \tloss 1.19813, train acc 71.88, train corr acc 47.06, val acc 74.81, val corr acc 50.13\n",
      "Ep 5 \titer 601  \tloss 1.03289, train acc 72.92, train corr acc 45.83, val acc 74.87, val corr acc 50.25\n",
      "Ep 5 \titer 602  \tloss 0.95779, train acc 78.12, train corr acc 54.84, val acc 74.87, val corr acc 50.25\n",
      "Ep 5 \titer 603  \tloss 0.89220, train acc 77.60, train corr acc 53.26, val acc 75.13, val corr acc 50.76\n",
      "Ep 5 \titer 604  \tloss 0.99680, train acc 74.48, train corr acc 50.00, val acc 75.51, val corr acc 51.51\n",
      "Ep 5 \titer 605  \tloss 1.01872, train acc 74.48, train corr acc 54.21, val acc 75.13, val corr acc 50.76\n",
      "Ep 5 \titer 606  \tloss 1.07957, train acc 73.44, train corr acc 50.49, val acc 75.64, val corr acc 51.76\n",
      "Ep 5 \titer 607  \tloss 1.07898, train acc 75.52, train corr acc 54.37, val acc 75.45, val corr acc 51.39\n",
      "Ep 5 \titer 608  \tloss 1.16161, train acc 72.92, train corr acc 49.02, val acc 75.38, val corr acc 51.26\n",
      "Ep 5 \titer 609  \tloss 0.90049, train acc 78.12, train corr acc 52.81, val acc 75.51, val corr acc 51.51\n",
      "Ep 5 \titer 610  \tloss 1.07323, train acc 71.35, train corr acc 45.00, val acc 75.64, val corr acc 51.76\n",
      "Ep 5 \titer 611  \tloss 1.01192, train acc 72.92, train corr acc 49.02, val acc 75.45, val corr acc 51.39\n",
      "Ep 5 \titer 612  \tloss 1.15572, train acc 73.44, train corr acc 51.89, val acc 75.19, val corr acc 50.88\n",
      "Ep 5 \titer 613  \tloss 0.99864, train acc 73.44, train corr acc 47.42, val acc 75.89, val corr acc 52.27\n",
      "Ep 5 \titer 614  \tloss 1.06858, train acc 75.00, train corr acc 52.48, val acc 75.32, val corr acc 51.13\n",
      "Ep 5 \titer 615  \tloss 1.02772, train acc 76.56, train corr acc 56.31, val acc 75.45, val corr acc 51.39\n",
      "Ep 5 \titer 616  \tloss 1.01239, train acc 73.96, train corr acc 51.46, val acc 75.51, val corr acc 51.51\n",
      "Ep 5 \titer 617  \tloss 0.90039, train acc 78.12, train corr acc 56.25, val acc 75.00, val corr acc 50.50\n",
      "Ep 5 \titer 618  \tloss 1.03039, train acc 73.96, train corr acc 51.92, val acc 75.45, val corr acc 51.39\n",
      "Ep 5 \titer 619  \tloss 1.10459, train acc 72.92, train corr acc 46.39, val acc 75.32, val corr acc 51.13\n",
      "Ep 5 \titer 620  \tloss 0.97011, train acc 77.08, train corr acc 55.56, val acc 75.70, val corr acc 51.89\n",
      "Ep 5 \titer 621  \tloss 1.12900, train acc 72.40, train corr acc 49.04, val acc 75.19, val corr acc 50.88\n",
      "Ep 5 \titer 622  \tloss 1.01189, train acc 73.44, train corr acc 53.64, val acc 74.81, val corr acc 50.13\n",
      "Ep 5 \titer 623  \tloss 1.14016, train acc 73.44, train corr acc 47.42, val acc 75.70, val corr acc 51.89\n",
      "Ep 5 \titer 624  \tloss 0.91701, train acc 78.12, train corr acc 54.35, val acc 75.51, val corr acc 51.51\n",
      "Ep 5 \titer 625  \tloss 0.98793, train acc 76.56, train corr acc 54.55, val acc 75.38, val corr acc 51.26\n",
      "Ep 5 \titer 626  \tloss 0.92964, train acc 75.00, train corr acc 48.94, val acc 75.83, val corr acc 52.14\n",
      "Ep 5 \titer 627  \tloss 1.08180, train acc 71.88, train corr acc 43.16, val acc 75.45, val corr acc 51.39\n",
      "Ep 5 \titer 628  \tloss 0.98563, train acc 75.52, train corr acc 52.53, val acc 74.87, val corr acc 50.25\n",
      "Ep 5 \titer 629  \tloss 0.84733, train acc 78.12, train corr acc 53.85, val acc 75.19, val corr acc 50.88\n",
      "Ep 5 \titer 630  \tloss 0.91180, train acc 76.56, train corr acc 48.28, val acc 75.06, val corr acc 50.63\n",
      "Ep 5 \titer 631  \tloss 0.83394, train acc 79.69, train corr acc 56.18, val acc 74.17, val corr acc 48.87\n",
      "Ep 5 \titer 632  \tloss 0.96483, train acc 76.56, train corr acc 55.45, val acc 73.98, val corr acc 48.49\n",
      "Ep 5 \titer 633  \tloss 1.04546, train acc 76.04, train corr acc 55.77, val acc 75.13, val corr acc 50.76\n",
      "Ep 5 \titer 634  \tloss 0.90130, train acc 78.65, train corr acc 59.80, val acc 74.36, val corr acc 49.24\n",
      "Ep 5 \titer 635  \tloss 0.95527, train acc 75.52, train corr acc 52.53, val acc 75.32, val corr acc 51.13\n",
      "Ep 5 \titer 636  \tloss 0.88720, train acc 76.56, train corr acc 52.13, val acc 75.32, val corr acc 51.13\n",
      "Ep 5 \titer 637  \tloss 0.94952, train acc 78.12, train corr acc 56.25, val acc 75.57, val corr acc 51.64\n",
      "Ep 5 \titer 638  \tloss 1.05065, train acc 76.04, train corr acc 52.58, val acc 75.83, val corr acc 52.14\n",
      "Ep 5 \titer 639  \tloss 1.02726, train acc 73.96, train corr acc 46.81, val acc 75.51, val corr acc 51.51\n",
      "Ep 5 \titer 640  \tloss 0.93908, train acc 74.48, train corr acc 54.21, val acc 75.83, val corr acc 52.14\n",
      "Ep 5 \titer 641  \tloss 0.94239, train acc 76.04, train corr acc 51.06, val acc 75.76, val corr acc 52.02\n",
      "Ep 5 \titer 642  \tloss 1.05808, train acc 72.92, train corr acc 49.51, val acc 75.45, val corr acc 51.39\n",
      "Ep 5 \titer 643  \tloss 1.00566, train acc 78.12, train corr acc 57.14, val acc 76.08, val corr acc 52.64\n",
      "Ep 5 \titer 644  \tloss 0.94370, train acc 76.56, train corr acc 51.09, val acc 76.21, val corr acc 52.90\n",
      "Ep 5 \titer 645  \tloss 1.06666, train acc 72.40, train corr acc 48.54, val acc 75.51, val corr acc 51.51\n",
      "Ep 5 \titer 646  \tloss 1.05046, train acc 74.48, train corr acc 51.96, val acc 75.25, val corr acc 51.01\n",
      "Ep 5 \titer 647  \tloss 0.97721, train acc 75.52, train corr acc 56.07, val acc 75.89, val corr acc 52.27\n",
      "Ep 5 \titer 648  \tloss 1.02306, train acc 75.52, train corr acc 52.53, val acc 76.08, val corr acc 52.64\n",
      "Ep 5 \titer 649  \tloss 0.95953, train acc 76.56, train corr acc 52.63, val acc 75.25, val corr acc 51.01\n",
      "Ep 5 \titer 650  \tloss 0.89105, train acc 76.56, train corr acc 52.13, val acc 75.57, val corr acc 51.64\n",
      "Ep 5 \titer 651  \tloss 1.09796, train acc 71.35, train corr acc 52.17, val acc 75.51, val corr acc 51.51\n",
      "Ep 5 \titer 652  \tloss 1.04438, train acc 71.88, train corr acc 45.45, val acc 75.51, val corr acc 51.51\n",
      "Ep 5 \titer 653  \tloss 0.96271, train acc 76.56, train corr acc 56.31, val acc 74.81, val corr acc 50.13\n",
      "Ep 5 \titer 654  \tloss 1.09724, train acc 72.40, train corr acc 45.92, val acc 75.70, val corr acc 51.89\n",
      "Ep 5 \titer 655  \tloss 0.92272, train acc 75.52, train corr acc 51.55, val acc 75.32, val corr acc 51.13\n",
      "Ep 5 \titer 656  \tloss 1.06094, train acc 70.31, train corr acc 43.00, val acc 75.19, val corr acc 50.88\n",
      "Ep 5 \titer 657  \tloss 1.05841, train acc 75.00, train corr acc 53.40, val acc 75.51, val corr acc 51.51\n",
      "Ep 5 \titer 658  \tloss 0.95437, train acc 76.56, train corr acc 52.13, val acc 75.13, val corr acc 50.76\n",
      "Ep 5 \titer 659  \tloss 0.97082, train acc 73.44, train corr acc 49.50, val acc 75.45, val corr acc 51.39\n",
      "Ep 5 \titer 660  \tloss 0.98189, train acc 73.96, train corr acc 52.83, val acc 75.51, val corr acc 51.51\n",
      "Ep 5 \titer 661  \tloss 0.91166, train acc 76.56, train corr acc 53.61, val acc 75.76, val corr acc 52.02\n",
      "Ep 5 \titer 662  \tloss 1.05741, train acc 72.40, train corr acc 46.46, val acc 75.76, val corr acc 52.02\n",
      "Ep 5 \titer 663  \tloss 1.03455, train acc 75.00, train corr acc 55.14, val acc 75.70, val corr acc 51.89\n",
      "Ep 5 \titer 664  \tloss 0.96061, train acc 76.56, train corr acc 53.61, val acc 76.21, val corr acc 52.90\n",
      "Ep 5 \titer 665  \tloss 0.93711, train acc 77.08, train corr acc 53.19, val acc 75.64, val corr acc 51.76\n",
      "Ep 5 \titer 666  \tloss 1.09069, train acc 72.40, train corr acc 48.54, val acc 75.57, val corr acc 51.64\n",
      "Ep 5 \titer 667  \tloss 0.88569, train acc 80.21, train corr acc 57.78, val acc 76.21, val corr acc 52.90\n",
      "Ep 5 \titer 668  \tloss 0.86163, train acc 76.56, train corr acc 53.12, val acc 75.70, val corr acc 51.89\n",
      "Ep 5 \titer 669  \tloss 1.06090, train acc 73.44, train corr acc 49.00, val acc 75.13, val corr acc 50.76\n",
      "Ep 5 \titer 670  \tloss 0.97689, train acc 75.52, train corr acc 51.55, val acc 75.57, val corr acc 51.64\n",
      "Ep 5 \titer 671  \tloss 0.94744, train acc 75.52, train corr acc 52.04, val acc 74.68, val corr acc 49.87\n",
      "Ep 5 \titer 672  \tloss 1.14323, train acc 68.75, train corr acc 44.95, val acc 75.70, val corr acc 51.89\n",
      "Ep 5 \titer 673  \tloss 0.87931, train acc 77.60, train corr acc 50.57, val acc 74.94, val corr acc 50.38\n",
      "Ep 5 \titer 674  \tloss 0.94926, train acc 77.60, train corr acc 51.69, val acc 74.81, val corr acc 50.13\n",
      "Ep 5 \titer 675  \tloss 0.91983, train acc 77.08, train corr acc 55.10, val acc 74.62, val corr acc 49.75\n",
      "Ep 5 \titer 676  \tloss 0.94357, train acc 76.56, train corr acc 55.45, val acc 75.25, val corr acc 51.01\n",
      "Ep 5 \titer 677  \tloss 0.81284, train acc 79.69, train corr acc 54.65, val acc 75.51, val corr acc 51.51\n",
      "Ep 5 \titer 678  \tloss 0.94768, train acc 77.60, train corr acc 55.67, val acc 75.06, val corr acc 50.63\n",
      "Ep 5 \titer 679  \tloss 0.76746, train acc 81.77, train corr acc 63.16, val acc 75.38, val corr acc 51.26\n",
      "Ep 5 \titer 680  \tloss 0.92675, train acc 77.60, train corr acc 52.22, val acc 75.13, val corr acc 50.76\n",
      "Ep 5 \titer 681  \tloss 0.87413, train acc 75.00, train corr acc 50.00, val acc 74.62, val corr acc 49.75\n",
      "Ep 5 \titer 682  \tloss 0.91309, train acc 76.04, train corr acc 53.06, val acc 74.75, val corr acc 50.00\n",
      "Ep 5 \titer 683  \tloss 1.09491, train acc 68.75, train corr acc 43.93, val acc 74.81, val corr acc 50.13\n",
      "Ep 5 \titer 684  \tloss 0.92959, train acc 77.60, train corr acc 54.26, val acc 75.13, val corr acc 50.76\n",
      "\n",
      "Epoch 6 of 10 took 69.046s\n",
      "  training loss:\t\t1.008459\n",
      "  training raw accuracy:\t\t75.01 %\n",
      "  training corrected acc:\t\t51.36 %\n",
      "  validation loss:\t\t1.106590\n",
      "  validation raw accuracy:\t\t75.89 %\n",
      "  validation corrected acc:\t\t52.27 % \n",
      "\n",
      "Ep 6 \titer 685  \tloss 1.06722, train acc 73.44, train corr acc 52.78, val acc 75.00, val corr acc 50.50\n",
      "Ep 6 \titer 686  \tloss 0.92618, train acc 76.56, train corr acc 51.09, val acc 75.76, val corr acc 52.02\n",
      "Ep 6 \titer 687  \tloss 1.00038, train acc 74.48, train corr acc 50.00, val acc 75.51, val corr acc 51.51\n",
      "Ep 6 \titer 688  \tloss 1.11434, train acc 71.88, train corr acc 44.90, val acc 75.45, val corr acc 51.39\n",
      "Ep 6 \titer 689  \tloss 1.07474, train acc 72.92, train corr acc 48.00, val acc 74.87, val corr acc 50.25\n",
      "Ep 6 \titer 690  \tloss 1.01321, train acc 73.96, train corr acc 54.13, val acc 75.00, val corr acc 50.50\n",
      "Ep 6 \titer 691  \tloss 1.09818, train acc 75.52, train corr acc 52.53, val acc 75.38, val corr acc 51.26\n",
      "Ep 6 \titer 692  \tloss 1.03688, train acc 75.52, train corr acc 51.55, val acc 74.81, val corr acc 50.13\n",
      "Ep 6 \titer 693  \tloss 0.94786, train acc 75.00, train corr acc 52.94, val acc 74.94, val corr acc 50.38\n",
      "Ep 6 \titer 694  \tloss 1.18238, train acc 71.88, train corr acc 50.00, val acc 75.83, val corr acc 52.14\n",
      "Ep 6 \titer 695  \tloss 0.96393, train acc 75.00, train corr acc 53.40, val acc 75.13, val corr acc 50.76\n",
      "Ep 6 \titer 696  \tloss 0.94050, train acc 76.04, train corr acc 51.06, val acc 75.38, val corr acc 51.26\n",
      "Ep 6 \titer 697  \tloss 0.91111, train acc 79.17, train corr acc 55.56, val acc 75.70, val corr acc 51.89\n",
      "Ep 6 \titer 698  \tloss 1.02754, train acc 76.04, train corr acc 52.08, val acc 75.51, val corr acc 51.51\n",
      "Ep 6 \titer 699  \tloss 1.09102, train acc 74.48, train corr acc 55.45, val acc 75.19, val corr acc 50.88\n",
      "Ep 6 \titer 700  \tloss 0.88365, train acc 78.65, train corr acc 56.38, val acc 75.19, val corr acc 50.88\n",
      "Ep 6 \titer 701  \tloss 0.96822, train acc 75.52, train corr acc 54.37, val acc 75.64, val corr acc 51.76\n",
      "Ep 6 \titer 702  \tloss 0.95089, train acc 74.48, train corr acc 52.88, val acc 75.64, val corr acc 51.76\n",
      "Ep 6 \titer 703  \tloss 1.01706, train acc 73.44, train corr acc 46.88, val acc 75.76, val corr acc 52.02\n",
      "Ep 6 \titer 704  \tloss 0.83803, train acc 78.65, train corr acc 57.29, val acc 75.51, val corr acc 51.51\n",
      "Ep 6 \titer 705  \tloss 0.85182, train acc 78.65, train corr acc 52.33, val acc 75.45, val corr acc 51.39\n",
      "Ep 6 \titer 706  \tloss 1.02742, train acc 72.92, train corr acc 50.00, val acc 75.25, val corr acc 51.01\n",
      "Ep 6 \titer 707  \tloss 0.84072, train acc 78.65, train corr acc 51.76, val acc 75.57, val corr acc 51.64\n",
      "Ep 6 \titer 708  \tloss 0.93412, train acc 76.04, train corr acc 54.46, val acc 75.45, val corr acc 51.39\n",
      "Ep 6 \titer 709  \tloss 0.96026, train acc 77.60, train corr acc 55.67, val acc 75.70, val corr acc 51.89\n",
      "Ep 6 \titer 710  \tloss 0.92342, train acc 77.60, train corr acc 56.12, val acc 75.64, val corr acc 51.76\n",
      "Ep 6 \titer 711  \tloss 1.08744, train acc 71.88, train corr acc 47.57, val acc 75.51, val corr acc 51.51\n",
      "Ep 6 \titer 712  \tloss 0.95770, train acc 75.52, train corr acc 52.53, val acc 75.89, val corr acc 52.27\n",
      "Ep 6 \titer 713  \tloss 1.03687, train acc 74.48, train corr acc 50.51, val acc 76.02, val corr acc 52.52\n",
      "Ep 6 \titer 714  \tloss 1.08013, train acc 73.96, train corr acc 50.98, val acc 75.25, val corr acc 51.01\n",
      "Ep 6 \titer 715  \tloss 1.02308, train acc 74.48, train corr acc 48.96, val acc 75.64, val corr acc 51.76\n",
      "Ep 6 \titer 716  \tloss 0.88104, train acc 78.12, train corr acc 54.84, val acc 74.81, val corr acc 50.13\n",
      "Ep 6 \titer 717  \tloss 0.80510, train acc 79.17, train corr acc 56.52, val acc 75.51, val corr acc 51.51\n",
      "Ep 6 \titer 718  \tloss 0.91869, train acc 76.56, train corr acc 54.08, val acc 75.51, val corr acc 51.51\n",
      "Ep 6 \titer 719  \tloss 0.97029, train acc 76.04, train corr acc 57.01, val acc 75.45, val corr acc 51.39\n",
      "Ep 6 \titer 720  \tloss 1.03468, train acc 73.44, train corr acc 50.49, val acc 75.70, val corr acc 51.89\n",
      "Ep 6 \titer 721  \tloss 1.02329, train acc 78.65, train corr acc 60.19, val acc 75.95, val corr acc 52.39\n",
      "Ep 6 \titer 722  \tloss 1.11874, train acc 73.96, train corr acc 50.98, val acc 75.57, val corr acc 51.64\n",
      "Ep 6 \titer 723  \tloss 0.85506, train acc 79.69, train corr acc 56.18, val acc 75.70, val corr acc 51.89\n",
      "Ep 6 \titer 724  \tloss 1.06673, train acc 72.92, train corr acc 48.00, val acc 76.02, val corr acc 52.52\n",
      "Ep 6 \titer 725  \tloss 0.94590, train acc 77.08, train corr acc 56.86, val acc 75.89, val corr acc 52.27\n",
      "Ep 6 \titer 726  \tloss 1.04627, train acc 74.48, train corr acc 53.77, val acc 75.89, val corr acc 52.27\n",
      "Ep 6 \titer 727  \tloss 0.95975, train acc 76.56, train corr acc 53.61, val acc 76.02, val corr acc 52.52\n",
      "Ep 6 \titer 728  \tloss 0.95588, train acc 76.56, train corr acc 55.45, val acc 75.89, val corr acc 52.27\n",
      "Ep 6 \titer 729  \tloss 0.96198, train acc 78.12, train corr acc 59.22, val acc 76.08, val corr acc 52.64\n",
      "Ep 6 \titer 730  \tloss 0.96578, train acc 76.04, train corr acc 55.34, val acc 76.02, val corr acc 52.52\n",
      "Ep 6 \titer 731  \tloss 0.85898, train acc 78.12, train corr acc 56.25, val acc 76.59, val corr acc 53.65\n",
      "Ep 6 \titer 732  \tloss 1.03158, train acc 73.96, train corr acc 51.92, val acc 75.76, val corr acc 52.02\n",
      "Ep 6 \titer 733  \tloss 1.02820, train acc 76.04, train corr acc 52.58, val acc 76.40, val corr acc 53.27\n",
      "Ep 6 \titer 734  \tloss 0.89259, train acc 78.12, train corr acc 57.58, val acc 76.34, val corr acc 53.15\n",
      "Ep 6 \titer 735  \tloss 1.06683, train acc 76.56, train corr acc 56.73, val acc 76.15, val corr acc 52.77\n",
      "Ep 6 \titer 736  \tloss 1.00806, train acc 73.96, train corr acc 54.55, val acc 75.83, val corr acc 52.14\n",
      "Ep 6 \titer 737  \tloss 1.04837, train acc 72.92, train corr acc 46.39, val acc 75.89, val corr acc 52.27\n",
      "Ep 6 \titer 738  \tloss 0.81291, train acc 80.21, train corr acc 58.70, val acc 75.70, val corr acc 51.89\n",
      "Ep 6 \titer 739  \tloss 0.89772, train acc 79.17, train corr acc 59.60, val acc 75.95, val corr acc 52.39\n",
      "Ep 6 \titer 740  \tloss 0.90655, train acc 76.56, train corr acc 52.13, val acc 75.83, val corr acc 52.14\n",
      "Ep 6 \titer 741  \tloss 0.97961, train acc 75.52, train corr acc 50.53, val acc 75.64, val corr acc 51.76\n",
      "Ep 6 \titer 742  \tloss 0.94284, train acc 73.96, train corr acc 49.49, val acc 75.70, val corr acc 51.89\n",
      "Ep 6 \titer 743  \tloss 0.81354, train acc 79.69, train corr acc 57.14, val acc 75.45, val corr acc 51.39\n",
      "Ep 6 \titer 744  \tloss 0.85628, train acc 77.60, train corr acc 50.57, val acc 75.13, val corr acc 50.76\n",
      "Ep 6 \titer 745  \tloss 0.80761, train acc 80.73, train corr acc 58.43, val acc 75.89, val corr acc 52.27\n",
      "Ep 6 \titer 746  \tloss 0.87179, train acc 79.17, train corr acc 60.40, val acc 74.49, val corr acc 49.50\n",
      "Ep 6 \titer 747  \tloss 0.97127, train acc 75.00, train corr acc 53.85, val acc 74.87, val corr acc 50.25\n",
      "Ep 6 \titer 748  \tloss 0.80179, train acc 81.25, train corr acc 64.71, val acc 75.32, val corr acc 51.13\n",
      "Ep 6 \titer 749  \tloss 0.89574, train acc 76.04, train corr acc 53.54, val acc 75.32, val corr acc 51.13\n",
      "Ep 6 \titer 750  \tloss 0.76259, train acc 78.65, train corr acc 56.38, val acc 75.19, val corr acc 50.88\n",
      "Ep 6 \titer 751  \tloss 0.93336, train acc 78.12, train corr acc 56.25, val acc 75.76, val corr acc 52.02\n",
      "Ep 6 \titer 752  \tloss 0.95243, train acc 77.08, train corr acc 54.64, val acc 75.70, val corr acc 51.89\n",
      "Ep 6 \titer 753  \tloss 1.02075, train acc 73.96, train corr acc 46.81, val acc 75.38, val corr acc 51.26\n",
      "Ep 6 \titer 754  \tloss 0.91897, train acc 72.40, train corr acc 50.47, val acc 75.57, val corr acc 51.64\n",
      "Ep 6 \titer 755  \tloss 0.88644, train acc 78.12, train corr acc 55.32, val acc 75.38, val corr acc 51.26\n",
      "Ep 6 \titer 756  \tloss 1.02755, train acc 72.92, train corr acc 49.51, val acc 75.32, val corr acc 51.13\n",
      "Ep 6 \titer 757  \tloss 0.94769, train acc 78.65, train corr acc 58.16, val acc 75.38, val corr acc 51.26\n",
      "Ep 6 \titer 758  \tloss 0.92943, train acc 76.04, train corr acc 50.00, val acc 75.19, val corr acc 50.88\n",
      "Ep 6 \titer 759  \tloss 1.02012, train acc 73.44, train corr acc 50.49, val acc 75.51, val corr acc 51.51\n",
      "Ep 6 \titer 760  \tloss 1.00724, train acc 73.96, train corr acc 50.98, val acc 74.94, val corr acc 50.38\n",
      "Ep 6 \titer 761  \tloss 0.95258, train acc 76.56, train corr acc 57.94, val acc 75.83, val corr acc 52.14\n",
      "Ep 6 \titer 762  \tloss 0.91346, train acc 76.56, train corr acc 54.55, val acc 76.02, val corr acc 52.52\n",
      "Ep 6 \titer 763  \tloss 0.92093, train acc 76.56, train corr acc 52.63, val acc 75.57, val corr acc 51.64\n",
      "Ep 6 \titer 764  \tloss 0.88539, train acc 78.65, train corr acc 56.38, val acc 74.87, val corr acc 50.25\n",
      "Ep 6 \titer 765  \tloss 0.99727, train acc 75.00, train corr acc 58.26, val acc 74.87, val corr acc 50.25\n",
      "Ep 6 \titer 766  \tloss 0.99002, train acc 74.48, train corr acc 50.51, val acc 75.64, val corr acc 51.76\n",
      "Ep 6 \titer 767  \tloss 0.92478, train acc 77.60, train corr acc 58.25, val acc 75.64, val corr acc 51.76\n",
      "Ep 6 \titer 768  \tloss 1.02536, train acc 73.44, train corr acc 47.96, val acc 76.15, val corr acc 52.77\n",
      "Ep 6 \titer 769  \tloss 0.89975, train acc 77.08, train corr acc 54.64, val acc 75.45, val corr acc 51.39\n",
      "Ep 6 \titer 770  \tloss 1.00746, train acc 71.88, train corr acc 46.00, val acc 75.95, val corr acc 52.39\n",
      "Ep 6 \titer 771  \tloss 0.94336, train acc 73.44, train corr acc 50.49, val acc 75.45, val corr acc 51.39\n",
      "Ep 6 \titer 772  \tloss 0.92023, train acc 76.56, train corr acc 52.13, val acc 75.13, val corr acc 50.76\n",
      "Ep 6 \titer 773  \tloss 0.97566, train acc 74.48, train corr acc 51.49, val acc 75.89, val corr acc 52.27\n",
      "Ep 6 \titer 774  \tloss 0.95763, train acc 75.00, train corr acc 54.72, val acc 76.21, val corr acc 52.90\n",
      "Ep 6 \titer 775  \tloss 0.90894, train acc 76.56, train corr acc 53.61, val acc 75.95, val corr acc 52.39\n",
      "Ep 6 \titer 776  \tloss 1.01954, train acc 73.96, train corr acc 49.49, val acc 75.64, val corr acc 51.76\n",
      "Ep 6 \titer 777  \tloss 1.00976, train acc 75.00, train corr acc 55.14, val acc 75.89, val corr acc 52.27\n",
      "Ep 6 \titer 778  \tloss 0.84161, train acc 77.60, train corr acc 55.67, val acc 76.21, val corr acc 52.90\n",
      "Ep 6 \titer 779  \tloss 0.91121, train acc 75.52, train corr acc 50.00, val acc 76.21, val corr acc 52.90\n",
      "Ep 6 \titer 780  \tloss 1.06746, train acc 71.35, train corr acc 46.60, val acc 76.72, val corr acc 53.90\n",
      "Ep 6 \titer 781  \tloss 0.83812, train acc 80.73, train corr acc 58.89, val acc 75.89, val corr acc 52.27\n",
      "Ep 6 \titer 782  \tloss 0.85140, train acc 77.08, train corr acc 54.17, val acc 75.25, val corr acc 51.01\n",
      "Ep 6 \titer 783  \tloss 1.03557, train acc 72.92, train corr acc 48.00, val acc 75.19, val corr acc 50.88\n",
      "Ep 6 \titer 784  \tloss 1.02274, train acc 73.96, train corr acc 48.45, val acc 75.64, val corr acc 51.76\n",
      "Ep 6 \titer 785  \tloss 0.89422, train acc 76.56, train corr acc 54.08, val acc 75.00, val corr acc 50.50\n",
      "Ep 6 \titer 786  \tloss 1.03713, train acc 71.35, train corr acc 49.54, val acc 74.81, val corr acc 50.13\n",
      "Ep 6 \titer 787  \tloss 0.86245, train acc 77.08, train corr acc 49.43, val acc 74.81, val corr acc 50.13\n",
      "Ep 6 \titer 788  \tloss 0.83160, train acc 78.65, train corr acc 53.93, val acc 75.06, val corr acc 50.63\n",
      "Ep 6 \titer 789  \tloss 0.90762, train acc 78.12, train corr acc 57.14, val acc 75.19, val corr acc 50.88\n",
      "Ep 6 \titer 790  \tloss 0.91297, train acc 78.65, train corr acc 59.41, val acc 75.13, val corr acc 50.76\n",
      "Ep 6 \titer 791  \tloss 0.78166, train acc 80.73, train corr acc 56.98, val acc 74.87, val corr acc 50.25\n",
      "Ep 6 \titer 792  \tloss 0.96619, train acc 76.56, train corr acc 53.61, val acc 74.87, val corr acc 50.25\n",
      "Ep 6 \titer 793  \tloss 0.76526, train acc 79.17, train corr acc 57.89, val acc 76.08, val corr acc 52.64\n",
      "Ep 6 \titer 794  \tloss 0.95946, train acc 77.08, train corr acc 51.11, val acc 75.38, val corr acc 51.26\n",
      "Ep 6 \titer 795  \tloss 0.84929, train acc 77.08, train corr acc 54.17, val acc 75.57, val corr acc 51.64\n",
      "Ep 6 \titer 796  \tloss 0.84834, train acc 78.65, train corr acc 58.16, val acc 75.32, val corr acc 51.13\n",
      "Ep 6 \titer 797  \tloss 1.03331, train acc 69.79, train corr acc 45.79, val acc 75.64, val corr acc 51.76\n",
      "Ep 6 \titer 798  \tloss 0.84527, train acc 78.12, train corr acc 55.32, val acc 75.06, val corr acc 50.63\n",
      "\n",
      "Epoch 7 of 10 took 65.258s\n",
      "  training loss:\t\t0.951748\n",
      "  training raw accuracy:\t\t76.05 %\n",
      "  training corrected acc:\t\t53.36 %\n",
      "  validation loss:\t\t1.144630\n",
      "  validation raw accuracy:\t\t74.87 %\n",
      "  validation corrected acc:\t\t50.25 % \n",
      "\n",
      "Ep 7 \titer 799  \tloss 1.02951, train acc 73.44, train corr acc 52.78, val acc 75.32, val corr acc 51.13\n",
      "Ep 7 \titer 800  \tloss 0.86603, train acc 80.21, train corr acc 58.70, val acc 75.51, val corr acc 51.51\n",
      "Ep 7 \titer 801  \tloss 0.98756, train acc 75.00, train corr acc 51.02, val acc 75.64, val corr acc 51.76\n",
      "Ep 7 \titer 802  \tloss 1.05565, train acc 73.44, train corr acc 47.96, val acc 75.70, val corr acc 51.89\n",
      "Ep 7 \titer 803  \tloss 1.02042, train acc 73.96, train corr acc 50.00, val acc 75.57, val corr acc 51.64\n",
      "Ep 7 \titer 804  \tloss 0.98067, train acc 73.96, train corr acc 54.13, val acc 75.06, val corr acc 50.63\n",
      "Ep 7 \titer 805  \tloss 1.01525, train acc 73.96, train corr acc 49.49, val acc 75.13, val corr acc 50.76\n",
      "Ep 7 \titer 806  \tloss 0.99547, train acc 76.04, train corr acc 52.58, val acc 75.38, val corr acc 51.26\n",
      "Ep 7 \titer 807  \tloss 0.93254, train acc 75.00, train corr acc 52.94, val acc 75.25, val corr acc 51.01\n",
      "Ep 7 \titer 808  \tloss 1.12089, train acc 72.40, train corr acc 50.93, val acc 75.25, val corr acc 51.01\n",
      "Ep 7 \titer 809  \tloss 0.97105, train acc 73.44, train corr acc 50.49, val acc 75.25, val corr acc 51.01\n",
      "Ep 7 \titer 810  \tloss 0.93874, train acc 77.08, train corr acc 53.19, val acc 75.19, val corr acc 50.88\n",
      "Ep 7 \titer 811  \tloss 0.88162, train acc 77.60, train corr acc 52.22, val acc 75.95, val corr acc 52.39\n",
      "Ep 7 \titer 812  \tloss 0.93659, train acc 76.56, train corr acc 53.12, val acc 76.08, val corr acc 52.64\n",
      "Ep 7 \titer 813  \tloss 1.03113, train acc 75.52, train corr acc 57.27, val acc 75.95, val corr acc 52.39\n",
      "Ep 7 \titer 814  \tloss 0.85776, train acc 80.21, train corr acc 59.57, val acc 76.02, val corr acc 52.52\n",
      "Ep 7 \titer 815  \tloss 0.93395, train acc 72.92, train corr acc 49.51, val acc 75.70, val corr acc 51.89\n",
      "Ep 7 \titer 816  \tloss 0.93751, train acc 75.52, train corr acc 54.81, val acc 76.15, val corr acc 52.77\n",
      "Ep 7 \titer 817  \tloss 0.95721, train acc 73.44, train corr acc 46.88, val acc 75.45, val corr acc 51.39\n",
      "Ep 7 \titer 818  \tloss 0.83823, train acc 78.12, train corr acc 56.25, val acc 75.83, val corr acc 52.14\n",
      "Ep 7 \titer 819  \tloss 0.78495, train acc 79.69, train corr acc 54.65, val acc 76.21, val corr acc 52.90\n",
      "Ep 7 \titer 820  \tloss 0.99068, train acc 75.00, train corr acc 53.85, val acc 75.64, val corr acc 51.76\n",
      "Ep 7 \titer 821  \tloss 0.81475, train acc 77.60, train corr acc 49.41, val acc 75.51, val corr acc 51.51\n",
      "Ep 7 \titer 822  \tloss 0.92945, train acc 77.08, train corr acc 56.44, val acc 76.02, val corr acc 52.52\n",
      "Ep 7 \titer 823  \tloss 0.87388, train acc 77.08, train corr acc 54.64, val acc 75.83, val corr acc 52.14\n",
      "Ep 7 \titer 824  \tloss 0.86368, train acc 76.56, train corr acc 54.08, val acc 75.51, val corr acc 51.51\n",
      "Ep 7 \titer 825  \tloss 1.05056, train acc 72.92, train corr acc 49.51, val acc 75.32, val corr acc 51.13\n",
      "Ep 7 \titer 826  \tloss 0.95733, train acc 75.52, train corr acc 52.53, val acc 75.25, val corr acc 51.01\n",
      "Ep 7 \titer 827  \tloss 0.99862, train acc 76.56, train corr acc 54.55, val acc 75.51, val corr acc 51.51\n",
      "Ep 7 \titer 828  \tloss 1.04720, train acc 72.40, train corr acc 48.04, val acc 75.00, val corr acc 50.50\n",
      "Ep 7 \titer 829  \tloss 0.96607, train acc 74.48, train corr acc 48.96, val acc 75.51, val corr acc 51.51\n",
      "Ep 7 \titer 830  \tloss 0.83381, train acc 80.73, train corr acc 60.22, val acc 75.32, val corr acc 51.13\n",
      "Ep 7 \titer 831  \tloss 0.77313, train acc 79.69, train corr acc 57.61, val acc 75.45, val corr acc 51.39\n",
      "Ep 7 \titer 832  \tloss 0.79316, train acc 78.65, train corr acc 58.16, val acc 75.51, val corr acc 51.51\n",
      "Ep 7 \titer 833  \tloss 0.93414, train acc 75.52, train corr acc 56.07, val acc 75.38, val corr acc 51.26\n",
      "Ep 7 \titer 834  \tloss 0.97962, train acc 73.44, train corr acc 50.49, val acc 74.62, val corr acc 49.75\n",
      "Ep 7 \titer 835  \tloss 0.98494, train acc 79.17, train corr acc 61.17, val acc 75.57, val corr acc 51.64\n",
      "Ep 7 \titer 836  \tloss 1.05916, train acc 74.48, train corr acc 51.96, val acc 75.70, val corr acc 51.89\n",
      "Ep 7 \titer 837  \tloss 0.81775, train acc 80.73, train corr acc 58.43, val acc 75.06, val corr acc 50.63\n",
      "Ep 7 \titer 838  \tloss 1.01758, train acc 75.00, train corr acc 52.00, val acc 75.45, val corr acc 51.39\n",
      "Ep 7 \titer 839  \tloss 0.94198, train acc 73.96, train corr acc 50.98, val acc 75.25, val corr acc 51.01\n",
      "Ep 7 \titer 840  \tloss 1.04485, train acc 72.40, train corr acc 50.00, val acc 75.76, val corr acc 52.02\n",
      "Ep 7 \titer 841  \tloss 0.92583, train acc 77.60, train corr acc 55.67, val acc 75.95, val corr acc 52.39\n",
      "Ep 7 \titer 842  \tloss 0.91491, train acc 77.60, train corr acc 57.43, val acc 75.57, val corr acc 51.64\n",
      "Ep 7 \titer 843  \tloss 0.95370, train acc 76.56, train corr acc 56.31, val acc 75.76, val corr acc 52.02\n",
      "Ep 7 \titer 844  \tloss 0.97060, train acc 75.00, train corr acc 53.40, val acc 74.68, val corr acc 49.87\n",
      "Ep 7 \titer 845  \tloss 0.84677, train acc 75.52, train corr acc 51.04, val acc 75.83, val corr acc 52.14\n",
      "Ep 7 \titer 846  \tloss 0.96645, train acc 73.44, train corr acc 50.96, val acc 75.64, val corr acc 51.76\n",
      "Ep 7 \titer 847  \tloss 0.93582, train acc 74.48, train corr acc 49.48, val acc 76.15, val corr acc 52.77\n",
      "Ep 7 \titer 848  \tloss 0.84625, train acc 77.60, train corr acc 56.57, val acc 75.70, val corr acc 51.89\n",
      "Ep 7 \titer 849  \tloss 0.94237, train acc 78.12, train corr acc 59.62, val acc 75.45, val corr acc 51.39\n",
      "Ep 7 \titer 850  \tloss 0.99718, train acc 73.44, train corr acc 53.64, val acc 75.38, val corr acc 51.26\n",
      "Ep 7 \titer 851  \tloss 0.98732, train acc 77.60, train corr acc 55.67, val acc 75.76, val corr acc 52.02\n",
      "Ep 7 \titer 852  \tloss 0.74292, train acc 79.69, train corr acc 57.61, val acc 75.70, val corr acc 51.89\n",
      "Ep 7 \titer 853  \tloss 0.87460, train acc 77.60, train corr acc 56.57, val acc 75.32, val corr acc 51.13\n",
      "Ep 7 \titer 854  \tloss 0.90136, train acc 77.08, train corr acc 53.19, val acc 75.57, val corr acc 51.64\n",
      "Ep 7 \titer 855  \tloss 0.91015, train acc 73.44, train corr acc 46.32, val acc 75.13, val corr acc 50.76\n",
      "Ep 7 \titer 856  \tloss 0.94704, train acc 75.52, train corr acc 52.53, val acc 75.06, val corr acc 50.63\n",
      "Ep 7 \titer 857  \tloss 0.73373, train acc 78.65, train corr acc 54.95, val acc 74.87, val corr acc 50.25\n",
      "Ep 7 \titer 858  \tloss 0.84123, train acc 78.12, train corr acc 51.72, val acc 74.68, val corr acc 49.87\n",
      "Ep 7 \titer 859  \tloss 0.75785, train acc 80.21, train corr acc 57.30, val acc 74.94, val corr acc 50.38\n",
      "Ep 7 \titer 860  \tloss 0.87908, train acc 78.12, train corr acc 58.42, val acc 74.75, val corr acc 50.00\n",
      "Ep 7 \titer 861  \tloss 0.94454, train acc 76.04, train corr acc 55.77, val acc 75.13, val corr acc 50.76\n",
      "Ep 7 \titer 862  \tloss 0.77269, train acc 80.73, train corr acc 63.73, val acc 74.94, val corr acc 50.38\n",
      "Ep 7 \titer 863  \tloss 0.83759, train acc 76.56, train corr acc 54.55, val acc 75.13, val corr acc 50.76\n",
      "Ep 7 \titer 864  \tloss 0.76970, train acc 78.12, train corr acc 55.32, val acc 75.13, val corr acc 50.76\n",
      "Ep 7 \titer 865  \tloss 0.88194, train acc 77.08, train corr acc 54.17, val acc 75.25, val corr acc 51.01\n",
      "Ep 7 \titer 866  \tloss 0.91612, train acc 76.04, train corr acc 52.58, val acc 75.38, val corr acc 51.26\n",
      "Ep 7 \titer 867  \tloss 0.93131, train acc 74.48, train corr acc 47.87, val acc 75.89, val corr acc 52.27\n",
      "Ep 7 \titer 868  \tloss 0.91674, train acc 73.96, train corr acc 53.27, val acc 75.57, val corr acc 51.64\n",
      "Ep 7 \titer 869  \tloss 0.89159, train acc 78.65, train corr acc 56.38, val acc 75.51, val corr acc 51.51\n",
      "Ep 7 \titer 870  \tloss 0.94470, train acc 75.52, train corr acc 54.37, val acc 75.25, val corr acc 51.01\n",
      "Ep 7 \titer 871  \tloss 0.95945, train acc 76.56, train corr acc 54.08, val acc 75.57, val corr acc 51.64\n",
      "Ep 7 \titer 872  \tloss 0.86668, train acc 78.12, train corr acc 54.35, val acc 75.51, val corr acc 51.51\n",
      "Ep 7 \titer 873  \tloss 0.93018, train acc 75.00, train corr acc 53.40, val acc 75.51, val corr acc 51.51\n",
      "Ep 7 \titer 874  \tloss 0.96278, train acc 74.48, train corr acc 51.96, val acc 75.64, val corr acc 51.76\n",
      "Ep 7 \titer 875  \tloss 0.94241, train acc 73.96, train corr acc 53.27, val acc 75.45, val corr acc 51.39\n",
      "Ep 7 \titer 876  \tloss 0.86475, train acc 76.56, train corr acc 54.55, val acc 75.19, val corr acc 50.88\n",
      "Ep 7 \titer 877  \tloss 0.85782, train acc 78.12, train corr acc 55.79, val acc 75.76, val corr acc 52.02\n",
      "Ep 7 \titer 878  \tloss 0.83307, train acc 78.65, train corr acc 56.38, val acc 75.64, val corr acc 51.76\n",
      "Ep 7 \titer 879  \tloss 0.95162, train acc 74.48, train corr acc 57.39, val acc 75.57, val corr acc 51.64\n",
      "Ep 7 \titer 880  \tloss 0.95485, train acc 72.40, train corr acc 46.46, val acc 75.51, val corr acc 51.51\n",
      "Ep 7 \titer 881  \tloss 0.87742, train acc 76.56, train corr acc 56.31, val acc 75.70, val corr acc 51.89\n",
      "Ep 7 \titer 882  \tloss 0.98829, train acc 72.40, train corr acc 45.92, val acc 75.57, val corr acc 51.64\n",
      "Ep 7 \titer 883  \tloss 0.84904, train acc 78.12, train corr acc 56.70, val acc 75.70, val corr acc 51.89\n",
      "Ep 7 \titer 884  \tloss 0.94863, train acc 72.92, train corr acc 48.00, val acc 75.57, val corr acc 51.64\n",
      "Ep 7 \titer 885  \tloss 0.91189, train acc 75.00, train corr acc 53.40, val acc 75.19, val corr acc 50.88\n",
      "Ep 7 \titer 886  \tloss 0.88775, train acc 76.04, train corr acc 51.06, val acc 75.51, val corr acc 51.51\n",
      "Ep 7 \titer 887  \tloss 0.92766, train acc 75.52, train corr acc 53.47, val acc 75.06, val corr acc 50.63\n",
      "Ep 7 \titer 888  \tloss 0.97834, train acc 74.48, train corr acc 53.77, val acc 75.19, val corr acc 50.88\n",
      "Ep 7 \titer 889  \tloss 0.91410, train acc 77.08, train corr acc 54.64, val acc 75.83, val corr acc 52.14\n",
      "Ep 7 \titer 890  \tloss 1.00083, train acc 74.48, train corr acc 50.51, val acc 75.64, val corr acc 51.76\n",
      "Ep 7 \titer 891  \tloss 0.93950, train acc 76.04, train corr acc 57.01, val acc 75.83, val corr acc 52.14\n",
      "Ep 7 \titer 892  \tloss 0.83627, train acc 78.12, train corr acc 56.70, val acc 75.83, val corr acc 52.14\n",
      "Ep 7 \titer 893  \tloss 0.85941, train acc 77.60, train corr acc 54.26, val acc 76.27, val corr acc 53.02\n",
      "Ep 7 \titer 894  \tloss 1.00825, train acc 72.92, train corr acc 49.51, val acc 76.34, val corr acc 53.15\n",
      "Ep 7 \titer 895  \tloss 0.79792, train acc 80.73, train corr acc 58.89, val acc 75.95, val corr acc 52.39\n",
      "Ep 7 \titer 896  \tloss 0.83251, train acc 77.60, train corr acc 55.21, val acc 76.59, val corr acc 53.65\n",
      "Ep 7 \titer 897  \tloss 0.98751, train acc 73.96, train corr acc 50.00, val acc 76.08, val corr acc 52.64\n",
      "Ep 7 \titer 898  \tloss 0.91313, train acc 75.52, train corr acc 51.55, val acc 75.38, val corr acc 51.26\n",
      "Ep 7 \titer 899  \tloss 0.88316, train acc 78.12, train corr acc 57.14, val acc 75.45, val corr acc 51.39\n",
      "Ep 7 \titer 900  \tloss 1.01381, train acc 71.35, train corr acc 49.54, val acc 75.19, val corr acc 50.88\n",
      "Ep 7 \titer 901  \tloss 0.81439, train acc 77.60, train corr acc 50.57, val acc 75.64, val corr acc 51.76\n",
      "Ep 7 \titer 902  \tloss 0.82623, train acc 78.12, train corr acc 52.81, val acc 74.68, val corr acc 49.87\n",
      "Ep 7 \titer 903  \tloss 0.82963, train acc 77.08, train corr acc 55.10, val acc 75.19, val corr acc 50.88\n",
      "Ep 7 \titer 904  \tloss 0.89628, train acc 76.04, train corr acc 54.46, val acc 74.81, val corr acc 50.13\n",
      "Ep 7 \titer 905  \tloss 0.74008, train acc 80.21, train corr acc 55.81, val acc 74.68, val corr acc 49.87\n",
      "Ep 7 \titer 906  \tloss 0.91879, train acc 75.52, train corr acc 51.55, val acc 75.38, val corr acc 51.26\n",
      "Ep 7 \titer 907  \tloss 0.73644, train acc 79.69, train corr acc 58.95, val acc 74.81, val corr acc 50.13\n",
      "Ep 7 \titer 908  \tloss 0.95396, train acc 76.04, train corr acc 48.89, val acc 75.83, val corr acc 52.14\n",
      "Ep 7 \titer 909  \tloss 0.83455, train acc 78.12, train corr acc 56.25, val acc 75.51, val corr acc 51.51\n",
      "Ep 7 \titer 910  \tloss 0.80366, train acc 77.60, train corr acc 56.12, val acc 74.94, val corr acc 50.38\n",
      "Ep 7 \titer 911  \tloss 1.02105, train acc 69.79, train corr acc 45.79, val acc 75.57, val corr acc 51.64\n",
      "Ep 7 \titer 912  \tloss 0.84896, train acc 78.12, train corr acc 55.32, val acc 76.15, val corr acc 52.77\n",
      "\n",
      "Epoch 8 of 10 took 55.970s\n",
      "  training loss:\t\t0.913423\n",
      "  training raw accuracy:\t\t76.19 %\n",
      "  training corrected acc:\t\t53.66 %\n",
      "  validation loss:\t\t1.127358\n",
      "  validation raw accuracy:\t\t75.13 %\n",
      "  validation corrected acc:\t\t50.76 % \n",
      "\n",
      "Ep 8 \titer 913  \tloss 1.02629, train acc 75.52, train corr acc 56.48, val acc 75.32, val corr acc 51.13\n",
      "Ep 8 \titer 914  \tloss 0.83144, train acc 77.60, train corr acc 53.26, val acc 75.64, val corr acc 51.76\n",
      "Ep 8 \titer 915  \tloss 0.91497, train acc 76.04, train corr acc 53.06, val acc 75.19, val corr acc 50.88\n",
      "Ep 8 \titer 916  \tloss 1.03095, train acc 72.92, train corr acc 46.94, val acc 75.00, val corr acc 50.50\n",
      "Ep 8 \titer 917  \tloss 0.97600, train acc 74.48, train corr acc 51.00, val acc 74.30, val corr acc 49.12\n",
      "Ep 8 \titer 918  \tloss 0.96192, train acc 72.40, train corr acc 51.38, val acc 74.75, val corr acc 50.00\n",
      "Ep 8 \titer 919  \tloss 0.96809, train acc 76.04, train corr acc 53.54, val acc 75.00, val corr acc 50.50\n",
      "Ep 8 \titer 920  \tloss 0.94354, train acc 75.00, train corr acc 50.52, val acc 75.32, val corr acc 51.13\n",
      "Ep 8 \titer 921  \tloss 0.90089, train acc 75.00, train corr acc 52.94, val acc 75.13, val corr acc 50.76\n",
      "Ep 8 \titer 922  \tloss 1.02727, train acc 72.92, train corr acc 51.85, val acc 75.00, val corr acc 50.50\n",
      "Ep 8 \titer 923  \tloss 0.95408, train acc 73.96, train corr acc 51.46, val acc 75.57, val corr acc 51.64\n",
      "Ep 8 \titer 924  \tloss 0.92876, train acc 77.60, train corr acc 54.26, val acc 75.00, val corr acc 50.50\n",
      "Ep 8 \titer 925  \tloss 0.83659, train acc 76.04, train corr acc 48.89, val acc 74.81, val corr acc 50.13\n",
      "Ep 8 \titer 926  \tloss 0.93943, train acc 76.04, train corr acc 52.08, val acc 74.81, val corr acc 50.13\n",
      "Ep 8 \titer 927  \tloss 0.95032, train acc 76.04, train corr acc 58.18, val acc 75.64, val corr acc 51.76\n",
      "Ep 8 \titer 928  \tloss 0.81849, train acc 79.17, train corr acc 57.45, val acc 75.32, val corr acc 51.13\n",
      "Ep 8 \titer 929  \tloss 0.87591, train acc 75.00, train corr acc 53.40, val acc 74.94, val corr acc 50.38\n",
      "Ep 8 \titer 930  \tloss 0.91246, train acc 74.48, train corr acc 52.88, val acc 74.94, val corr acc 50.38\n",
      "Ep 8 \titer 931  \tloss 0.97217, train acc 72.40, train corr acc 44.79, val acc 74.55, val corr acc 49.62\n",
      "Ep 8 \titer 932  \tloss 0.80821, train acc 79.17, train corr acc 58.33, val acc 74.68, val corr acc 49.87\n",
      "Ep 8 \titer 933  \tloss 0.75220, train acc 80.73, train corr acc 56.98, val acc 74.49, val corr acc 49.50\n",
      "Ep 8 \titer 934  \tloss 0.96498, train acc 73.96, train corr acc 51.92, val acc 74.87, val corr acc 50.25\n",
      "Ep 8 \titer 935  \tloss 0.78155, train acc 78.12, train corr acc 50.59, val acc 75.13, val corr acc 50.76\n",
      "Ep 8 \titer 936  \tloss 0.89311, train acc 78.12, train corr acc 58.42, val acc 75.32, val corr acc 51.13\n",
      "Ep 8 \titer 937  \tloss 0.86973, train acc 77.08, train corr acc 54.64, val acc 74.87, val corr acc 50.25\n",
      "Ep 8 \titer 938  \tloss 0.82658, train acc 79.69, train corr acc 60.20, val acc 74.87, val corr acc 50.25\n",
      "Ep 8 \titer 939  \tloss 0.96646, train acc 75.52, train corr acc 54.37, val acc 75.51, val corr acc 51.51\n",
      "Ep 8 \titer 940  \tloss 0.92952, train acc 75.00, train corr acc 51.52, val acc 74.68, val corr acc 49.87\n",
      "Ep 8 \titer 941  \tloss 0.88284, train acc 76.56, train corr acc 54.55, val acc 74.75, val corr acc 50.00\n",
      "Ep 8 \titer 942  \tloss 0.98610, train acc 73.44, train corr acc 50.00, val acc 75.32, val corr acc 51.13\n",
      "Ep 8 \titer 943  \tloss 0.89981, train acc 75.52, train corr acc 51.04, val acc 75.32, val corr acc 51.13\n",
      "Ep 8 \titer 944  \tloss 0.79022, train acc 79.17, train corr acc 56.99, val acc 74.81, val corr acc 50.13\n",
      "Ep 8 \titer 945  \tloss 0.75418, train acc 79.17, train corr acc 56.52, val acc 75.25, val corr acc 51.01\n",
      "Ep 8 \titer 946  \tloss 0.78753, train acc 77.08, train corr acc 55.10, val acc 75.00, val corr acc 50.50\n",
      "Ep 8 \titer 947  \tloss 0.92404, train acc 77.60, train corr acc 59.81, val acc 75.25, val corr acc 51.01\n",
      "Ep 8 \titer 948  \tloss 0.99455, train acc 71.88, train corr acc 47.57, val acc 75.45, val corr acc 51.39\n",
      "Ep 8 \titer 949  \tloss 0.95341, train acc 78.65, train corr acc 60.19, val acc 75.83, val corr acc 52.14\n",
      "Ep 8 \titer 950  \tloss 0.97306, train acc 77.60, train corr acc 57.84, val acc 75.38, val corr acc 51.26\n",
      "Ep 8 \titer 951  \tloss 0.76072, train acc 79.69, train corr acc 56.18, val acc 75.89, val corr acc 52.27\n",
      "Ep 8 \titer 952  \tloss 0.93766, train acc 73.96, train corr acc 50.00, val acc 74.81, val corr acc 50.13\n",
      "Ep 8 \titer 953  \tloss 0.86084, train acc 74.48, train corr acc 51.96, val acc 75.25, val corr acc 51.01\n",
      "Ep 8 \titer 954  \tloss 0.97743, train acc 74.48, train corr acc 53.77, val acc 76.34, val corr acc 53.15\n",
      "Ep 8 \titer 955  \tloss 0.84270, train acc 78.12, train corr acc 56.70, val acc 75.95, val corr acc 52.39\n",
      "Ep 8 \titer 956  \tloss 0.87462, train acc 76.56, train corr acc 55.45, val acc 76.21, val corr acc 52.90\n",
      "Ep 8 \titer 957  \tloss 0.84270, train acc 76.04, train corr acc 55.34, val acc 75.38, val corr acc 51.26\n",
      "Ep 8 \titer 958  \tloss 0.95103, train acc 76.04, train corr acc 55.34, val acc 75.83, val corr acc 52.14\n",
      "Ep 8 \titer 959  \tloss 0.85903, train acc 77.60, train corr acc 55.21, val acc 76.08, val corr acc 52.64\n",
      "Ep 8 \titer 960  \tloss 0.93440, train acc 73.96, train corr acc 51.92, val acc 75.32, val corr acc 51.13\n",
      "Ep 8 \titer 961  \tloss 0.94469, train acc 75.00, train corr acc 50.52, val acc 75.76, val corr acc 52.02\n",
      "Ep 8 \titer 962  \tloss 0.84684, train acc 76.56, train corr acc 54.55, val acc 76.15, val corr acc 52.77\n",
      "Ep 8 \titer 963  \tloss 0.92941, train acc 76.04, train corr acc 55.77, val acc 75.83, val corr acc 52.14\n",
      "Ep 8 \titer 964  \tloss 0.96124, train acc 75.00, train corr acc 56.36, val acc 76.34, val corr acc 53.15\n",
      "Ep 8 \titer 965  \tloss 0.93752, train acc 77.60, train corr acc 55.67, val acc 75.70, val corr acc 51.89\n",
      "Ep 8 \titer 966  \tloss 0.71270, train acc 79.17, train corr acc 56.52, val acc 76.27, val corr acc 53.02\n",
      "Ep 8 \titer 967  \tloss 0.81079, train acc 77.60, train corr acc 56.57, val acc 76.21, val corr acc 52.90\n",
      "Ep 8 \titer 968  \tloss 0.86273, train acc 77.60, train corr acc 55.32, val acc 75.64, val corr acc 51.76\n",
      "Ep 8 \titer 969  \tloss 0.94453, train acc 78.12, train corr acc 55.79, val acc 76.21, val corr acc 52.90\n",
      "Ep 8 \titer 970  \tloss 0.86587, train acc 75.52, train corr acc 52.53, val acc 75.76, val corr acc 52.02\n",
      "Ep 8 \titer 971  \tloss 0.70528, train acc 78.12, train corr acc 53.85, val acc 75.70, val corr acc 51.89\n",
      "Ep 8 \titer 972  \tloss 0.78205, train acc 79.69, train corr acc 55.17, val acc 75.51, val corr acc 51.51\n",
      "Ep 8 \titer 973  \tloss 0.77239, train acc 79.69, train corr acc 56.18, val acc 74.68, val corr acc 49.87\n",
      "Ep 8 \titer 974  \tloss 0.88902, train acc 77.08, train corr acc 56.44, val acc 75.13, val corr acc 50.76\n",
      "Ep 8 \titer 975  \tloss 0.93780, train acc 74.48, train corr acc 52.88, val acc 75.00, val corr acc 50.50\n",
      "Ep 8 \titer 976  \tloss 0.73718, train acc 80.21, train corr acc 62.75, val acc 75.06, val corr acc 50.63\n",
      "Ep 8 \titer 977  \tloss 0.83037, train acc 77.60, train corr acc 56.57, val acc 74.75, val corr acc 50.00\n",
      "Ep 8 \titer 978  \tloss 0.79384, train acc 77.08, train corr acc 53.19, val acc 75.13, val corr acc 50.76\n",
      "Ep 8 \titer 979  \tloss 0.87622, train acc 78.65, train corr acc 57.29, val acc 75.38, val corr acc 51.26\n",
      "Ep 8 \titer 980  \tloss 0.90033, train acc 76.56, train corr acc 53.61, val acc 75.45, val corr acc 51.39\n",
      "Ep 8 \titer 981  \tloss 0.93643, train acc 75.00, train corr acc 48.94, val acc 75.57, val corr acc 51.64\n",
      "Ep 8 \titer 982  \tloss 0.85335, train acc 72.40, train corr acc 50.47, val acc 75.64, val corr acc 51.76\n",
      "Ep 8 \titer 983  \tloss 0.83847, train acc 79.69, train corr acc 58.51, val acc 75.83, val corr acc 52.14\n",
      "Ep 8 \titer 984  \tloss 0.88611, train acc 75.52, train corr acc 54.37, val acc 75.89, val corr acc 52.27\n",
      "Ep 8 \titer 985  \tloss 0.84044, train acc 78.12, train corr acc 57.14, val acc 75.76, val corr acc 52.02\n",
      "Ep 8 \titer 986  \tloss 0.86111, train acc 77.60, train corr acc 53.26, val acc 75.19, val corr acc 50.88\n",
      "Ep 8 \titer 987  \tloss 0.93730, train acc 73.96, train corr acc 51.46, val acc 75.25, val corr acc 51.01\n",
      "Ep 8 \titer 988  \tloss 0.91237, train acc 75.52, train corr acc 53.92, val acc 75.32, val corr acc 51.13\n",
      "Ep 8 \titer 989  \tloss 0.88815, train acc 76.56, train corr acc 57.94, val acc 74.62, val corr acc 49.75\n",
      "Ep 8 \titer 990  \tloss 0.81845, train acc 75.52, train corr acc 52.53, val acc 74.81, val corr acc 50.13\n",
      "Ep 8 \titer 991  \tloss 0.82867, train acc 77.60, train corr acc 54.74, val acc 75.00, val corr acc 50.50\n",
      "Ep 8 \titer 992  \tloss 0.81851, train acc 78.65, train corr acc 56.38, val acc 75.00, val corr acc 50.50\n",
      "Ep 8 \titer 993  \tloss 0.91179, train acc 75.00, train corr acc 58.26, val acc 75.57, val corr acc 51.64\n",
      "Ep 8 \titer 994  \tloss 0.90637, train acc 75.00, train corr acc 51.52, val acc 75.19, val corr acc 50.88\n",
      "Ep 8 \titer 995  \tloss 0.85962, train acc 77.08, train corr acc 57.28, val acc 75.06, val corr acc 50.63\n",
      "Ep 8 \titer 996  \tloss 0.96384, train acc 72.92, train corr acc 46.94, val acc 74.36, val corr acc 49.24\n",
      "Ep 8 \titer 997  \tloss 0.80646, train acc 75.52, train corr acc 51.55, val acc 75.19, val corr acc 50.88\n",
      "Ep 8 \titer 998  \tloss 0.96021, train acc 73.96, train corr acc 50.00, val acc 75.06, val corr acc 50.63\n",
      "Ep 8 \titer 999  \tloss 0.88420, train acc 76.56, train corr acc 56.31, val acc 75.51, val corr acc 51.51\n",
      "Ep 8 \titer 1000  \tloss 0.88916, train acc 76.04, train corr acc 51.06, val acc 75.51, val corr acc 51.51\n",
      "Ep 8 \titer 1001  \tloss 0.91468, train acc 75.00, train corr acc 52.48, val acc 75.83, val corr acc 52.14\n",
      "Ep 8 \titer 1002  \tloss 0.89815, train acc 75.52, train corr acc 55.66, val acc 76.21, val corr acc 52.90\n",
      "Ep 8 \titer 1003  \tloss 0.86215, train acc 76.56, train corr acc 53.61, val acc 76.21, val corr acc 52.90\n",
      "Ep 8 \titer 1004  \tloss 0.97659, train acc 72.40, train corr acc 46.46, val acc 75.95, val corr acc 52.39\n",
      "Ep 8 \titer 1005  \tloss 0.91083, train acc 75.00, train corr acc 55.14, val acc 74.68, val corr acc 49.87\n",
      "Ep 8 \titer 1006  \tloss 0.78247, train acc 75.00, train corr acc 50.52, val acc 74.87, val corr acc 50.25\n",
      "Ep 8 \titer 1007  \tloss 0.85002, train acc 76.04, train corr acc 51.06, val acc 75.45, val corr acc 51.39\n",
      "Ep 8 \titer 1008  \tloss 0.96839, train acc 71.35, train corr acc 46.60, val acc 75.45, val corr acc 51.39\n",
      "Ep 8 \titer 1009  \tloss 0.82481, train acc 79.69, train corr acc 56.67, val acc 75.64, val corr acc 51.76\n",
      "Ep 8 \titer 1010  \tloss 0.79766, train acc 76.56, train corr acc 53.12, val acc 75.64, val corr acc 51.76\n",
      "Ep 8 \titer 1011  \tloss 0.91696, train acc 74.48, train corr acc 51.00, val acc 76.15, val corr acc 52.77\n",
      "Ep 8 \titer 1012  \tloss 0.86594, train acc 75.00, train corr acc 50.52, val acc 75.64, val corr acc 51.76\n",
      "Ep 8 \titer 1013  \tloss 0.87519, train acc 76.56, train corr acc 54.08, val acc 75.70, val corr acc 51.89\n",
      "Ep 8 \titer 1014  \tloss 0.98931, train acc 71.35, train corr acc 49.54, val acc 75.45, val corr acc 51.39\n",
      "Ep 8 \titer 1015  \tloss 0.78769, train acc 77.60, train corr acc 50.57, val acc 75.51, val corr acc 51.51\n",
      "Ep 8 \titer 1016  \tloss 0.75493, train acc 80.21, train corr acc 57.30, val acc 75.38, val corr acc 51.26\n",
      "Ep 8 \titer 1017  \tloss 0.81258, train acc 78.65, train corr acc 58.16, val acc 75.19, val corr acc 50.88\n",
      "Ep 8 \titer 1018  \tloss 0.85136, train acc 77.08, train corr acc 56.44, val acc 74.36, val corr acc 49.24\n",
      "Ep 8 \titer 1019  \tloss 0.72443, train acc 80.21, train corr acc 55.81, val acc 75.57, val corr acc 51.64\n",
      "Ep 8 \titer 1020  \tloss 0.81659, train acc 80.21, train corr acc 60.82, val acc 75.38, val corr acc 51.26\n",
      "Ep 8 \titer 1021  \tloss 0.71505, train acc 82.29, train corr acc 64.21, val acc 75.00, val corr acc 50.50\n",
      "Ep 8 \titer 1022  \tloss 0.87538, train acc 76.56, train corr acc 50.00, val acc 74.81, val corr acc 50.13\n",
      "Ep 8 \titer 1023  \tloss 0.83041, train acc 77.08, train corr acc 54.17, val acc 75.25, val corr acc 51.01\n",
      "Ep 8 \titer 1024  \tloss 0.79132, train acc 77.60, train corr acc 56.12, val acc 75.13, val corr acc 50.76\n",
      "Ep 8 \titer 1025  \tloss 1.00288, train acc 70.31, train corr acc 46.73, val acc 75.19, val corr acc 50.88\n",
      "Ep 8 \titer 1026  \tloss 0.86072, train acc 79.17, train corr acc 57.45, val acc 75.06, val corr acc 50.63\n",
      "\n",
      "Epoch 9 of 10 took 61.385s\n",
      "  training loss:\t\t0.879361\n",
      "  training raw accuracy:\t\t76.38 %\n",
      "  training corrected acc:\t\t54.01 %\n",
      "  validation loss:\t\t1.161455\n",
      "  validation raw accuracy:\t\t74.75 %\n",
      "  validation corrected acc:\t\t50.00 % \n",
      "\n",
      "Ep 9 \titer 1027  \tloss 0.98665, train acc 73.44, train corr acc 52.78, val acc 75.57, val corr acc 51.64\n",
      "Ep 9 \titer 1028  \tloss 0.83193, train acc 79.17, train corr acc 56.52, val acc 75.19, val corr acc 50.88\n",
      "Ep 9 \titer 1029  \tloss 0.86729, train acc 75.52, train corr acc 52.04, val acc 75.32, val corr acc 51.13\n",
      "Ep 9 \titer 1030  \tloss 0.98797, train acc 73.44, train corr acc 47.96, val acc 75.06, val corr acc 50.63\n",
      "Ep 9 \titer 1031  \tloss 0.94591, train acc 71.88, train corr acc 46.00, val acc 75.13, val corr acc 50.76\n",
      "Ep 9 \titer 1032  \tloss 0.90042, train acc 75.52, train corr acc 56.88, val acc 74.11, val corr acc 48.74\n",
      "Ep 9 \titer 1033  \tloss 0.92426, train acc 75.00, train corr acc 51.52, val acc 74.17, val corr acc 48.87\n",
      "Ep 9 \titer 1034  \tloss 0.95899, train acc 75.00, train corr acc 50.52, val acc 74.55, val corr acc 49.62\n",
      "Ep 9 \titer 1035  \tloss 0.82677, train acc 76.56, train corr acc 55.88, val acc 74.55, val corr acc 49.62\n",
      "Ep 9 \titer 1036  \tloss 1.08767, train acc 72.92, train corr acc 51.85, val acc 74.81, val corr acc 50.13\n",
      "Ep 9 \titer 1037  \tloss 0.93695, train acc 73.96, train corr acc 51.46, val acc 74.62, val corr acc 49.75\n",
      "Ep 9 \titer 1038  \tloss 0.85240, train acc 76.04, train corr acc 51.06, val acc 74.81, val corr acc 50.13\n",
      "Ep 9 \titer 1039  \tloss 0.84158, train acc 77.08, train corr acc 51.11, val acc 75.13, val corr acc 50.76\n",
      "Ep 9 \titer 1040  \tloss 0.88558, train acc 75.52, train corr acc 51.04, val acc 75.06, val corr acc 50.63\n",
      "Ep 9 \titer 1041  \tloss 0.89953, train acc 73.44, train corr acc 53.64, val acc 74.55, val corr acc 49.62\n",
      "Ep 9 \titer 1042  \tloss 0.78064, train acc 80.21, train corr acc 59.57, val acc 74.94, val corr acc 50.38\n",
      "Ep 9 \titer 1043  \tloss 0.85381, train acc 72.92, train corr acc 49.51, val acc 74.62, val corr acc 49.75\n",
      "Ep 9 \titer 1044  \tloss 0.86406, train acc 76.56, train corr acc 56.73, val acc 74.94, val corr acc 50.38\n",
      "Ep 9 \titer 1045  \tloss 0.96481, train acc 73.44, train corr acc 46.88, val acc 74.68, val corr acc 49.87\n",
      "Ep 9 \titer 1046  \tloss 0.79405, train acc 78.65, train corr acc 57.29, val acc 74.43, val corr acc 49.37\n",
      "Ep 9 \titer 1047  \tloss 0.73156, train acc 79.69, train corr acc 54.65, val acc 74.87, val corr acc 50.25\n",
      "Ep 9 \titer 1048  \tloss 0.96190, train acc 73.44, train corr acc 50.96, val acc 75.51, val corr acc 51.51\n",
      "Ep 9 \titer 1049  \tloss 0.72783, train acc 78.65, train corr acc 51.76, val acc 74.94, val corr acc 50.38\n",
      "Ep 9 \titer 1050  \tloss 0.85528, train acc 78.65, train corr acc 59.41, val acc 75.51, val corr acc 51.51\n",
      "Ep 9 \titer 1051  \tloss 0.86763, train acc 78.65, train corr acc 57.73, val acc 74.55, val corr acc 49.62\n",
      "Ep 9 \titer 1052  \tloss 0.79956, train acc 79.17, train corr acc 59.18, val acc 74.81, val corr acc 50.13\n",
      "Ep 9 \titer 1053  \tloss 0.97132, train acc 78.65, train corr acc 60.19, val acc 74.55, val corr acc 49.62\n",
      "Ep 9 \titer 1054  \tloss 0.91137, train acc 76.04, train corr acc 53.54, val acc 74.68, val corr acc 49.87\n",
      "Ep 9 \titer 1055  \tloss 0.88948, train acc 78.12, train corr acc 57.58, val acc 74.68, val corr acc 49.87\n",
      "Ep 9 \titer 1056  \tloss 0.97002, train acc 74.48, train corr acc 51.96, val acc 75.25, val corr acc 51.01\n",
      "Ep 9 \titer 1057  \tloss 0.88576, train acc 77.08, train corr acc 54.17, val acc 75.45, val corr acc 51.39\n",
      "Ep 9 \titer 1058  \tloss 0.77131, train acc 78.12, train corr acc 54.84, val acc 75.06, val corr acc 50.63\n",
      "Ep 9 \titer 1059  \tloss 0.72582, train acc 80.73, train corr acc 59.78, val acc 75.00, val corr acc 50.50\n",
      "Ep 9 \titer 1060  \tloss 0.77518, train acc 79.69, train corr acc 60.20, val acc 74.94, val corr acc 50.38\n",
      "Ep 9 \titer 1061  \tloss 0.88130, train acc 77.60, train corr acc 59.81, val acc 74.81, val corr acc 50.13\n",
      "Ep 9 \titer 1062  \tloss 0.99839, train acc 71.35, train corr acc 46.60, val acc 74.68, val corr acc 49.87\n",
      "Ep 9 \titer 1063  \tloss 0.96499, train acc 76.56, train corr acc 56.31, val acc 75.25, val corr acc 51.01\n",
      "Ep 9 \titer 1064  \tloss 0.97006, train acc 75.52, train corr acc 53.92, val acc 75.00, val corr acc 50.50\n",
      "Ep 9 \titer 1065  \tloss 0.73217, train acc 79.17, train corr acc 55.06, val acc 75.64, val corr acc 51.76\n",
      "Ep 9 \titer 1066  \tloss 0.86451, train acc 77.60, train corr acc 57.00, val acc 75.38, val corr acc 51.26\n",
      "Ep 9 \titer 1067  \tloss 0.87560, train acc 76.56, train corr acc 55.88, val acc 75.32, val corr acc 51.13\n",
      "Ep 9 \titer 1068  \tloss 0.96381, train acc 72.92, train corr acc 50.94, val acc 75.19, val corr acc 50.88\n",
      "Ep 9 \titer 1069  \tloss 0.85720, train acc 79.17, train corr acc 58.76, val acc 75.32, val corr acc 51.13\n",
      "Ep 9 \titer 1070  \tloss 0.83158, train acc 77.60, train corr acc 57.43, val acc 75.57, val corr acc 51.64\n",
      "Ep 9 \titer 1071  \tloss 0.91006, train acc 74.48, train corr acc 52.43, val acc 75.00, val corr acc 50.50\n",
      "Ep 9 \titer 1072  \tloss 0.91239, train acc 73.96, train corr acc 51.46, val acc 75.19, val corr acc 50.88\n",
      "Ep 9 \titer 1073  \tloss 0.79566, train acc 78.12, train corr acc 56.25, val acc 74.75, val corr acc 50.00\n",
      "Ep 9 \titer 1074  \tloss 0.93797, train acc 74.48, train corr acc 52.88, val acc 75.70, val corr acc 51.89\n",
      "Ep 9 \titer 1075  \tloss 0.92376, train acc 76.04, train corr acc 52.58, val acc 75.64, val corr acc 51.76\n",
      "Ep 9 \titer 1076  \tloss 0.80062, train acc 78.65, train corr acc 58.59, val acc 75.89, val corr acc 52.27\n",
      "Ep 9 \titer 1077  \tloss 0.88491, train acc 77.08, train corr acc 57.69, val acc 75.64, val corr acc 51.76\n",
      "Ep 9 \titer 1078  \tloss 0.92271, train acc 76.04, train corr acc 58.18, val acc 75.83, val corr acc 52.14\n",
      "Ep 9 \titer 1079  \tloss 0.88805, train acc 76.04, train corr acc 52.58, val acc 74.87, val corr acc 50.25\n",
      "Ep 9 \titer 1080  \tloss 0.73222, train acc 82.29, train corr acc 63.04, val acc 75.57, val corr acc 51.64\n",
      "Ep 9 \titer 1081  \tloss 0.72666, train acc 80.73, train corr acc 62.63, val acc 75.89, val corr acc 52.27\n",
      "Ep 9 \titer 1082  \tloss 0.86081, train acc 77.08, train corr acc 53.19, val acc 74.94, val corr acc 50.38\n",
      "Ep 9 \titer 1083  \tloss 0.90256, train acc 76.04, train corr acc 51.58, val acc 75.00, val corr acc 50.50\n",
      "Ep 9 \titer 1084  \tloss 0.91896, train acc 75.52, train corr acc 52.53, val acc 74.94, val corr acc 50.38\n",
      "Ep 9 \titer 1085  \tloss 0.65920, train acc 80.21, train corr acc 58.24, val acc 74.75, val corr acc 50.00\n",
      "Ep 9 \titer 1086  \tloss 0.79521, train acc 79.17, train corr acc 54.02, val acc 74.68, val corr acc 49.87\n",
      "Ep 9 \titer 1087  \tloss 0.75303, train acc 80.73, train corr acc 58.43, val acc 74.62, val corr acc 49.75\n",
      "Ep 9 \titer 1088  \tloss 0.81561, train acc 78.65, train corr acc 59.41, val acc 74.49, val corr acc 49.50\n",
      "Ep 9 \titer 1089  \tloss 0.88500, train acc 79.17, train corr acc 61.54, val acc 74.30, val corr acc 49.12\n",
      "Ep 9 \titer 1090  \tloss 0.74161, train acc 81.25, train corr acc 64.71, val acc 74.55, val corr acc 49.62\n",
      "Ep 9 \titer 1091  \tloss 0.77886, train acc 77.08, train corr acc 55.56, val acc 74.36, val corr acc 49.24\n",
      "Ep 9 \titer 1092  \tloss 0.74282, train acc 78.12, train corr acc 55.32, val acc 74.43, val corr acc 49.37\n",
      "Ep 9 \titer 1093  \tloss 0.83334, train acc 79.17, train corr acc 58.33, val acc 74.87, val corr acc 50.25\n",
      "Ep 9 \titer 1094  \tloss 0.84672, train acc 76.56, train corr acc 53.61, val acc 74.62, val corr acc 49.75\n",
      "Ep 9 \titer 1095  \tloss 0.91622, train acc 78.12, train corr acc 55.32, val acc 75.06, val corr acc 50.63\n",
      "Ep 9 \titer 1096  \tloss 0.83037, train acc 77.60, train corr acc 59.81, val acc 75.25, val corr acc 51.01\n",
      "Ep 9 \titer 1097  \tloss 0.81241, train acc 79.17, train corr acc 57.45, val acc 75.25, val corr acc 51.01\n",
      "Ep 9 \titer 1098  \tloss 0.90928, train acc 76.04, train corr acc 55.34, val acc 75.00, val corr acc 50.50\n",
      "Ep 9 \titer 1099  \tloss 0.84926, train acc 77.60, train corr acc 56.12, val acc 75.06, val corr acc 50.63\n",
      "Ep 9 \titer 1100  \tloss 0.79660, train acc 79.69, train corr acc 57.61, val acc 73.66, val corr acc 47.86\n",
      "Ep 9 \titer 1101  \tloss 0.92628, train acc 75.52, train corr acc 54.37, val acc 74.62, val corr acc 49.75\n",
      "Ep 9 \titer 1102  \tloss 0.89208, train acc 74.48, train corr acc 51.96, val acc 74.62, val corr acc 49.75\n",
      "Ep 9 \titer 1103  \tloss 0.83910, train acc 76.04, train corr acc 57.01, val acc 74.94, val corr acc 50.38\n",
      "Ep 9 \titer 1104  \tloss 0.78193, train acc 75.52, train corr acc 52.53, val acc 75.64, val corr acc 51.76\n",
      "Ep 9 \titer 1105  \tloss 0.78447, train acc 77.60, train corr acc 54.74, val acc 75.00, val corr acc 50.50\n",
      "Ep 9 \titer 1106  \tloss 0.80972, train acc 80.21, train corr acc 59.57, val acc 75.70, val corr acc 51.89\n",
      "Ep 9 \titer 1107  \tloss 0.87989, train acc 77.08, train corr acc 61.74, val acc 75.51, val corr acc 51.51\n",
      "Ep 9 \titer 1108  \tloss 0.87533, train acc 75.00, train corr acc 51.52, val acc 75.19, val corr acc 50.88\n",
      "Ep 9 \titer 1109  \tloss 0.85057, train acc 78.12, train corr acc 59.22, val acc 75.38, val corr acc 51.26\n",
      "Ep 9 \titer 1110  \tloss 0.91157, train acc 75.00, train corr acc 51.02, val acc 75.32, val corr acc 51.13\n",
      "Ep 9 \titer 1111  \tloss 0.75823, train acc 75.52, train corr acc 51.55, val acc 75.57, val corr acc 51.64\n",
      "Ep 9 \titer 1112  \tloss 0.89141, train acc 73.96, train corr acc 50.00, val acc 74.81, val corr acc 50.13\n",
      "Ep 9 \titer 1113  \tloss 0.85550, train acc 78.12, train corr acc 59.22, val acc 75.13, val corr acc 50.76\n",
      "Ep 9 \titer 1114  \tloss 0.87556, train acc 77.08, train corr acc 53.19, val acc 75.00, val corr acc 50.50\n",
      "Ep 9 \titer 1115  \tloss 0.83225, train acc 76.04, train corr acc 54.46, val acc 75.51, val corr acc 51.51\n",
      "Ep 9 \titer 1116  \tloss 0.91674, train acc 76.04, train corr acc 56.60, val acc 74.81, val corr acc 50.13\n",
      "Ep 9 \titer 1117  \tloss 0.84996, train acc 76.04, train corr acc 52.58, val acc 74.75, val corr acc 50.00\n",
      "Ep 9 \titer 1118  \tloss 0.91038, train acc 72.92, train corr acc 47.47, val acc 74.55, val corr acc 49.62\n",
      "Ep 9 \titer 1119  \tloss 0.90114, train acc 75.52, train corr acc 56.07, val acc 74.81, val corr acc 50.13\n",
      "Ep 9 \titer 1120  \tloss 0.81905, train acc 75.00, train corr acc 50.52, val acc 74.75, val corr acc 50.00\n",
      "Ep 9 \titer 1121  \tloss 0.79849, train acc 79.17, train corr acc 57.45, val acc 74.87, val corr acc 50.25\n",
      "Ep 9 \titer 1122  \tloss 0.97586, train acc 72.40, train corr acc 48.54, val acc 75.06, val corr acc 50.63\n",
      "Ep 9 \titer 1123  \tloss 0.78970, train acc 81.25, train corr acc 60.00, val acc 75.83, val corr acc 52.14\n",
      "Ep 9 \titer 1124  \tloss 0.80204, train acc 76.56, train corr acc 53.12, val acc 75.13, val corr acc 50.76\n",
      "Ep 9 \titer 1125  \tloss 0.88409, train acc 75.52, train corr acc 53.00, val acc 75.19, val corr acc 50.88\n",
      "Ep 9 \titer 1126  \tloss 0.85618, train acc 76.56, train corr acc 53.61, val acc 75.00, val corr acc 50.50\n",
      "Ep 9 \titer 1127  \tloss 0.86021, train acc 78.12, train corr acc 57.14, val acc 75.32, val corr acc 51.13\n",
      "Ep 9 \titer 1128  \tloss 0.99057, train acc 72.40, train corr acc 51.38, val acc 75.25, val corr acc 51.01\n",
      "Ep 9 \titer 1129  \tloss 0.77189, train acc 78.12, train corr acc 51.72, val acc 75.19, val corr acc 50.88\n",
      "Ep 9 \titer 1130  \tloss 0.77573, train acc 76.04, train corr acc 48.31, val acc 75.25, val corr acc 51.01\n",
      "Ep 9 \titer 1131  \tloss 0.80359, train acc 81.77, train corr acc 64.29, val acc 74.55, val corr acc 49.62\n",
      "Ep 9 \titer 1132  \tloss 0.86240, train acc 77.60, train corr acc 57.43, val acc 75.32, val corr acc 51.13\n",
      "Ep 9 \titer 1133  \tloss 0.76282, train acc 78.65, train corr acc 52.33, val acc 75.25, val corr acc 51.01\n",
      "Ep 9 \titer 1134  \tloss 0.88088, train acc 77.08, train corr acc 54.64, val acc 74.24, val corr acc 48.99\n",
      "Ep 9 \titer 1135  \tloss 0.72443, train acc 80.21, train corr acc 60.00, val acc 75.00, val corr acc 50.50\n",
      "Ep 9 \titer 1136  \tloss 0.82306, train acc 78.65, train corr acc 54.44, val acc 75.19, val corr acc 50.88\n",
      "Ep 9 \titer 1137  \tloss 0.77882, train acc 76.04, train corr acc 52.08, val acc 74.68, val corr acc 49.87\n",
      "Ep 9 \titer 1138  \tloss 0.79324, train acc 76.04, train corr acc 53.06, val acc 74.17, val corr acc 48.87\n",
      "Ep 9 \titer 1139  \tloss 0.98931, train acc 72.40, train corr acc 50.47, val acc 75.06, val corr acc 50.63\n",
      "Ep 9 \titer 1140  \tloss 0.78615, train acc 79.17, train corr acc 57.45, val acc 74.87, val corr acc 50.25\n",
      "\n",
      "Epoch 10 of 10 took 82.187s\n",
      "  training loss:\t\t0.857266\n",
      "  training raw accuracy:\t\t76.85 %\n",
      "  training corrected acc:\t\t54.91 %\n",
      "  validation loss:\t\t1.159733\n",
      "  validation raw accuracy:\t\t74.49 %\n",
      "  validation corrected acc:\t\t49.50 % \n",
      "\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Training!\n",
    "train_losses, train_accs, train_corrected_accs, val_losses, val_accs, val_corrected_accs = model.train(train_data, val_data, train_loss_acc, compute_loss_acc, num_epochs=num_epochs, batchsize=batchsize, record_per_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Final results:\n",
      "  test loss:\t\t\t1.107700\n",
      "  test raw accuracy:\t\t74.59 %\n",
      "  test corrected accuracy:\t51.04 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_raw_acc, test_corrected_acc, pred_test = model.check_accuracy(test_data, compute_loss_acc, row_to_ast_id_map, dataset_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X AST IDs\n",
      "[[   1.   75.   29.   -1.   -1.   -1.]\n",
      " [  13.   74.    0.   -1.   -1.   -1.]\n",
      " [ 214.   14.   -1.   -1.   -1.   -1.]\n",
      " [   1.    8.    3.    0.   -1.   -1.]\n",
      " [   1.    3.    7.    9.   -1.   -1.]\n",
      " [   5.    1.    5.    0.   -1.   -1.]\n",
      " [  38.    1.   -1.   -1.   -1.   -1.]\n",
      " [   2.   27.    0.   -1.   -1.   -1.]\n",
      " [   1.    4.   11.    2.    0.   -1.]\n",
      " [ 498.    0.   -1.   -1.   -1.   -1.]]\n",
      "Truth AST IDs\n",
      "[[ 75.  29.  -1.  -1.  -1.  -1.]\n",
      " [ 74.   0.  -1.  -1.  -1.  -1.]\n",
      " [ 14.  -1.  -1.  -1.  -1.  -1.]\n",
      " [  8.   3.   0.  -1.  -1.  -1.]\n",
      " [  3.   7.   9.  -1.  -1.  -1.]\n",
      " [  1.   5.   0.  -1.  -1.  -1.]\n",
      " [  1.  -1.  -1.  -1.  -1.  -1.]\n",
      " [ 27.   0.  -1.  -1.  -1.  -1.]\n",
      " [  4.  11.   2.   0.  -1.  -1.]\n",
      " [  0.  -1.  -1.  -1.  -1.  -1.]]\n",
      "Predicted AST IDs\n",
      "[[   4.   66.    0.   -1.   -1.   -1.]\n",
      " [  74.   27.   -1.   -1.   -1.   -1.]\n",
      " [   0.   -1.   -1.   -1.   -1.   -1.]\n",
      " [   4.    3.    0.   -1.   -1.   -1.]\n",
      " [   4.   11.    0.   -1.   -1.   -1.]\n",
      " [   3.    0.    0.   -1.   -1.   -1.]\n",
      " [  20.    0.   -1.   -1.   -1.   -1.]\n",
      " [ 108.    0.   -1.   -1.   -1.   -1.]\n",
      " [   4.    3.    3.    0.   -1.   -1.]\n",
      " [  -1.   -1.   -1.   -1.   -1.   -1.]]\n"
     ]
    }
   ],
   "source": [
    "# Convert to AST IDs so we can look at the AST json files\n",
    "X_test_ast_ids, y_test_ast_ids = utils.convert_data_to_ast_ids(test_data, row_to_ast_id_map)\n",
    "pred_test_ast_ids = utils.convert_pred_to_ast_ids(pred_test, row_to_ast_id_map)\n",
    "print(\"X AST IDs\")\n",
    "print X_test_ast_ids[:10,:]\n",
    "print (\"Truth AST IDs\")\n",
    "print y_test_ast_ids[:10, :]\n",
    "print(\"Predicted AST IDs\")\n",
    "print pred_test_ast_ids[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAE+CAYAAABSoh3fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x+Ck1hilRlM0wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12d7fbbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss and the accuracies for both training and validation sets for each epoch\n",
    "visualize.plot_loss_acc(DATA_SET + '_train', train_losses, train_corrected_accs, val_corrected_accs, learning_rate, reg_strength, num_epochs, num_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
