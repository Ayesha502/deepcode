{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\"\"\"\n",
    "Implements an RNN on a synthetic data set, following the architecture \n",
    "described in \"Deep Knowledge Tracing\" by Chris Piech et al.\n",
    "The RNN implementation is based on min-char-rnn.py by Andrej Karpathy (@karpathy).\n",
    "BSD License\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "# Python libraries\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as Tensor\n",
    "import lasagne\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# allows plots to show inline in ipython notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# our own modules\n",
    "# from utils import *\n",
    "# from visualize import *\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "import visualize\n",
    "\n",
    "data_sets_map = {\n",
    "    'synth':\"syntheticDetailed/naive_c5_q50_s4000_v0.csv\",\n",
    "    'code_org' : \"data/hoc_1-9_binary_input.csv\"\n",
    "}\n",
    "\n",
    "# DATA_SET = 'code_org'\n",
    "# DATA_SZ = 500000\n",
    "DATA_SET = 'synth'\n",
    "\n",
    "# if DATA_SZ = -1, use entire data set\n",
    "DATA_SZ = 50000\n",
    "# DATA_SZ = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the data set\n",
    "# This function can be moved to utils.py\n",
    "data_array = np.array(list(csv.reader(open(data_sets_map[DATA_SET],\"rb\"),delimiter=','))).astype('int')\n",
    "if DATA_SZ != -1:\n",
    "    data_array = data_array[:DATA_SZ]\n",
    "    \n",
    "np.random.shuffle(data_array)\n",
    "num_samples = data_array.shape[0]\n",
    "num_problems = data_array.shape[1]\n",
    "\n",
    "# time steps is number of problems - 1 because we cannot predict on the last problem.\n",
    "num_timesteps = num_problems - 1 \n",
    "\n",
    "# Split data into train and test (half and half)\n",
    "\n",
    "train_data = data_array[0:7*num_samples/8,:]\n",
    "val_data =  data_array[7*num_samples/8: 15*num_samples/16 ,:]\n",
    "test_data = data_array[15*num_samples/16:num_samples,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "Train : percent correct 0.6078\n",
      "Train : percent correct 0.6088\n",
      "Val : percent correct 0.60136\n",
      "Test : percent correct 0.60024\n"
     ]
    }
   ],
   "source": [
    "# code to see how many percent is correct\n",
    "print (num_problems)\n",
    "# for prob in xrange(num_problems):\n",
    "#     print ('Train Prob {} : percent correct {}'.format(prob, np.mean(train_data[:,prob]) ))\n",
    "#     print ('Val Prob {} : percent correct {}'.format(prob, np.mean(val_data[:,prob]) ))\n",
    "#     print ('Test Prob {} : percent correct {}'.format(prob, np.mean(test_data[:,prob]) ))\n",
    "print ('Train : percent correct {}'.format( np.mean(np.concatenate((np.concatenate((train_data, val_data), axis=0),test_data), axis=0 ))))\n",
    "print ('Train : percent correct {}'.format( np.mean(train_data) ))\n",
    "print ('Val : percent correct {}'.format( np.mean(val_data) ))\n",
    "print ('Test : percent correct {}'.format( np.mean(test_data) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Vectorization done!\n",
      "(3500, 49, 100)\n",
      "(250, 49, 100)\n",
      "(250, 49, 100)\n"
     ]
    }
   ],
   "source": [
    "num_train = train_data.shape[0]\n",
    "num_test = test_data.shape[0]\n",
    "\n",
    "print('Vectorization...')\n",
    "X_train, next_problem_train, truth_train = utils.vectorize_data(train_data)\n",
    "X_val, next_problem_val, truth_val = utils.vectorize_data(val_data)\n",
    "X_test, next_problem_test, truth_test = utils.vectorize_data(test_data)\n",
    "print (\"Vectorization done!\")\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 200 # size of hidden layer of neurons\n",
    "learning_rate = 1e-2\n",
    "lr_decay = 1.0\n",
    "reg_strength = 0.0\n",
    "grad_clip = 10\n",
    "batchsize = 32\n",
    "num_epochs = 20\n",
    "theano.config.optimizer='fast_compile'\n",
    "theano.config.exception_verbosity='high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Compiling done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/theano/scan_module/scan.py:1019: Warning: In the strict mode, all neccessary shared variables must be passed as a part of non_sequences\n",
      "  'must be passed as a part of non_sequences', Warning)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create a model using Lasagne layering system. The layers are as follows:\n",
    " - Input layer: [n_sample, n_time, n_input] -> [n_sample, n_time, n_input]\n",
    " - LSTM layer: [n_sample, n_time, n_input] -> [n_sample, n_time, n_hidden]\n",
    " - Reshape layer: [n_sample, n_time, n_hidden] -> [n_sample*n_time, n_hidden]\n",
    " - Dense layer: [n_sample*n_time, n_hidden] -> [n_sample*n_time, n_output]\n",
    " - Output reshape layer: [n_sample*n_time, n_output] -> [n_sample, n_time, n_output]\n",
    "'''\n",
    "# add dropout\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, num_timesteps, num_problems * 2))\n",
    "l_lstm = lasagne.layers.LSTMLayer(\n",
    "    l_in, hidden_size, grad_clipping=grad_clip,\n",
    "    nonlinearity=lasagne.nonlinearities.tanh)\n",
    "l_reshape = lasagne.layers.ReshapeLayer(l_lstm, (-1, hidden_size))\n",
    "l_out = lasagne.layers.DenseLayer(l_reshape,\n",
    "    num_units=num_problems,\n",
    "    W = lasagne.init.Normal(),\n",
    "    nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "l_pred = lasagne.layers.ReshapeLayer(l_out, (-1, num_timesteps, num_problems))\n",
    "\n",
    "''' pred:for each student, a vector that gives probability of next question being answered correctly\n",
    "    y: for each student, a one-hot-encoding of shape (num_timesteps, num_problems), indicating which problem the student will do at the next timestep \n",
    "    truth: for each student, a vector that indicates for each timestep whether next problem was answered correctly\n",
    "'''\n",
    "pred = lasagne.layers.get_output(l_pred)\n",
    "next_problem = Tensor.tensor3('next_problem')\n",
    "truth = Tensor.imatrix(\"truth\")\n",
    "\n",
    "# pred_probs: shape(num_samples, num_timesteps)\n",
    "# we reduce the three-dimensional probability tensor to two dimensions\n",
    "# by only keeping the probabilities corresponding to the next problem\n",
    "# we don't care about the predicted probabilities for other problems\n",
    "pred_probs = Tensor.sum(pred * next_problem, axis = 2)\n",
    "# loss function\n",
    "loss = Tensor.nnet.binary_crossentropy(pred_probs, truth)\n",
    "\n",
    "# TODO: why do we use both cost and loss? (naming :D)\n",
    "# take average of loss per sample, makes loss more comparable when batch sizes change\n",
    "cost = loss.mean()\n",
    "\n",
    "# update function\n",
    "print(\"Computing updates ...\")\n",
    "all_params = lasagne.layers.get_all_params(l_pred)\n",
    "updates = lasagne.updates.adam(cost, all_params, learning_rate)\n",
    "\n",
    "# Function to compute accuracy:\n",
    "# if probability was > 0.5, we consider it a 1, and 0 otherwise.\n",
    "# binary_pred: for each student, a vector of length num_timesteps indicating whether the \n",
    "# probability of getting the next problem correct is larger than 0.5.\n",
    "binary_pred = Tensor.round(pred_probs)\n",
    "acc = Tensor.mean(Tensor.eq(binary_pred, truth))\n",
    "    \n",
    "# training function\n",
    "print(\"Compiling functions ...\")\n",
    "# training function, only returns loss\n",
    "train_fn = theano.function([l_in.input_var, next_problem, truth], cost, updates=updates, allow_input_downcast=True)\n",
    "# training function, returns loss and acc\n",
    "train_acc_fn = theano.function([l_in.input_var, next_problem, truth], [cost, acc], updates=updates, allow_input_downcast=True)\n",
    "\n",
    "compute_cost = theano.function([l_in.input_var, next_problem, truth], cost, allow_input_downcast=True)\n",
    "# computes loss and accuracy\n",
    "compute_cost_acc = theano.function([l_in.input_var, next_problem, truth], [cost, acc], allow_input_downcast=True)\n",
    "\n",
    "print(\"Compiling done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_val_loss_acc(X_val, next_problem_val, truth_val, batchsize):\n",
    "    # a full pass over the validation data:\n",
    "    val_err = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_batches = 0\n",
    "    for batch in utils.iterate_minibatches(X_val, next_problem_val, truth_val, batchsize, shuffle=False):\n",
    "        X_, next_problem_, truth_ = batch\n",
    "        err, acc = compute_cost_acc(X_, next_problem_, truth_)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "    val_loss = val_err/val_batches\n",
    "    val_acc = val_acc/val_batches * 100\n",
    "    return val_loss, val_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "  Epoch 0 \tbatch 1 \tloss 0.645171233615 \ttrain acc 62.63 \tval acc 63.23 \n",
      "  Epoch 0 \tbatch 2 \tloss 0.665062102826 \ttrain acc 61.10 \tval acc 63.22 \n",
      "  Epoch 0 \tbatch 3 \tloss 0.65959301944 \ttrain acc 62.37 \tval acc 63.29 \n",
      "  Epoch 0 \tbatch 4 \tloss 0.651853748839 \ttrain acc 62.88 \tval acc 63.66 \n",
      "  Epoch 0 \tbatch 5 \tloss 0.629320028142 \ttrain acc 67.03 \tval acc 63.58 \n",
      "  Epoch 0 \tbatch 6 \tloss 0.62523341794 \ttrain acc 65.82 \tval acc 63.58 \n",
      "  Epoch 0 \tbatch 7 \tloss 0.6553340689 \ttrain acc 60.84 \tval acc 63.99 \n",
      "  Epoch 0 \tbatch 8 \tloss 0.626001584317 \ttrain acc 64.86 \tval acc 65.11 \n",
      "  Epoch 0 \tbatch 9 \tloss 0.618713581125 \ttrain acc 66.33 \tval acc 65.67 \n",
      "  Epoch 0 \tbatch 10 \tloss 0.628097683024 \ttrain acc 65.18 \tval acc 65.79 \n",
      "  Epoch 0 \tbatch 11 \tloss 0.600305107393 \ttrain acc 68.94 \tval acc 65.36 \n",
      "  Epoch 0 \tbatch 12 \tloss 0.612026290801 \ttrain acc 64.80 \tval acc 66.65 \n",
      "  Epoch 0 \tbatch 13 \tloss 0.587630479851 \ttrain acc 69.71 \tval acc 65.72 \n",
      "  Epoch 0 \tbatch 14 \tloss 0.603962606756 \ttrain acc 66.45 \tval acc 65.90 \n",
      "  Epoch 0 \tbatch 15 \tloss 0.621101496254 \ttrain acc 65.82 \tval acc 65.74 \n",
      "  Epoch 0 \tbatch 16 \tloss 0.602003504871 \ttrain acc 67.35 \tval acc 66.07 \n",
      "  Epoch 0 \tbatch 17 \tloss 0.612184691959 \ttrain acc 65.69 \tval acc 66.43 \n",
      "  Epoch 0 \tbatch 18 \tloss 0.600523231327 \ttrain acc 65.94 \tval acc 66.09 \n",
      "  Epoch 0 \tbatch 19 \tloss 0.576493082001 \ttrain acc 67.67 \tval acc 67.30 \n",
      "  Epoch 0 \tbatch 20 \tloss 0.558058347245 \ttrain acc 70.28 \tval acc 67.42 \n",
      "  Epoch 0 \tbatch 21 \tloss 0.605404054508 \ttrain acc 64.41 \tval acc 67.51 \n",
      "  Epoch 0 \tbatch 22 \tloss 0.604797224354 \ttrain acc 67.09 \tval acc 68.02 \n",
      "  Epoch 0 \tbatch 23 \tloss 0.575494894087 \ttrain acc 69.26 \tval acc 67.81 \n",
      "  Epoch 0 \tbatch 24 \tloss 0.612103463769 \ttrain acc 64.22 \tval acc 67.81 \n",
      "  Epoch 0 \tbatch 25 \tloss 0.562648458729 \ttrain acc 70.15 \tval acc 68.29 \n",
      "  Epoch 0 \tbatch 26 \tloss 0.576095036774 \ttrain acc 68.81 \tval acc 68.12 \n",
      "  Epoch 0 \tbatch 27 \tloss 0.576568994653 \ttrain acc 69.58 \tval acc 68.34 \n",
      "  Epoch 0 \tbatch 28 \tloss 0.548364004417 \ttrain acc 72.45 \tval acc 68.77 \n",
      "  Epoch 0 \tbatch 29 \tloss 0.570761433738 \ttrain acc 69.13 \tval acc 68.14 \n",
      "  Epoch 0 \tbatch 30 \tloss 0.625092260941 \ttrain acc 64.99 \tval acc 67.93 \n",
      "  Epoch 0 \tbatch 31 \tloss 0.603691802787 \ttrain acc 65.43 \tval acc 67.93 \n",
      "  Epoch 0 \tbatch 32 \tloss 0.597663455811 \ttrain acc 66.77 \tval acc 68.39 \n",
      "  Epoch 0 \tbatch 33 \tloss 0.563860851934 \ttrain acc 70.03 \tval acc 68.71 \n",
      "  Epoch 0 \tbatch 34 \tloss 0.585043433829 \ttrain acc 68.43 \tval acc 69.17 \n",
      "  Epoch 0 \tbatch 35 \tloss 0.616751097946 \ttrain acc 65.43 \tval acc 68.90 \n",
      "  Epoch 0 \tbatch 36 \tloss 0.540183811013 \ttrain acc 71.75 \tval acc 68.88 \n",
      "  Epoch 0 \tbatch 37 \tloss 0.563337562353 \ttrain acc 70.28 \tval acc 69.31 \n",
      "  Epoch 0 \tbatch 38 \tloss 0.568168645746 \ttrain acc 69.32 \tval acc 69.58 \n",
      "  Epoch 0 \tbatch 39 \tloss 0.569395811955 \ttrain acc 69.39 \tval acc 69.70 \n",
      "  Epoch 0 \tbatch 40 \tloss 0.55134885753 \ttrain acc 71.17 \tval acc 70.11 \n",
      "  Epoch 0 \tbatch 41 \tloss 0.54993791769 \ttrain acc 70.47 \tval acc 70.23 \n",
      "  Epoch 0 \tbatch 42 \tloss 0.550826353242 \ttrain acc 70.92 \tval acc 70.30 \n",
      "  Epoch 0 \tbatch 43 \tloss 0.556415817742 \ttrain acc 70.15 \tval acc 70.11 \n",
      "  Epoch 0 \tbatch 44 \tloss 0.569905999007 \ttrain acc 69.20 \tval acc 69.62 \n",
      "  Epoch 0 \tbatch 45 \tloss 0.559205318952 \ttrain acc 70.41 \tval acc 69.88 \n",
      "  Epoch 0 \tbatch 46 \tloss 0.574314864117 \ttrain acc 68.94 \tval acc 70.31 \n",
      "  Epoch 0 \tbatch 47 \tloss 0.544533724712 \ttrain acc 70.79 \tval acc 70.39 \n",
      "  Epoch 0 \tbatch 48 \tloss 0.555464303279 \ttrain acc 70.98 \tval acc 70.14 \n",
      "  Epoch 0 \tbatch 49 \tloss 0.550910991519 \ttrain acc 72.07 \tval acc 70.00 \n",
      "  Epoch 0 \tbatch 50 \tloss 0.569926760243 \ttrain acc 68.43 \tval acc 70.20 \n",
      "  Epoch 0 \tbatch 51 \tloss 0.533506422026 \ttrain acc 72.07 \tval acc 70.71 \n",
      "  Epoch 0 \tbatch 52 \tloss 0.539653264668 \ttrain acc 70.79 \tval acc 70.64 \n",
      "  Epoch 0 \tbatch 53 \tloss 0.548604145665 \ttrain acc 70.54 \tval acc 70.59 \n",
      "  Epoch 0 \tbatch 54 \tloss 0.583717345905 \ttrain acc 66.65 \tval acc 70.44 \n",
      "  Epoch 0 \tbatch 55 \tloss 0.531735165396 \ttrain acc 72.07 \tval acc 70.49 \n",
      "  Epoch 0 \tbatch 56 \tloss 0.528251858307 \ttrain acc 71.05 \tval acc 70.63 \n",
      "  Epoch 0 \tbatch 57 \tloss 0.565620202893 \ttrain acc 69.32 \tval acc 70.99 \n",
      "  Epoch 0 \tbatch 58 \tloss 0.563025965961 \ttrain acc 69.90 \tval acc 71.06 \n",
      "  Epoch 0 \tbatch 59 \tloss 0.564327866113 \ttrain acc 71.88 \tval acc 71.10 \n",
      "  Epoch 0 \tbatch 60 \tloss 0.535161083097 \ttrain acc 72.32 \tval acc 71.03 \n",
      "  Epoch 0 \tbatch 61 \tloss 0.537707685952 \ttrain acc 71.49 \tval acc 71.48 \n",
      "  Epoch 0 \tbatch 62 \tloss 0.544110712019 \ttrain acc 71.30 \tval acc 71.76 \n",
      "  Epoch 0 \tbatch 63 \tloss 0.542339242374 \ttrain acc 71.24 \tval acc 71.47 \n",
      "  Epoch 0 \tbatch 64 \tloss 0.542693052655 \ttrain acc 71.30 \tval acc 71.67 \n",
      "  Epoch 0 \tbatch 65 \tloss 0.547141094819 \ttrain acc 71.68 \tval acc 72.18 \n",
      "  Epoch 0 \tbatch 66 \tloss 0.539355357292 \ttrain acc 72.13 \tval acc 72.15 \n",
      "  Epoch 0 \tbatch 67 \tloss 0.509401673808 \ttrain acc 73.15 \tval acc 72.04 \n",
      "  Epoch 0 \tbatch 68 \tloss 0.549302639583 \ttrain acc 70.98 \tval acc 71.86 \n",
      "  Epoch 0 \tbatch 69 \tloss 0.537433673661 \ttrain acc 72.13 \tval acc 71.74 \n",
      "  Epoch 0 \tbatch 70 \tloss 0.557053690905 \ttrain acc 70.34 \tval acc 71.97 \n",
      "  Epoch 0 \tbatch 71 \tloss 0.529092688724 \ttrain acc 73.34 \tval acc 71.88 \n",
      "  Epoch 0 \tbatch 72 \tloss 0.519874878144 \ttrain acc 73.72 \tval acc 71.76 \n",
      "  Epoch 0 \tbatch 73 \tloss 0.55497082631 \ttrain acc 70.66 \tval acc 71.46 \n",
      "  Epoch 0 \tbatch 74 \tloss 0.548292729485 \ttrain acc 71.17 \tval acc 72.03 \n",
      "  Epoch 0 \tbatch 75 \tloss 0.544706500996 \ttrain acc 71.75 \tval acc 71.99 \n",
      "  Epoch 0 \tbatch 76 \tloss 0.551644610555 \ttrain acc 70.09 \tval acc 71.89 \n",
      "  Epoch 0 \tbatch 77 \tloss 0.537872455767 \ttrain acc 70.66 \tval acc 72.11 \n",
      "  Epoch 0 \tbatch 78 \tloss 0.5674878457 \ttrain acc 71.17 \tval acc 71.83 \n",
      "  Epoch 0 \tbatch 79 \tloss 0.536982418666 \ttrain acc 71.17 \tval acc 71.97 \n",
      "  Epoch 0 \tbatch 80 \tloss 0.548493011622 \ttrain acc 71.05 \tval acc 72.08 \n",
      "  Epoch 0 \tbatch 81 \tloss 0.522114951753 \ttrain acc 73.28 \tval acc 72.04 \n",
      "  Epoch 0 \tbatch 82 \tloss 0.529323594603 \ttrain acc 72.77 \tval acc 72.36 \n",
      "  Epoch 0 \tbatch 83 \tloss 0.530784980686 \ttrain acc 73.41 \tval acc 72.46 \n",
      "  Epoch 0 \tbatch 84 \tloss 0.526451852678 \ttrain acc 73.53 \tval acc 72.73 \n",
      "  Epoch 0 \tbatch 85 \tloss 0.562525738237 \ttrain acc 69.77 \tval acc 72.80 \n",
      "  Epoch 0 \tbatch 86 \tloss 0.527839572852 \ttrain acc 72.45 \tval acc 72.94 \n",
      "  Epoch 0 \tbatch 87 \tloss 0.533674077312 \ttrain acc 71.30 \tval acc 72.85 \n",
      "  Epoch 0 \tbatch 88 \tloss 0.507270084834 \ttrain acc 74.87 \tval acc 72.83 \n",
      "  Epoch 0 \tbatch 89 \tloss 0.504194948507 \ttrain acc 73.92 \tval acc 72.57 \n",
      "  Epoch 0 \tbatch 90 \tloss 0.573236205877 \ttrain acc 69.13 \tval acc 72.67 \n",
      "  Epoch 0 \tbatch 91 \tloss 0.505670798317 \ttrain acc 74.36 \tval acc 72.25 \n",
      "  Epoch 0 \tbatch 92 \tloss 0.538199432801 \ttrain acc 73.15 \tval acc 72.59 \n",
      "  Epoch 0 \tbatch 93 \tloss 0.538116312843 \ttrain acc 70.47 \tval acc 72.52 \n",
      "  Epoch 0 \tbatch 94 \tloss 0.535799112328 \ttrain acc 72.90 \tval acc 72.83 \n",
      "  Epoch 0 \tbatch 95 \tloss 0.548928713913 \ttrain acc 71.43 \tval acc 72.65 \n",
      "  Epoch 0 \tbatch 96 \tloss 0.54703770366 \ttrain acc 70.92 \tval acc 72.35 \n",
      "  Epoch 0 \tbatch 97 \tloss 0.556180059997 \ttrain acc 70.34 \tval acc 72.47 \n",
      "  Epoch 0 \tbatch 98 \tloss 0.509177418364 \ttrain acc 74.11 \tval acc 72.48 \n",
      "  Epoch 0 \tbatch 99 \tloss 0.510623216017 \ttrain acc 73.85 \tval acc 72.67 \n",
      "  Epoch 0 \tbatch 100 \tloss 0.506731304632 \ttrain acc 73.92 \tval acc 72.57 \n",
      "  Epoch 0 \tbatch 101 \tloss 0.547489407603 \ttrain acc 71.05 \tval acc 72.56 \n",
      "  Epoch 0 \tbatch 102 \tloss 0.541246352324 \ttrain acc 71.75 \tval acc 72.19 \n",
      "  Epoch 0 \tbatch 103 \tloss 0.509935935216 \ttrain acc 73.34 \tval acc 72.40 \n",
      "  Epoch 0 \tbatch 104 \tloss 0.531648759922 \ttrain acc 72.19 \tval acc 72.54 \n",
      "  Epoch 0 \tbatch 105 \tloss 0.518780062091 \ttrain acc 74.04 \tval acc 72.96 \n",
      "  Epoch 0 \tbatch 106 \tloss 0.564066038923 \ttrain acc 70.41 \tval acc 73.05 \n",
      "  Epoch 0 \tbatch 107 \tloss 0.494546114869 \ttrain acc 74.94 \tval acc 72.68 \n",
      "  Epoch 0 \tbatch 108 \tloss 0.509551041365 \ttrain acc 73.47 \tval acc 72.65 \n",
      "  Epoch 0 \tbatch 109 \tloss 0.520046648036 \ttrain acc 72.19 \tval acc 72.39 \n",
      "Epoch 1 of 20 took 110.077s\n",
      "  training loss:\t\t0.562088\n",
      "  training accuracy:\t\t69.93 %\n",
      "  validation loss:\t\t0.528435\n",
      "  validation accuracy:\t\t72.39 %\n",
      "  Epoch 1 \tbatch 1 \tloss 0.543396598544 \ttrain acc 71.94 \tval acc 72.35 \n",
      "  Epoch 1 \tbatch 2 \tloss 0.542741704784 \ttrain acc 71.56 \tval acc 72.52 \n",
      "  Epoch 1 \tbatch 3 \tloss 0.537770486479 \ttrain acc 71.75 \tval acc 72.85 \n",
      "  Epoch 1 \tbatch 4 \tloss 0.543560540838 \ttrain acc 71.24 \tval acc 73.24 \n",
      "  Epoch 1 \tbatch 5 \tloss 0.492749824411 \ttrain acc 75.89 \tval acc 73.25 \n",
      "  Epoch 1 \tbatch 6 \tloss 0.52624893522 \ttrain acc 72.64 \tval acc 73.19 \n",
      "  Epoch 1 \tbatch 7 \tloss 0.544163544175 \ttrain acc 71.49 \tval acc 73.26 \n",
      "  Epoch 1 \tbatch 8 \tloss 0.541357800772 \ttrain acc 71.88 \tval acc 73.23 \n",
      "  Epoch 1 \tbatch 9 \tloss 0.530271962467 \ttrain acc 73.47 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 10 \tloss 0.525274487679 \ttrain acc 71.24 \tval acc 73.29 \n",
      "  Epoch 1 \tbatch 11 \tloss 0.511724296969 \ttrain acc 74.11 \tval acc 73.07 \n",
      "  Epoch 1 \tbatch 12 \tloss 0.501699598188 \ttrain acc 74.17 \tval acc 73.17 \n",
      "  Epoch 1 \tbatch 13 \tloss 0.503052205484 \ttrain acc 74.36 \tval acc 73.04 \n",
      "  Epoch 1 \tbatch 14 \tloss 0.526584840685 \ttrain acc 74.11 \tval acc 72.77 \n",
      "  Epoch 1 \tbatch 15 \tloss 0.553668510979 \ttrain acc 71.81 \tval acc 72.72 \n",
      "  Epoch 1 \tbatch 16 \tloss 0.521022843698 \ttrain acc 74.23 \tval acc 72.62 \n",
      "  Epoch 1 \tbatch 17 \tloss 0.560511461381 \ttrain acc 70.41 \tval acc 72.62 \n",
      "  Epoch 1 \tbatch 18 \tloss 0.533075840315 \ttrain acc 71.17 \tval acc 72.63 \n",
      "  Epoch 1 \tbatch 19 \tloss 0.507572868959 \ttrain acc 74.11 \tval acc 72.83 \n",
      "  Epoch 1 \tbatch 20 \tloss 0.50046344328 \ttrain acc 74.55 \tval acc 73.13 \n",
      "  Epoch 1 \tbatch 21 \tloss 0.54534530291 \ttrain acc 70.79 \tval acc 73.45 \n",
      "  Epoch 1 \tbatch 22 \tloss 0.537905059362 \ttrain acc 72.51 \tval acc 73.34 \n",
      "  Epoch 1 \tbatch 23 \tloss 0.499871641668 \ttrain acc 75.32 \tval acc 73.48 \n",
      "  Epoch 1 \tbatch 24 \tloss 0.554144916477 \ttrain acc 69.71 \tval acc 73.72 \n",
      "  Epoch 1 \tbatch 25 \tloss 0.508434172964 \ttrain acc 73.72 \tval acc 73.71 \n",
      "  Epoch 1 \tbatch 26 \tloss 0.500082388254 \ttrain acc 75.32 \tval acc 73.64 \n",
      "  Epoch 1 \tbatch 27 \tloss 0.534154367523 \ttrain acc 71.81 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 28 \tloss 0.482889482096 \ttrain acc 77.49 \tval acc 73.28 \n",
      "  Epoch 1 \tbatch 29 \tloss 0.528045667818 \ttrain acc 72.64 \tval acc 73.34 \n",
      "  Epoch 1 \tbatch 30 \tloss 0.560728624971 \ttrain acc 70.85 \tval acc 73.57 \n",
      "  Epoch 1 \tbatch 31 \tloss 0.526894364096 \ttrain acc 72.13 \tval acc 73.47 \n",
      "  Epoch 1 \tbatch 32 \tloss 0.53526106466 \ttrain acc 71.88 \tval acc 73.13 \n",
      "  Epoch 1 \tbatch 33 \tloss 0.507293843088 \ttrain acc 74.74 \tval acc 73.47 \n",
      "  Epoch 1 \tbatch 34 \tloss 0.51256866868 \ttrain acc 74.36 \tval acc 73.38 \n",
      "  Epoch 1 \tbatch 35 \tloss 0.568386559535 \ttrain acc 68.37 \tval acc 73.49 \n",
      "  Epoch 1 \tbatch 36 \tloss 0.486156942737 \ttrain acc 76.28 \tval acc 73.45 \n",
      "  Epoch 1 \tbatch 37 \tloss 0.514247224373 \ttrain acc 75.45 \tval acc 73.29 \n",
      "  Epoch 1 \tbatch 38 \tloss 0.520175750011 \ttrain acc 74.04 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 39 \tloss 0.538450501535 \ttrain acc 71.56 \tval acc 73.20 \n",
      "  Epoch 1 \tbatch 40 \tloss 0.493166738171 \ttrain acc 75.64 \tval acc 73.27 \n",
      "  Epoch 1 \tbatch 41 \tloss 0.50028119346 \ttrain acc 75.26 \tval acc 73.34 \n",
      "  Epoch 1 \tbatch 42 \tloss 0.51233405031 \ttrain acc 73.72 \tval acc 73.49 \n",
      "  Epoch 1 \tbatch 43 \tloss 0.511236323054 \ttrain acc 73.53 \tval acc 73.43 \n",
      "  Epoch 1 \tbatch 44 \tloss 0.542531916474 \ttrain acc 70.98 \tval acc 73.11 \n",
      "  Epoch 1 \tbatch 45 \tloss 0.517661074275 \ttrain acc 74.23 \tval acc 73.24 \n",
      "  Epoch 1 \tbatch 46 \tloss 0.518126337709 \ttrain acc 73.15 \tval acc 73.19 \n",
      "  Epoch 1 \tbatch 47 \tloss 0.519091530548 \ttrain acc 73.85 \tval acc 73.32 \n",
      "  Epoch 1 \tbatch 48 \tloss 0.52921236205 \ttrain acc 72.70 \tval acc 73.24 \n",
      "  Epoch 1 \tbatch 49 \tloss 0.512459362577 \ttrain acc 75.13 \tval acc 73.14 \n",
      "  Epoch 1 \tbatch 50 \tloss 0.533518956749 \ttrain acc 71.17 \tval acc 73.29 \n",
      "  Epoch 1 \tbatch 51 \tloss 0.509241425013 \ttrain acc 74.55 \tval acc 73.44 \n",
      "  Epoch 1 \tbatch 52 \tloss 0.496785855346 \ttrain acc 76.08 \tval acc 73.60 \n",
      "  Epoch 1 \tbatch 53 \tloss 0.534033103129 \ttrain acc 72.64 \tval acc 73.41 \n",
      "  Epoch 1 \tbatch 54 \tloss 0.545852220525 \ttrain acc 71.36 \tval acc 73.51 \n",
      "  Epoch 1 \tbatch 55 \tloss 0.509875110031 \ttrain acc 74.30 \tval acc 73.79 \n",
      "  Epoch 1 \tbatch 56 \tloss 0.494389064284 \ttrain acc 74.49 \tval acc 73.77 \n",
      "  Epoch 1 \tbatch 57 \tloss 0.542934299218 \ttrain acc 72.26 \tval acc 73.86 \n",
      "  Epoch 1 \tbatch 58 \tloss 0.530917909495 \ttrain acc 72.64 \tval acc 73.82 \n",
      "  Epoch 1 \tbatch 59 \tloss 0.534042741812 \ttrain acc 72.77 \tval acc 73.94 \n",
      "  Epoch 1 \tbatch 60 \tloss 0.508136223728 \ttrain acc 75.26 \tval acc 73.77 \n",
      "  Epoch 1 \tbatch 61 \tloss 0.496946477377 \ttrain acc 74.55 \tval acc 73.50 \n",
      "  Epoch 1 \tbatch 62 \tloss 0.509337551197 \ttrain acc 74.23 \tval acc 73.52 \n",
      "  Epoch 1 \tbatch 63 \tloss 0.522202230301 \ttrain acc 72.83 \tval acc 73.24 \n",
      "  Epoch 1 \tbatch 64 \tloss 0.510675028077 \ttrain acc 73.79 \tval acc 73.11 \n",
      "  Epoch 1 \tbatch 65 \tloss 0.525027104398 \ttrain acc 73.34 \tval acc 73.32 \n",
      "  Epoch 1 \tbatch 66 \tloss 0.512081451182 \ttrain acc 73.92 \tval acc 73.57 \n",
      "  Epoch 1 \tbatch 67 \tloss 0.476855538115 \ttrain acc 76.15 \tval acc 73.67 \n",
      "  Epoch 1 \tbatch 68 \tloss 0.531561310428 \ttrain acc 72.19 \tval acc 73.78 \n",
      "  Epoch 1 \tbatch 69 \tloss 0.51192802542 \ttrain acc 75.19 \tval acc 73.64 \n",
      "  Epoch 1 \tbatch 70 \tloss 0.531430873606 \ttrain acc 71.49 \tval acc 73.55 \n",
      "  Epoch 1 \tbatch 71 \tloss 0.508649308946 \ttrain acc 74.74 \tval acc 73.84 \n",
      "  Epoch 1 \tbatch 72 \tloss 0.499780219482 \ttrain acc 75.26 \tval acc 73.75 \n",
      "  Epoch 1 \tbatch 73 \tloss 0.520884609023 \ttrain acc 73.02 \tval acc 73.73 \n",
      "  Epoch 1 \tbatch 74 \tloss 0.522478574736 \ttrain acc 72.51 \tval acc 73.64 \n",
      "  Epoch 1 \tbatch 75 \tloss 0.524547180092 \ttrain acc 73.41 \tval acc 73.69 \n",
      "  Epoch 1 \tbatch 76 \tloss 0.537446088051 \ttrain acc 72.19 \tval acc 73.69 \n",
      "  Epoch 1 \tbatch 77 \tloss 0.515789503631 \ttrain acc 72.70 \tval acc 73.57 \n",
      "  Epoch 1 \tbatch 78 \tloss 0.547270020729 \ttrain acc 72.58 \tval acc 73.34 \n",
      "  Epoch 1 \tbatch 79 \tloss 0.514516042159 \ttrain acc 74.36 \tval acc 73.48 \n",
      "  Epoch 1 \tbatch 80 \tloss 0.522054004207 \ttrain acc 73.34 \tval acc 73.62 \n",
      "  Epoch 1 \tbatch 81 \tloss 0.502512981893 \ttrain acc 74.74 \tval acc 73.78 \n",
      "  Epoch 1 \tbatch 82 \tloss 0.510501351486 \ttrain acc 73.66 \tval acc 73.83 \n",
      "  Epoch 1 \tbatch 83 \tloss 0.505895118798 \ttrain acc 74.43 \tval acc 73.86 \n",
      "  Epoch 1 \tbatch 84 \tloss 0.520398842528 \ttrain acc 74.43 \tval acc 73.83 \n",
      "  Epoch 1 \tbatch 85 \tloss 0.538456939733 \ttrain acc 71.62 \tval acc 73.72 \n",
      "  Epoch 1 \tbatch 86 \tloss 0.503644078677 \ttrain acc 74.62 \tval acc 73.78 \n",
      "  Epoch 1 \tbatch 87 \tloss 0.513123390933 \ttrain acc 72.77 \tval acc 73.83 \n",
      "  Epoch 1 \tbatch 88 \tloss 0.493669133832 \ttrain acc 76.02 \tval acc 74.13 \n",
      "  Epoch 1 \tbatch 89 \tloss 0.482373157401 \ttrain acc 75.70 \tval acc 74.03 \n",
      "  Epoch 1 \tbatch 90 \tloss 0.546085909974 \ttrain acc 71.81 \tval acc 73.84 \n",
      "  Epoch 1 \tbatch 91 \tloss 0.484595488775 \ttrain acc 75.64 \tval acc 73.74 \n",
      "  Epoch 1 \tbatch 92 \tloss 0.518235160342 \ttrain acc 75.00 \tval acc 73.56 \n",
      "  Epoch 1 \tbatch 93 \tloss 0.52680907443 \ttrain acc 71.75 \tval acc 73.48 \n",
      "  Epoch 1 \tbatch 94 \tloss 0.520453964797 \ttrain acc 74.55 \tval acc 73.55 \n",
      "  Epoch 1 \tbatch 95 \tloss 0.540219451487 \ttrain acc 72.39 \tval acc 73.40 \n",
      "  Epoch 1 \tbatch 96 \tloss 0.533566051047 \ttrain acc 72.26 \tval acc 73.29 \n",
      "  Epoch 1 \tbatch 97 \tloss 0.540957205369 \ttrain acc 71.49 \tval acc 73.43 \n",
      "  Epoch 1 \tbatch 98 \tloss 0.49500103283 \ttrain acc 76.53 \tval acc 73.55 \n",
      "  Epoch 1 \tbatch 99 \tloss 0.49263543232 \ttrain acc 74.94 \tval acc 73.44 \n",
      "  Epoch 1 \tbatch 100 \tloss 0.497853582633 \ttrain acc 74.55 \tval acc 73.32 \n",
      "  Epoch 1 \tbatch 101 \tloss 0.532942211257 \ttrain acc 71.68 \tval acc 73.37 \n",
      "  Epoch 1 \tbatch 102 \tloss 0.535658235676 \ttrain acc 72.70 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 103 \tloss 0.502907841882 \ttrain acc 74.30 \tval acc 73.35 \n",
      "  Epoch 1 \tbatch 104 \tloss 0.527072173497 \ttrain acc 73.34 \tval acc 73.58 \n",
      "  Epoch 1 \tbatch 105 \tloss 0.501888299925 \ttrain acc 74.68 \tval acc 73.42 \n",
      "  Epoch 1 \tbatch 106 \tloss 0.543658245476 \ttrain acc 71.56 \tval acc 73.37 \n",
      "  Epoch 1 \tbatch 107 \tloss 0.477803784563 \ttrain acc 76.66 \tval acc 73.31 \n",
      "  Epoch 1 \tbatch 108 \tloss 0.497319138069 \ttrain acc 75.89 \tval acc 73.12 \n",
      "  Epoch 1 \tbatch 109 \tloss 0.507787490857 \ttrain acc 73.41 \tval acc 73.19 \n",
      "Epoch 2 of 20 took 107.132s\n",
      "  training loss:\t\t0.519626\n",
      "  training accuracy:\t\t73.45 %\n",
      "  validation loss:\t\t0.518669\n",
      "  validation accuracy:\t\t73.19 %\n",
      "  Epoch 2 \tbatch 1 \tloss 0.526246151109 \ttrain acc 73.09 \tval acc 73.56 \n",
      "  Epoch 2 \tbatch 2 \tloss 0.530939764661 \ttrain acc 72.77 \tval acc 73.86 \n",
      "  Epoch 2 \tbatch 3 \tloss 0.517099009887 \ttrain acc 74.68 \tval acc 74.12 \n",
      "  Epoch 2 \tbatch 4 \tloss 0.527214613163 \ttrain acc 73.92 \tval acc 74.05 \n",
      "  Epoch 2 \tbatch 5 \tloss 0.479517883743 \ttrain acc 76.28 \tval acc 73.97 \n",
      "  Epoch 2 \tbatch 6 \tloss 0.526843539415 \ttrain acc 72.51 \tval acc 74.06 \n",
      "  Epoch 2 \tbatch 7 \tloss 0.528401293269 \ttrain acc 72.77 \tval acc 74.20 \n",
      "  Epoch 2 \tbatch 8 \tloss 0.536733225577 \ttrain acc 72.64 \tval acc 74.14 \n",
      "  Epoch 2 \tbatch 9 \tloss 0.517786968521 \ttrain acc 74.43 \tval acc 74.13 \n",
      "  Epoch 2 \tbatch 10 \tloss 0.512207334549 \ttrain acc 72.83 \tval acc 74.02 \n",
      "  Epoch 2 \tbatch 11 \tloss 0.494073461387 \ttrain acc 74.55 \tval acc 73.82 \n",
      "  Epoch 2 \tbatch 12 \tloss 0.486681803849 \ttrain acc 74.43 \tval acc 73.76 \n",
      "  Epoch 2 \tbatch 13 \tloss 0.496384555475 \ttrain acc 74.74 \tval acc 73.76 \n",
      "  Epoch 2 \tbatch 14 \tloss 0.520591821845 \ttrain acc 73.85 \tval acc 73.50 \n",
      "  Epoch 2 \tbatch 15 \tloss 0.542694984954 \ttrain acc 72.39 \tval acc 73.47 \n",
      "  Epoch 2 \tbatch 16 \tloss 0.50082368022 \ttrain acc 75.00 \tval acc 73.35 \n",
      "  Epoch 2 \tbatch 17 \tloss 0.553497889142 \ttrain acc 71.24 \tval acc 73.34 \n",
      "  Epoch 2 \tbatch 18 \tloss 0.521174314881 \ttrain acc 72.26 \tval acc 73.48 \n",
      "  Epoch 2 \tbatch 19 \tloss 0.492686010102 \ttrain acc 75.57 \tval acc 73.55 \n",
      "  Epoch 2 \tbatch 20 \tloss 0.495262945951 \ttrain acc 75.32 \tval acc 73.51 \n",
      "  Epoch 2 \tbatch 21 \tloss 0.529582577794 \ttrain acc 72.51 \tval acc 73.65 \n",
      "  Epoch 2 \tbatch 22 \tloss 0.520963889771 \ttrain acc 73.79 \tval acc 73.81 \n",
      "  Epoch 2 \tbatch 23 \tloss 0.490611960295 \ttrain acc 73.85 \tval acc 73.96 \n",
      "  Epoch 2 \tbatch 24 \tloss 0.548142111903 \ttrain acc 70.98 \tval acc 74.19 \n",
      "  Epoch 2 \tbatch 25 \tloss 0.499968151794 \ttrain acc 73.98 \tval acc 74.11 \n",
      "  Epoch 2 \tbatch 26 \tloss 0.493069152925 \ttrain acc 76.34 \tval acc 74.06 \n",
      "  Epoch 2 \tbatch 27 \tloss 0.523970911909 \ttrain acc 73.21 \tval acc 73.93 \n",
      "  Epoch 2 \tbatch 28 \tloss 0.467631756025 \ttrain acc 77.49 \tval acc 73.99 \n",
      "  Epoch 2 \tbatch 29 \tloss 0.515223263975 \ttrain acc 74.23 \tval acc 74.09 \n",
      "  Epoch 2 \tbatch 30 \tloss 0.549563557954 \ttrain acc 72.00 \tval acc 73.96 \n",
      "  Epoch 2 \tbatch 31 \tloss 0.527666484412 \ttrain acc 73.09 \tval acc 73.88 \n",
      "  Epoch 2 \tbatch 32 \tloss 0.534110233568 \ttrain acc 72.19 \tval acc 73.78 \n",
      "  Epoch 2 \tbatch 33 \tloss 0.495229924297 \ttrain acc 75.51 \tval acc 73.91 \n",
      "  Epoch 2 \tbatch 34 \tloss 0.501330707562 \ttrain acc 75.19 \tval acc 73.85 \n",
      "  Epoch 2 \tbatch 35 \tloss 0.555117310646 \ttrain acc 69.39 \tval acc 73.96 \n",
      "  Epoch 2 \tbatch 36 \tloss 0.488269073517 \ttrain acc 75.38 \tval acc 73.82 \n",
      "  Epoch 2 \tbatch 37 \tloss 0.508448454705 \ttrain acc 76.21 \tval acc 73.76 \n",
      "  Epoch 2 \tbatch 38 \tloss 0.507776555381 \ttrain acc 74.94 \tval acc 73.57 \n",
      "  Epoch 2 \tbatch 39 \tloss 0.523723883118 \ttrain acc 72.90 \tval acc 73.56 \n",
      "  Epoch 2 \tbatch 40 \tloss 0.47662287358 \ttrain acc 76.02 \tval acc 73.50 \n",
      "  Epoch 2 \tbatch 41 \tloss 0.481270107451 \ttrain acc 76.40 \tval acc 73.45 \n",
      "  Epoch 2 \tbatch 42 \tloss 0.503476825204 \ttrain acc 74.17 \tval acc 73.49 \n",
      "  Epoch 2 \tbatch 43 \tloss 0.498121730368 \ttrain acc 74.74 \tval acc 73.59 \n",
      "  Epoch 2 \tbatch 44 \tloss 0.541396069307 \ttrain acc 71.30 \tval acc 73.73 \n",
      "  Epoch 2 \tbatch 45 \tloss 0.511724428369 \ttrain acc 74.36 \tval acc 73.77 \n",
      "  Epoch 2 \tbatch 46 \tloss 0.503288243785 \ttrain acc 74.62 \tval acc 73.85 \n",
      "  Epoch 2 \tbatch 47 \tloss 0.502317462735 \ttrain acc 75.38 \tval acc 73.97 \n",
      "  Epoch 2 \tbatch 48 \tloss 0.502234340867 \ttrain acc 74.30 \tval acc 73.90 \n",
      "  Epoch 2 \tbatch 49 \tloss 0.50114736948 \ttrain acc 75.57 \tval acc 73.98 \n",
      "  Epoch 2 \tbatch 50 \tloss 0.524074489316 \ttrain acc 72.26 \tval acc 73.95 \n",
      "  Epoch 2 \tbatch 51 \tloss 0.496032380255 \ttrain acc 75.51 \tval acc 73.94 \n",
      "  Epoch 2 \tbatch 52 \tloss 0.492330275173 \ttrain acc 74.87 \tval acc 74.04 \n",
      "  Epoch 2 \tbatch 53 \tloss 0.529819041666 \ttrain acc 73.53 \tval acc 74.17 \n",
      "  Epoch 2 \tbatch 54 \tloss 0.539793797527 \ttrain acc 70.85 \tval acc 73.88 \n",
      "  Epoch 2 \tbatch 55 \tloss 0.493108231398 \ttrain acc 75.00 \tval acc 73.93 \n",
      "  Epoch 2 \tbatch 56 \tloss 0.487863927306 \ttrain acc 75.38 \tval acc 73.96 \n",
      "  Epoch 2 \tbatch 57 \tloss 0.533095124027 \ttrain acc 71.94 \tval acc 73.91 \n",
      "  Epoch 2 \tbatch 58 \tloss 0.517287091543 \ttrain acc 73.34 \tval acc 74.01 \n",
      "  Epoch 2 \tbatch 59 \tloss 0.524596392524 \ttrain acc 72.96 \tval acc 73.99 \n",
      "  Epoch 2 \tbatch 60 \tloss 0.504084531613 \ttrain acc 75.45 \tval acc 73.97 \n",
      "  Epoch 2 \tbatch 61 \tloss 0.488468333077 \ttrain acc 74.55 \tval acc 74.02 \n",
      "  Epoch 2 \tbatch 62 \tloss 0.501179641834 \ttrain acc 74.11 \tval acc 74.03 \n",
      "  Epoch 2 \tbatch 63 \tloss 0.5138215857 \ttrain acc 73.53 \tval acc 73.82 \n",
      "  Epoch 2 \tbatch 64 \tloss 0.508364971334 \ttrain acc 74.36 \tval acc 73.76 \n",
      "  Epoch 2 \tbatch 65 \tloss 0.515003763214 \ttrain acc 73.92 \tval acc 73.79 \n",
      "  Epoch 2 \tbatch 66 \tloss 0.499214609883 \ttrain acc 74.87 \tval acc 74.06 \n",
      "  Epoch 2 \tbatch 67 \tloss 0.467208393161 \ttrain acc 77.17 \tval acc 74.03 \n",
      "  Epoch 2 \tbatch 68 \tloss 0.524258316919 \ttrain acc 72.13 \tval acc 74.13 \n",
      "  Epoch 2 \tbatch 69 \tloss 0.506851116734 \ttrain acc 75.51 \tval acc 74.13 \n",
      "  Epoch 2 \tbatch 70 \tloss 0.522032468868 \ttrain acc 72.32 \tval acc 74.07 \n",
      "  Epoch 2 \tbatch 71 \tloss 0.510379168459 \ttrain acc 74.49 \tval acc 74.10 \n",
      "  Epoch 2 \tbatch 72 \tloss 0.491404583658 \ttrain acc 76.28 \tval acc 73.97 \n",
      "  Epoch 2 \tbatch 73 \tloss 0.510581858276 \ttrain acc 73.09 \tval acc 73.93 \n",
      "  Epoch 2 \tbatch 74 \tloss 0.521755299111 \ttrain acc 72.77 \tval acc 73.89 \n",
      "  Epoch 2 \tbatch 75 \tloss 0.520695439897 \ttrain acc 74.11 \tval acc 73.98 \n",
      "  Epoch 2 \tbatch 76 \tloss 0.533616560524 \ttrain acc 72.83 \tval acc 73.96 \n",
      "  Epoch 2 \tbatch 77 \tloss 0.521118123096 \ttrain acc 72.90 \tval acc 74.13 \n",
      "  Epoch 2 \tbatch 78 \tloss 0.53815001201 \ttrain acc 73.53 \tval acc 74.12 \n",
      "  Epoch 2 \tbatch 79 \tloss 0.503250940954 \ttrain acc 74.30 \tval acc 73.93 \n",
      "  Epoch 2 \tbatch 80 \tloss 0.520610939022 \ttrain acc 72.45 \tval acc 73.82 \n",
      "  Epoch 2 \tbatch 81 \tloss 0.505708529888 \ttrain acc 74.55 \tval acc 73.76 \n",
      "  Epoch 2 \tbatch 82 \tloss 0.497264601439 \ttrain acc 73.47 \tval acc 73.62 \n",
      "  Epoch 2 \tbatch 83 \tloss 0.49922194533 \ttrain acc 74.74 \tval acc 73.60 \n",
      "  Epoch 2 \tbatch 84 \tloss 0.511873092458 \ttrain acc 73.85 \tval acc 73.58 \n",
      "  Epoch 2 \tbatch 85 \tloss 0.537423181361 \ttrain acc 72.19 \tval acc 73.89 \n",
      "  Epoch 2 \tbatch 86 \tloss 0.496068009282 \ttrain acc 75.19 \tval acc 73.85 \n",
      "  Epoch 2 \tbatch 87 \tloss 0.508360081287 \ttrain acc 73.79 \tval acc 73.81 \n",
      "  Epoch 2 \tbatch 88 \tloss 0.486536632961 \ttrain acc 76.59 \tval acc 73.76 \n",
      "  Epoch 2 \tbatch 89 \tloss 0.478174637574 \ttrain acc 75.89 \tval acc 73.82 \n",
      "  Epoch 2 \tbatch 90 \tloss 0.543563446684 \ttrain acc 73.02 \tval acc 73.55 \n",
      "  Epoch 2 \tbatch 91 \tloss 0.483209192493 \ttrain acc 75.19 \tval acc 73.31 \n",
      "  Epoch 2 \tbatch 92 \tloss 0.518746761313 \ttrain acc 75.19 \tval acc 73.56 \n",
      "  Epoch 2 \tbatch 93 \tloss 0.52041754414 \ttrain acc 72.00 \tval acc 73.71 \n",
      "  Epoch 2 \tbatch 94 \tloss 0.512632958307 \ttrain acc 74.17 \tval acc 73.73 \n",
      "  Epoch 2 \tbatch 95 \tloss 0.535706680512 \ttrain acc 72.00 \tval acc 73.69 \n",
      "  Epoch 2 \tbatch 96 \tloss 0.531139928345 \ttrain acc 72.26 \tval acc 73.65 \n",
      "  Epoch 2 \tbatch 97 \tloss 0.538145100923 \ttrain acc 71.49 \tval acc 73.82 \n",
      "  Epoch 2 \tbatch 98 \tloss 0.486618394173 \ttrain acc 76.53 \tval acc 73.80 \n",
      "  Epoch 2 \tbatch 99 \tloss 0.49124410729 \ttrain acc 75.00 \tval acc 74.04 \n",
      "  Epoch 2 \tbatch 100 \tloss 0.489291222492 \ttrain acc 76.15 \tval acc 74.09 \n",
      "  Epoch 2 \tbatch 101 \tloss 0.52798105115 \ttrain acc 72.58 \tval acc 74.03 \n",
      "  Epoch 2 \tbatch 102 \tloss 0.531645933988 \ttrain acc 71.81 \tval acc 73.87 \n",
      "  Epoch 2 \tbatch 103 \tloss 0.499865859989 \ttrain acc 74.49 \tval acc 73.75 \n",
      "  Epoch 2 \tbatch 104 \tloss 0.524333865545 \ttrain acc 73.34 \tval acc 73.51 \n",
      "  Epoch 2 \tbatch 105 \tloss 0.500424795679 \ttrain acc 74.74 \tval acc 73.34 \n",
      "  Epoch 2 \tbatch 106 \tloss 0.544206995285 \ttrain acc 71.81 \tval acc 73.51 \n",
      "  Epoch 2 \tbatch 107 \tloss 0.479687234846 \ttrain acc 76.21 \tval acc 74.00 \n",
      "  Epoch 2 \tbatch 108 \tloss 0.49253303709 \ttrain acc 74.94 \tval acc 73.92 \n",
      "  Epoch 2 \tbatch 109 \tloss 0.504092473621 \ttrain acc 73.92 \tval acc 73.87 \n",
      "Epoch 3 of 20 took 106.743s\n",
      "  training loss:\t\t0.511422\n",
      "  training accuracy:\t\t73.97 %\n",
      "  validation loss:\t\t0.514851\n",
      "  validation accuracy:\t\t73.87 %\n",
      "  Epoch 3 \tbatch 1 \tloss 0.520559054214 \ttrain acc 73.72 \tval acc 73.97 \n",
      "  Epoch 3 \tbatch 2 \tloss 0.523147683913 \ttrain acc 73.21 \tval acc 73.94 \n",
      "  Epoch 3 \tbatch 3 \tloss 0.513761171476 \ttrain acc 74.04 \tval acc 73.83 \n",
      "  Epoch 3 \tbatch 4 \tloss 0.524378541978 \ttrain acc 73.92 \tval acc 73.98 \n",
      "  Epoch 3 \tbatch 5 \tloss 0.474576795628 \ttrain acc 76.66 \tval acc 73.89 \n",
      "  Epoch 3 \tbatch 6 \tloss 0.521595140953 \ttrain acc 73.41 \tval acc 74.26 \n",
      "  Epoch 3 \tbatch 7 \tloss 0.523145286489 \ttrain acc 73.60 \tval acc 74.22 \n",
      "  Epoch 3 \tbatch 8 \tloss 0.528365042289 \ttrain acc 72.83 \tval acc 74.27 \n",
      "  Epoch 3 \tbatch 9 \tloss 0.50710760845 \ttrain acc 73.85 \tval acc 73.95 \n",
      "  Epoch 3 \tbatch 10 \tloss 0.516494091725 \ttrain acc 73.09 \tval acc 74.15 \n",
      "  Epoch 3 \tbatch 11 \tloss 0.491815743381 \ttrain acc 75.32 \tval acc 74.02 \n",
      "  Epoch 3 \tbatch 12 \tloss 0.48171302495 \ttrain acc 75.38 \tval acc 74.03 \n",
      "  Epoch 3 \tbatch 13 \tloss 0.492298753648 \ttrain acc 74.62 \tval acc 73.97 \n",
      "  Epoch 3 \tbatch 14 \tloss 0.517012296045 \ttrain acc 74.23 \tval acc 73.74 \n",
      "  Epoch 3 \tbatch 15 \tloss 0.533157083231 \ttrain acc 72.96 \tval acc 73.62 \n",
      "  Epoch 3 \tbatch 16 \tloss 0.497001051064 \ttrain acc 75.32 \tval acc 73.53 \n",
      "  Epoch 3 \tbatch 17 \tloss 0.542073616264 \ttrain acc 72.51 \tval acc 73.62 \n",
      "  Epoch 3 \tbatch 18 \tloss 0.514714370935 \ttrain acc 72.64 \tval acc 73.62 \n",
      "  Epoch 3 \tbatch 19 \tloss 0.489574425526 \ttrain acc 74.87 \tval acc 73.62 \n",
      "  Epoch 3 \tbatch 20 \tloss 0.488150493024 \ttrain acc 76.02 \tval acc 73.80 \n",
      "  Epoch 3 \tbatch 21 \tloss 0.529745155378 \ttrain acc 72.00 \tval acc 73.90 \n",
      "  Epoch 3 \tbatch 22 \tloss 0.5171112222 \ttrain acc 74.36 \tval acc 73.90 \n",
      "  Epoch 3 \tbatch 23 \tloss 0.480166133312 \ttrain acc 75.13 \tval acc 74.13 \n",
      "  Epoch 3 \tbatch 24 \tloss 0.539893104315 \ttrain acc 71.62 \tval acc 74.39 \n",
      "  Epoch 3 \tbatch 25 \tloss 0.496646547887 \ttrain acc 74.68 \tval acc 74.18 \n",
      "  Epoch 3 \tbatch 26 \tloss 0.489855604102 \ttrain acc 75.70 \tval acc 74.27 \n",
      "  Epoch 3 \tbatch 27 \tloss 0.520396381081 \ttrain acc 73.15 \tval acc 74.17 \n",
      "  Epoch 3 \tbatch 28 \tloss 0.463872372253 \ttrain acc 77.61 \tval acc 74.03 \n",
      "  Epoch 3 \tbatch 29 \tloss 0.515263438084 \ttrain acc 74.04 \tval acc 73.93 \n",
      "  Epoch 3 \tbatch 30 \tloss 0.544036432044 \ttrain acc 71.68 \tval acc 73.97 \n",
      "  Epoch 3 \tbatch 31 \tloss 0.520422560208 \ttrain acc 73.34 \tval acc 74.03 \n",
      "  Epoch 3 \tbatch 32 \tloss 0.529614813629 \ttrain acc 73.02 \tval acc 74.05 \n",
      "  Epoch 3 \tbatch 33 \tloss 0.490512685749 \ttrain acc 75.96 \tval acc 74.00 \n",
      "  Epoch 3 \tbatch 34 \tloss 0.504227892665 \ttrain acc 74.62 \tval acc 73.86 \n",
      "  Epoch 3 \tbatch 35 \tloss 0.549213502832 \ttrain acc 69.52 \tval acc 73.92 \n",
      "  Epoch 3 \tbatch 36 \tloss 0.488787694493 \ttrain acc 75.83 \tval acc 74.20 \n",
      "  Epoch 3 \tbatch 37 \tloss 0.505537954363 \ttrain acc 77.04 \tval acc 74.38 \n",
      "  Epoch 3 \tbatch 38 \tloss 0.50755481411 \ttrain acc 75.89 \tval acc 74.52 \n",
      "  Epoch 3 \tbatch 39 \tloss 0.51749345392 \ttrain acc 74.49 \tval acc 74.57 \n",
      "  Epoch 3 \tbatch 40 \tloss 0.469828995497 \ttrain acc 76.28 \tval acc 74.50 \n",
      "  Epoch 3 \tbatch 41 \tloss 0.478430165473 \ttrain acc 75.70 \tval acc 74.44 \n",
      "  Epoch 3 \tbatch 42 \tloss 0.495959879789 \ttrain acc 75.45 \tval acc 74.35 \n",
      "  Epoch 3 \tbatch 43 \tloss 0.494317780533 \ttrain acc 75.45 \tval acc 74.33 \n",
      "  Epoch 3 \tbatch 44 \tloss 0.526932098644 \ttrain acc 72.70 \tval acc 74.26 \n",
      "  Epoch 3 \tbatch 45 \tloss 0.504220244688 \ttrain acc 74.87 \tval acc 74.17 \n",
      "  Epoch 3 \tbatch 46 \tloss 0.502825412111 \ttrain acc 75.00 \tval acc 74.03 \n",
      "  Epoch 3 \tbatch 47 \tloss 0.500155065253 \ttrain acc 75.70 \tval acc 74.09 \n",
      "  Epoch 3 \tbatch 48 \tloss 0.494416114338 \ttrain acc 74.55 \tval acc 74.32 \n",
      "  Epoch 3 \tbatch 49 \tloss 0.497081437289 \ttrain acc 75.96 \tval acc 74.13 \n",
      "  Epoch 3 \tbatch 50 \tloss 0.518648532957 \ttrain acc 72.19 \tval acc 74.05 \n",
      "  Epoch 3 \tbatch 51 \tloss 0.49371704843 \ttrain acc 74.68 \tval acc 74.18 \n",
      "  Epoch 3 \tbatch 52 \tloss 0.487828303404 \ttrain acc 75.45 \tval acc 74.28 \n",
      "  Epoch 3 \tbatch 53 \tloss 0.52573506523 \ttrain acc 72.77 \tval acc 74.36 \n",
      "  Epoch 3 \tbatch 54 \tloss 0.536873153954 \ttrain acc 72.13 \tval acc 74.44 \n",
      "  Epoch 3 \tbatch 55 \tloss 0.489081503512 \ttrain acc 75.89 \tval acc 74.61 \n",
      "  Epoch 3 \tbatch 56 \tloss 0.4849445575 \ttrain acc 75.64 \tval acc 74.65 \n",
      "  Epoch 3 \tbatch 57 \tloss 0.531330025956 \ttrain acc 72.64 \tval acc 74.59 \n",
      "  Epoch 3 \tbatch 58 \tloss 0.514022830857 \ttrain acc 73.34 \tval acc 74.47 \n",
      "  Epoch 3 \tbatch 59 \tloss 0.517577744926 \ttrain acc 74.11 \tval acc 74.46 \n",
      "  Epoch 3 \tbatch 60 \tloss 0.492450623411 \ttrain acc 76.21 \tval acc 74.43 \n",
      "  Epoch 3 \tbatch 61 \tloss 0.489834415923 \ttrain acc 74.11 \tval acc 74.24 \n",
      "  Epoch 3 \tbatch 62 \tloss 0.497599580215 \ttrain acc 74.74 \tval acc 74.15 \n",
      "  Epoch 3 \tbatch 63 \tloss 0.505453399264 \ttrain acc 74.62 \tval acc 74.30 \n",
      "  Epoch 3 \tbatch 64 \tloss 0.503226316134 \ttrain acc 74.23 \tval acc 74.19 \n",
      "  Epoch 3 \tbatch 65 \tloss 0.509475636202 \ttrain acc 73.66 \tval acc 74.06 \n",
      "  Epoch 3 \tbatch 66 \tloss 0.498232288268 \ttrain acc 74.49 \tval acc 74.17 \n",
      "  Epoch 3 \tbatch 67 \tloss 0.464830334586 \ttrain acc 77.17 \tval acc 74.34 \n",
      "  Epoch 3 \tbatch 68 \tloss 0.51337089473 \ttrain acc 72.96 \tval acc 74.41 \n",
      "  Epoch 3 \tbatch 69 \tloss 0.501041589509 \ttrain acc 75.45 \tval acc 74.52 \n",
      "  Epoch 3 \tbatch 70 \tloss 0.515980141334 \ttrain acc 73.15 \tval acc 74.65 \n",
      "  Epoch 3 \tbatch 71 \tloss 0.506585355093 \ttrain acc 74.74 \tval acc 74.59 \n",
      "  Epoch 3 \tbatch 72 \tloss 0.483097414425 \ttrain acc 76.85 \tval acc 74.57 \n",
      "  Epoch 3 \tbatch 73 \tloss 0.504919968946 \ttrain acc 73.66 \tval acc 74.48 \n",
      "  Epoch 3 \tbatch 74 \tloss 0.512449108953 \ttrain acc 73.60 \tval acc 74.41 \n",
      "  Epoch 3 \tbatch 75 \tloss 0.517186410717 \ttrain acc 74.62 \tval acc 74.46 \n",
      "  Epoch 3 \tbatch 76 \tloss 0.531432367637 \ttrain acc 72.90 \tval acc 74.41 \n",
      "  Epoch 3 \tbatch 77 \tloss 0.510754829316 \ttrain acc 73.53 \tval acc 74.38 \n",
      "  Epoch 3 \tbatch 78 \tloss 0.528862564019 \ttrain acc 74.74 \tval acc 74.54 \n",
      "  Epoch 3 \tbatch 79 \tloss 0.499390541852 \ttrain acc 74.81 \tval acc 74.44 \n",
      "  Epoch 3 \tbatch 80 \tloss 0.515148762516 \ttrain acc 72.64 \tval acc 74.59 \n",
      "  Epoch 3 \tbatch 81 \tloss 0.495173330871 \ttrain acc 75.45 \tval acc 74.62 \n",
      "  Epoch 3 \tbatch 82 \tloss 0.490620610001 \ttrain acc 74.94 \tval acc 74.54 \n",
      "  Epoch 3 \tbatch 83 \tloss 0.493949359362 \ttrain acc 75.06 \tval acc 74.36 \n",
      "  Epoch 3 \tbatch 84 \tloss 0.508164703252 \ttrain acc 74.43 \tval acc 74.42 \n",
      "  Epoch 3 \tbatch 85 \tloss 0.521338901205 \ttrain acc 73.66 \tval acc 74.34 \n",
      "  Epoch 3 \tbatch 86 \tloss 0.494469150789 \ttrain acc 75.26 \tval acc 74.25 \n",
      "  Epoch 3 \tbatch 87 \tloss 0.512481171283 \ttrain acc 72.77 \tval acc 74.42 \n",
      "  Epoch 3 \tbatch 88 \tloss 0.47968896335 \ttrain acc 77.55 \tval acc 74.35 \n",
      "  Epoch 3 \tbatch 89 \tloss 0.473666970376 \ttrain acc 76.08 \tval acc 74.43 \n",
      "  Epoch 3 \tbatch 90 \tloss 0.538753329894 \ttrain acc 72.90 \tval acc 74.03 \n",
      "  Epoch 3 \tbatch 91 \tloss 0.478018358009 \ttrain acc 76.28 \tval acc 73.92 \n",
      "  Epoch 3 \tbatch 92 \tloss 0.514448716913 \ttrain acc 75.00 \tval acc 74.03 \n",
      "  Epoch 3 \tbatch 93 \tloss 0.516573217649 \ttrain acc 72.00 \tval acc 74.10 \n",
      "  Epoch 3 \tbatch 94 \tloss 0.505916437848 \ttrain acc 75.26 \tval acc 73.86 \n",
      "  Epoch 3 \tbatch 95 \tloss 0.532201423607 \ttrain acc 72.00 \tval acc 73.88 \n",
      "  Epoch 3 \tbatch 96 \tloss 0.525590534613 \ttrain acc 73.85 \tval acc 74.03 \n",
      "  Epoch 3 \tbatch 97 \tloss 0.539010200061 \ttrain acc 71.94 \tval acc 74.11 \n",
      "  Epoch 3 \tbatch 98 \tloss 0.485030736727 \ttrain acc 75.96 \tval acc 74.27 \n",
      "  Epoch 3 \tbatch 99 \tloss 0.483269036278 \ttrain acc 75.51 \tval acc 74.41 \n",
      "  Epoch 3 \tbatch 100 \tloss 0.478806775895 \ttrain acc 77.42 \tval acc 74.48 \n",
      "  Epoch 3 \tbatch 101 \tloss 0.522010247102 \ttrain acc 72.83 \tval acc 74.54 \n",
      "  Epoch 3 \tbatch 102 \tloss 0.516667678907 \ttrain acc 73.02 \tval acc 74.36 \n",
      "  Epoch 3 \tbatch 103 \tloss 0.496404225785 \ttrain acc 74.55 \tval acc 74.31 \n",
      "  Epoch 3 \tbatch 104 \tloss 0.514533924951 \ttrain acc 74.23 \tval acc 74.37 \n",
      "  Epoch 3 \tbatch 105 \tloss 0.492860048761 \ttrain acc 76.15 \tval acc 74.37 \n",
      "  Epoch 3 \tbatch 106 \tloss 0.537346534714 \ttrain acc 72.83 \tval acc 74.34 \n",
      "  Epoch 3 \tbatch 107 \tloss 0.468874618584 \ttrain acc 76.72 \tval acc 74.35 \n",
      "  Epoch 3 \tbatch 108 \tloss 0.482463842034 \ttrain acc 76.02 \tval acc 74.28 \n",
      "  Epoch 3 \tbatch 109 \tloss 0.495388341102 \ttrain acc 74.74 \tval acc 74.24 \n",
      "Epoch 4 of 20 took 111.371s\n",
      "  training loss:\t\t0.506146\n",
      "  training accuracy:\t\t74.40 %\n",
      "  validation loss:\t\t0.507160\n",
      "  validation accuracy:\t\t74.24 %\n",
      "  Epoch 4 \tbatch 1 \tloss 0.509773777257 \ttrain acc 74.49 \tval acc 74.29 \n",
      "  Epoch 4 \tbatch 2 \tloss 0.521366692855 \ttrain acc 73.41 \tval acc 74.31 \n",
      "  Epoch 4 \tbatch 3 \tloss 0.50670625727 \ttrain acc 74.43 \tval acc 74.31 \n",
      "  Epoch 4 \tbatch 4 \tloss 0.515050653105 \ttrain acc 74.23 \tval acc 74.47 \n",
      "  Epoch 4 \tbatch 5 \tloss 0.468911242748 \ttrain acc 76.40 \tval acc 74.52 \n",
      "  Epoch 4 \tbatch 6 \tloss 0.512155705928 \ttrain acc 74.17 \tval acc 74.59 \n",
      "  Epoch 4 \tbatch 7 \tloss 0.519177300516 \ttrain acc 74.23 \tval acc 74.76 \n",
      "  Epoch 4 \tbatch 8 \tloss 0.524640648655 \ttrain acc 73.28 \tval acc 74.75 \n",
      "  Epoch 4 \tbatch 9 \tloss 0.503334543503 \ttrain acc 74.55 \tval acc 74.81 \n",
      "  Epoch 4 \tbatch 10 \tloss 0.510328545482 \ttrain acc 72.45 \tval acc 74.83 \n",
      "  Epoch 4 \tbatch 11 \tloss 0.483853832519 \ttrain acc 75.64 \tval acc 74.91 \n",
      "  Epoch 4 \tbatch 12 \tloss 0.473060828689 \ttrain acc 75.83 \tval acc 74.93 \n",
      "  Epoch 4 \tbatch 13 \tloss 0.492049760413 \ttrain acc 74.68 \tval acc 74.90 \n",
      "  Epoch 4 \tbatch 14 \tloss 0.513432403846 \ttrain acc 75.19 \tval acc 74.83 \n",
      "  Epoch 4 \tbatch 15 \tloss 0.526643820011 \ttrain acc 73.41 \tval acc 74.68 \n",
      "  Epoch 4 \tbatch 16 \tloss 0.493027230315 \ttrain acc 76.15 \tval acc 74.62 \n",
      "  Epoch 4 \tbatch 17 \tloss 0.534746502824 \ttrain acc 72.70 \tval acc 74.45 \n",
      "  Epoch 4 \tbatch 18 \tloss 0.508766433052 \ttrain acc 72.64 \tval acc 74.36 \n",
      "  Epoch 4 \tbatch 19 \tloss 0.484259943188 \ttrain acc 75.32 \tval acc 74.35 \n",
      "  Epoch 4 \tbatch 20 \tloss 0.476599218489 \ttrain acc 75.89 \tval acc 74.44 \n",
      "  Epoch 4 \tbatch 21 \tloss 0.528271376554 \ttrain acc 72.07 \tval acc 74.59 \n",
      "  Epoch 4 \tbatch 22 \tloss 0.515095710473 \ttrain acc 74.81 \tval acc 74.69 \n",
      "  Epoch 4 \tbatch 23 \tloss 0.475846332312 \ttrain acc 75.70 \tval acc 74.62 \n",
      "  Epoch 4 \tbatch 24 \tloss 0.529053357157 \ttrain acc 73.21 \tval acc 74.61 \n",
      "  Epoch 4 \tbatch 25 \tloss 0.491310542164 \ttrain acc 74.74 \tval acc 74.50 \n",
      "  Epoch 4 \tbatch 26 \tloss 0.483741428627 \ttrain acc 75.70 \tval acc 74.61 \n",
      "  Epoch 4 \tbatch 27 \tloss 0.516381788745 \ttrain acc 73.02 \tval acc 74.42 \n",
      "  Epoch 4 \tbatch 28 \tloss 0.457607655414 \ttrain acc 78.19 \tval acc 74.63 \n",
      "  Epoch 4 \tbatch 29 \tloss 0.504688725947 \ttrain acc 75.57 \tval acc 74.59 \n",
      "  Epoch 4 \tbatch 30 \tloss 0.53844003851 \ttrain acc 72.51 \tval acc 74.64 \n",
      "  Epoch 4 \tbatch 31 \tloss 0.512521554154 \ttrain acc 74.04 \tval acc 74.58 \n",
      "  Epoch 4 \tbatch 32 \tloss 0.520556507892 \ttrain acc 73.21 \tval acc 74.65 \n",
      "  Epoch 4 \tbatch 33 \tloss 0.480376174884 \ttrain acc 76.91 \tval acc 74.64 \n",
      "  Epoch 4 \tbatch 34 \tloss 0.499046536487 \ttrain acc 75.32 \tval acc 74.64 \n",
      "  Epoch 4 \tbatch 35 \tloss 0.537793348778 \ttrain acc 70.09 \tval acc 74.50 \n",
      "  Epoch 4 \tbatch 36 \tloss 0.474311589661 \ttrain acc 76.28 \tval acc 74.59 \n",
      "  Epoch 4 \tbatch 37 \tloss 0.495638748677 \ttrain acc 77.87 \tval acc 74.65 \n",
      "  Epoch 4 \tbatch 38 \tloss 0.501340099291 \ttrain acc 75.06 \tval acc 74.64 \n",
      "  Epoch 4 \tbatch 39 \tloss 0.516216496863 \ttrain acc 74.36 \tval acc 74.71 \n",
      "  Epoch 4 \tbatch 40 \tloss 0.466528609367 \ttrain acc 76.40 \tval acc 74.71 \n",
      "  Epoch 4 \tbatch 41 \tloss 0.476702802586 \ttrain acc 75.70 \tval acc 74.77 \n",
      "  Epoch 4 \tbatch 42 \tloss 0.493546773483 \ttrain acc 75.57 \tval acc 74.59 \n",
      "  Epoch 4 \tbatch 43 \tloss 0.491323384212 \ttrain acc 75.57 \tval acc 74.74 \n",
      "  Epoch 4 \tbatch 44 \tloss 0.518419283743 \ttrain acc 73.34 \tval acc 74.66 \n",
      "  Epoch 4 \tbatch 45 \tloss 0.506096345309 \ttrain acc 75.13 \tval acc 74.60 \n",
      "  Epoch 4 \tbatch 46 \tloss 0.498299459856 \ttrain acc 74.74 \tval acc 74.60 \n",
      "  Epoch 4 \tbatch 47 \tloss 0.491325801563 \ttrain acc 76.28 \tval acc 74.59 \n",
      "  Epoch 4 \tbatch 48 \tloss 0.488504539056 \ttrain acc 76.02 \tval acc 74.68 \n",
      "  Epoch 4 \tbatch 49 \tloss 0.4889957467 \ttrain acc 76.79 \tval acc 74.58 \n",
      "  Epoch 4 \tbatch 50 \tloss 0.518193800118 \ttrain acc 71.88 \tval acc 74.51 \n",
      "  Epoch 4 \tbatch 51 \tloss 0.496838319234 \ttrain acc 74.55 \tval acc 74.23 \n",
      "  Epoch 4 \tbatch 52 \tloss 0.48424970909 \ttrain acc 76.34 \tval acc 74.12 \n",
      "  Epoch 4 \tbatch 53 \tloss 0.522983714888 \ttrain acc 73.92 \tval acc 74.26 \n",
      "  Epoch 4 \tbatch 54 \tloss 0.536214197314 \ttrain acc 71.68 \tval acc 74.37 \n",
      "  Epoch 4 \tbatch 55 \tloss 0.483625912664 \ttrain acc 76.02 \tval acc 74.57 \n",
      "  Epoch 4 \tbatch 56 \tloss 0.481802938404 \ttrain acc 75.83 \tval acc 74.58 \n",
      "  Epoch 4 \tbatch 57 \tloss 0.528489265771 \ttrain acc 73.41 \tval acc 74.53 \n",
      "  Epoch 4 \tbatch 58 \tloss 0.510776751591 \ttrain acc 73.98 \tval acc 74.56 \n",
      "  Epoch 4 \tbatch 59 \tloss 0.505618587001 \ttrain acc 74.04 \tval acc 74.54 \n",
      "  Epoch 4 \tbatch 60 \tloss 0.488363195958 \ttrain acc 76.47 \tval acc 74.54 \n",
      "  Epoch 4 \tbatch 61 \tloss 0.483898042898 \ttrain acc 74.55 \tval acc 74.45 \n",
      "  Epoch 4 \tbatch 62 \tloss 0.492868965631 \ttrain acc 74.87 \tval acc 74.29 \n",
      "  Epoch 4 \tbatch 63 \tloss 0.502729481045 \ttrain acc 75.06 \tval acc 74.30 \n",
      "  Epoch 4 \tbatch 64 \tloss 0.497770309886 \ttrain acc 74.94 \tval acc 74.33 \n",
      "  Epoch 4 \tbatch 65 \tloss 0.499786207926 \ttrain acc 74.87 \tval acc 74.36 \n",
      "  Epoch 4 \tbatch 66 \tloss 0.492758179753 \ttrain acc 75.13 \tval acc 74.48 \n",
      "  Epoch 4 \tbatch 67 \tloss 0.461158504169 \ttrain acc 76.98 \tval acc 74.55 \n",
      "  Epoch 4 \tbatch 68 \tloss 0.509408772529 \ttrain acc 73.41 \tval acc 74.64 \n",
      "  Epoch 4 \tbatch 69 \tloss 0.499141902961 \ttrain acc 75.96 \tval acc 74.59 \n",
      "  Epoch 4 \tbatch 70 \tloss 0.509060641457 \ttrain acc 73.47 \tval acc 74.54 \n",
      "  Epoch 4 \tbatch 71 \tloss 0.498167154679 \ttrain acc 75.19 \tval acc 74.45 \n",
      "  Epoch 4 \tbatch 72 \tloss 0.481246656215 \ttrain acc 75.70 \tval acc 74.46 \n",
      "  Epoch 4 \tbatch 73 \tloss 0.499284356875 \ttrain acc 72.90 \tval acc 74.64 \n",
      "  Epoch 4 \tbatch 74 \tloss 0.504163279285 \ttrain acc 73.66 \tval acc 74.55 \n",
      "  Epoch 4 \tbatch 75 \tloss 0.513030042049 \ttrain acc 74.36 \tval acc 74.64 \n",
      "  Epoch 4 \tbatch 76 \tloss 0.526326420104 \ttrain acc 73.72 \tval acc 74.67 \n",
      "  Epoch 4 \tbatch 77 \tloss 0.508905337568 \ttrain acc 73.60 \tval acc 74.77 \n",
      "  Epoch 4 \tbatch 78 \tloss 0.522002592439 \ttrain acc 75.19 \tval acc 74.55 \n",
      "  Epoch 4 \tbatch 79 \tloss 0.496402703215 \ttrain acc 75.26 \tval acc 74.50 \n",
      "  Epoch 4 \tbatch 80 \tloss 0.503801474159 \ttrain acc 73.66 \tval acc 74.51 \n",
      "  Epoch 4 \tbatch 81 \tloss 0.49213072792 \ttrain acc 75.38 \tval acc 74.39 \n",
      "  Epoch 4 \tbatch 82 \tloss 0.487905650227 \ttrain acc 75.38 \tval acc 74.40 \n",
      "  Epoch 4 \tbatch 83 \tloss 0.490199948513 \ttrain acc 75.13 \tval acc 74.35 \n",
      "  Epoch 4 \tbatch 84 \tloss 0.504063398446 \ttrain acc 74.81 \tval acc 74.36 \n",
      "  Epoch 4 \tbatch 85 \tloss 0.515901602136 \ttrain acc 73.98 \tval acc 74.56 \n",
      "  Epoch 4 \tbatch 86 \tloss 0.480303277227 \ttrain acc 75.64 \tval acc 74.55 \n",
      "  Epoch 4 \tbatch 87 \tloss 0.506162329064 \ttrain acc 73.98 \tval acc 74.45 \n",
      "  Epoch 4 \tbatch 88 \tloss 0.470578216132 \ttrain acc 78.25 \tval acc 74.74 \n",
      "  Epoch 4 \tbatch 89 \tloss 0.471970527912 \ttrain acc 76.47 \tval acc 74.70 \n",
      "  Epoch 4 \tbatch 90 \tloss 0.537212138144 \ttrain acc 72.96 \tval acc 74.64 \n",
      "  Epoch 4 \tbatch 91 \tloss 0.464256677236 \ttrain acc 76.85 \tval acc 74.46 \n",
      "  Epoch 4 \tbatch 92 \tloss 0.508350489061 \ttrain acc 76.47 \tval acc 74.49 \n",
      "  Epoch 4 \tbatch 93 \tloss 0.50975658738 \ttrain acc 72.58 \tval acc 74.54 \n",
      "  Epoch 4 \tbatch 94 \tloss 0.501009816535 \ttrain acc 75.89 \tval acc 74.61 \n",
      "  Epoch 4 \tbatch 95 \tloss 0.528118245126 \ttrain acc 71.88 \tval acc 74.65 \n",
      "  Epoch 4 \tbatch 96 \tloss 0.515431753846 \ttrain acc 73.98 \tval acc 74.61 \n",
      "  Epoch 4 \tbatch 97 \tloss 0.529614463534 \ttrain acc 72.83 \tval acc 74.72 \n",
      "  Epoch 4 \tbatch 98 \tloss 0.480904418755 \ttrain acc 76.47 \tval acc 74.68 \n",
      "  Epoch 4 \tbatch 99 \tloss 0.475404004558 \ttrain acc 75.89 \tval acc 74.61 \n",
      "  Epoch 4 \tbatch 100 \tloss 0.481407909984 \ttrain acc 76.79 \tval acc 74.67 \n",
      "  Epoch 4 \tbatch 101 \tloss 0.516058648066 \ttrain acc 73.47 \tval acc 74.58 \n",
      "  Epoch 4 \tbatch 102 \tloss 0.513112718152 \ttrain acc 73.92 \tval acc 74.65 \n",
      "  Epoch 4 \tbatch 103 \tloss 0.496050242875 \ttrain acc 74.81 \tval acc 74.58 \n",
      "  Epoch 4 \tbatch 104 \tloss 0.511169128274 \ttrain acc 74.43 \tval acc 74.58 \n",
      "  Epoch 4 \tbatch 105 \tloss 0.487463837282 \ttrain acc 75.13 \tval acc 74.35 \n",
      "  Epoch 4 \tbatch 106 \tloss 0.527225011222 \ttrain acc 72.00 \tval acc 74.38 \n",
      "  Epoch 4 \tbatch 107 \tloss 0.465359991587 \ttrain acc 77.55 \tval acc 74.30 \n",
      "  Epoch 4 \tbatch 108 \tloss 0.481018994803 \ttrain acc 75.38 \tval acc 74.31 \n",
      "  Epoch 4 \tbatch 109 \tloss 0.494289157434 \ttrain acc 74.74 \tval acc 74.31 \n",
      "Epoch 5 of 20 took 110.266s\n",
      "  training loss:\t\t0.500710\n",
      "  training accuracy:\t\t74.74 %\n",
      "  validation loss:\t\t0.508230\n",
      "  validation accuracy:\t\t74.31 %\n",
      "  Epoch 5 \tbatch 1 \tloss 0.514021730302 \ttrain acc 73.92 \tval acc 74.28 \n",
      "  Epoch 5 \tbatch 2 \tloss 0.514820269206 \ttrain acc 73.53 \tval acc 74.37 \n",
      "  Epoch 5 \tbatch 3 \tloss 0.503290793091 \ttrain acc 75.45 \tval acc 74.49 \n",
      "  Epoch 5 \tbatch 4 \tloss 0.511810704765 \ttrain acc 74.43 \tval acc 74.44 \n",
      "  Epoch 5 \tbatch 5 \tloss 0.463352924358 \ttrain acc 77.23 \tval acc 74.54 \n",
      "  Epoch 5 \tbatch 6 \tloss 0.501201479688 \ttrain acc 74.04 \tval acc 74.61 \n",
      "  Epoch 5 \tbatch 7 \tloss 0.513727975594 \ttrain acc 74.68 \tval acc 74.70 \n",
      "  Epoch 5 \tbatch 8 \tloss 0.521206674013 \ttrain acc 73.60 \tval acc 75.04 \n",
      "  Epoch 5 \tbatch 9 \tloss 0.493738532108 \ttrain acc 74.81 \tval acc 74.81 \n",
      "  Epoch 5 \tbatch 10 \tloss 0.504162166169 \ttrain acc 72.77 \tval acc 74.76 \n",
      "  Epoch 5 \tbatch 11 \tloss 0.484440475 \ttrain acc 75.45 \tval acc 74.84 \n",
      "  Epoch 5 \tbatch 12 \tloss 0.4676273676 \ttrain acc 76.72 \tval acc 74.90 \n",
      "  Epoch 5 \tbatch 13 \tloss 0.487327931113 \ttrain acc 74.74 \tval acc 74.97 \n",
      "  Epoch 5 \tbatch 14 \tloss 0.511198361726 \ttrain acc 74.43 \tval acc 74.79 \n",
      "  Epoch 5 \tbatch 15 \tloss 0.527206930941 \ttrain acc 73.66 \tval acc 74.64 \n",
      "  Epoch 5 \tbatch 16 \tloss 0.486145149557 \ttrain acc 77.23 \tval acc 74.67 \n",
      "  Epoch 5 \tbatch 17 \tloss 0.533288615887 \ttrain acc 72.51 \tval acc 74.44 \n",
      "  Epoch 5 \tbatch 18 \tloss 0.504989677187 \ttrain acc 72.77 \tval acc 74.37 \n",
      "  Epoch 5 \tbatch 19 \tloss 0.479521298141 \ttrain acc 76.21 \tval acc 74.48 \n",
      "  Epoch 5 \tbatch 20 \tloss 0.478748064642 \ttrain acc 75.64 \tval acc 74.41 \n",
      "  Epoch 5 \tbatch 21 \tloss 0.520437653069 \ttrain acc 72.77 \tval acc 74.50 \n",
      "  Epoch 5 \tbatch 22 \tloss 0.514061981575 \ttrain acc 75.06 \tval acc 74.56 \n",
      "  Epoch 5 \tbatch 23 \tloss 0.470915483819 \ttrain acc 76.34 \tval acc 74.55 \n",
      "  Epoch 5 \tbatch 24 \tloss 0.523085559535 \ttrain acc 73.79 \tval acc 74.58 \n",
      "  Epoch 5 \tbatch 25 \tloss 0.487635611278 \ttrain acc 75.06 \tval acc 74.54 \n",
      "  Epoch 5 \tbatch 26 \tloss 0.479682696134 \ttrain acc 76.02 \tval acc 74.50 \n",
      "  Epoch 5 \tbatch 27 \tloss 0.507870173896 \ttrain acc 73.53 \tval acc 74.57 \n",
      "  Epoch 5 \tbatch 28 \tloss 0.451470411783 \ttrain acc 79.21 \tval acc 74.42 \n",
      "  Epoch 5 \tbatch 29 \tloss 0.501167805037 \ttrain acc 75.45 \tval acc 74.34 \n",
      "  Epoch 5 \tbatch 30 \tloss 0.537068510777 \ttrain acc 72.32 \tval acc 74.46 \n",
      "  Epoch 5 \tbatch 31 \tloss 0.515173121679 \ttrain acc 73.92 \tval acc 74.73 \n",
      "  Epoch 5 \tbatch 32 \tloss 0.518586378726 \ttrain acc 73.79 \tval acc 74.78 \n",
      "  Epoch 5 \tbatch 33 \tloss 0.477322020195 \ttrain acc 77.23 \tval acc 74.74 \n",
      "  Epoch 5 \tbatch 34 \tloss 0.49128947422 \ttrain acc 77.10 \tval acc 74.71 \n",
      "  Epoch 5 \tbatch 35 \tloss 0.535410680541 \ttrain acc 70.60 \tval acc 74.71 \n",
      "  Epoch 5 \tbatch 36 \tloss 0.468965026861 \ttrain acc 77.04 \tval acc 74.64 \n",
      "  Epoch 5 \tbatch 37 \tloss 0.489269767833 \ttrain acc 77.30 \tval acc 74.85 \n",
      "  Epoch 5 \tbatch 38 \tloss 0.497234478357 \ttrain acc 75.57 \tval acc 74.89 \n",
      "  Epoch 5 \tbatch 39 \tloss 0.512130141453 \ttrain acc 74.04 \tval acc 74.70 \n",
      "  Epoch 5 \tbatch 40 \tloss 0.46294139692 \ttrain acc 77.10 \tval acc 74.51 \n",
      "  Epoch 5 \tbatch 41 \tloss 0.478996239337 \ttrain acc 75.77 \tval acc 74.63 \n",
      "  Epoch 5 \tbatch 42 \tloss 0.497161792793 \ttrain acc 75.19 \tval acc 74.71 \n",
      "  Epoch 5 \tbatch 43 \tloss 0.490569434161 \ttrain acc 75.89 \tval acc 74.95 \n",
      "  Epoch 5 \tbatch 44 \tloss 0.514582449283 \ttrain acc 73.60 \tval acc 75.00 \n",
      "  Epoch 5 \tbatch 45 \tloss 0.497752935065 \ttrain acc 75.64 \tval acc 74.80 \n",
      "  Epoch 5 \tbatch 46 \tloss 0.498235206513 \ttrain acc 75.38 \tval acc 74.64 \n",
      "  Epoch 5 \tbatch 47 \tloss 0.491735230658 \ttrain acc 76.21 \tval acc 74.62 \n",
      "  Epoch 5 \tbatch 48 \tloss 0.489884639338 \ttrain acc 76.21 \tval acc 74.79 \n",
      "  Epoch 5 \tbatch 49 \tloss 0.49391825535 \ttrain acc 75.00 \tval acc 74.68 \n",
      "  Epoch 5 \tbatch 50 \tloss 0.514169072291 \ttrain acc 71.94 \tval acc 74.72 \n",
      "  Epoch 5 \tbatch 51 \tloss 0.491414709034 \ttrain acc 75.32 \tval acc 74.41 \n",
      "  Epoch 5 \tbatch 52 \tloss 0.479428184809 \ttrain acc 77.55 \tval acc 74.33 \n",
      "  Epoch 5 \tbatch 53 \tloss 0.515537661798 \ttrain acc 74.11 \tval acc 74.33 \n",
      "  Epoch 5 \tbatch 54 \tloss 0.533470712844 \ttrain acc 72.00 \tval acc 74.42 \n",
      "  Epoch 5 \tbatch 55 \tloss 0.480383401541 \ttrain acc 75.70 \tval acc 74.86 \n",
      "  Epoch 5 \tbatch 56 \tloss 0.475572812401 \ttrain acc 75.77 \tval acc 74.84 \n",
      "  Epoch 5 \tbatch 57 \tloss 0.524744219214 \ttrain acc 73.34 \tval acc 75.00 \n",
      "  Epoch 5 \tbatch 58 \tloss 0.50263128401 \ttrain acc 75.26 \tval acc 74.91 \n",
      "  Epoch 5 \tbatch 59 \tloss 0.501902470073 \ttrain acc 74.49 \tval acc 74.86 \n",
      "  Epoch 5 \tbatch 60 \tloss 0.484317245992 \ttrain acc 76.53 \tval acc 74.76 \n",
      "  Epoch 5 \tbatch 61 \tloss 0.480236193782 \ttrain acc 75.38 \tval acc 74.54 \n",
      "  Epoch 5 \tbatch 62 \tloss 0.49315848466 \ttrain acc 74.68 \tval acc 74.54 \n",
      "  Epoch 5 \tbatch 63 \tloss 0.498777121717 \ttrain acc 75.19 \tval acc 74.44 \n",
      "  Epoch 5 \tbatch 64 \tloss 0.498927064765 \ttrain acc 74.17 \tval acc 74.43 \n",
      "  Epoch 5 \tbatch 65 \tloss 0.49698735099 \ttrain acc 75.45 \tval acc 74.64 \n",
      "  Epoch 5 \tbatch 66 \tloss 0.4908753611 \ttrain acc 75.38 \tval acc 74.75 \n",
      "  Epoch 5 \tbatch 67 \tloss 0.460577575387 \ttrain acc 77.61 \tval acc 74.72 \n",
      "  Epoch 5 \tbatch 68 \tloss 0.508600617881 \ttrain acc 74.04 \tval acc 74.81 \n",
      "  Epoch 5 \tbatch 69 \tloss 0.500836798488 \ttrain acc 75.13 \tval acc 74.77 \n",
      "  Epoch 5 \tbatch 70 \tloss 0.507559571148 \ttrain acc 73.66 \tval acc 74.90 \n",
      "  Epoch 5 \tbatch 71 \tloss 0.489846724294 \ttrain acc 75.45 \tval acc 74.89 \n",
      "  Epoch 5 \tbatch 72 \tloss 0.471881784609 \ttrain acc 76.47 \tval acc 74.75 \n",
      "  Epoch 5 \tbatch 73 \tloss 0.496790545788 \ttrain acc 74.36 \tval acc 74.86 \n",
      "  Epoch 5 \tbatch 74 \tloss 0.500892286481 \ttrain acc 73.85 \tval acc 74.76 \n",
      "  Epoch 5 \tbatch 75 \tloss 0.508186689767 \ttrain acc 75.19 \tval acc 74.85 \n",
      "  Epoch 5 \tbatch 76 \tloss 0.51986619934 \ttrain acc 73.79 \tval acc 74.83 \n",
      "  Epoch 5 \tbatch 77 \tloss 0.504387903743 \ttrain acc 73.98 \tval acc 74.74 \n",
      "  Epoch 5 \tbatch 78 \tloss 0.520854402335 \ttrain acc 74.94 \tval acc 74.83 \n",
      "  Epoch 5 \tbatch 79 \tloss 0.493291023112 \ttrain acc 75.64 \tval acc 74.77 \n",
      "  Epoch 5 \tbatch 80 \tloss 0.501424424156 \ttrain acc 73.60 \tval acc 74.77 \n",
      "  Epoch 5 \tbatch 81 \tloss 0.484600090966 \ttrain acc 76.40 \tval acc 74.83 \n",
      "  Epoch 5 \tbatch 82 \tloss 0.483589563508 \ttrain acc 75.19 \tval acc 74.80 \n",
      "  Epoch 5 \tbatch 83 \tloss 0.487649072647 \ttrain acc 75.26 \tval acc 74.88 \n",
      "  Epoch 5 \tbatch 84 \tloss 0.49507798913 \ttrain acc 76.28 \tval acc 74.84 \n",
      "  Epoch 5 \tbatch 85 \tloss 0.509716756843 \ttrain acc 74.04 \tval acc 74.72 \n",
      "  Epoch 5 \tbatch 86 \tloss 0.477317030465 \ttrain acc 76.53 \tval acc 74.76 \n",
      "  Epoch 5 \tbatch 87 \tloss 0.501001288929 \ttrain acc 74.55 \tval acc 74.86 \n",
      "  Epoch 5 \tbatch 88 \tloss 0.467455066658 \ttrain acc 77.42 \tval acc 75.02 \n",
      "  Epoch 5 \tbatch 89 \tloss 0.465866419846 \ttrain acc 77.10 \tval acc 74.89 \n",
      "  Epoch 5 \tbatch 90 \tloss 0.529929249278 \ttrain acc 72.64 \tval acc 74.84 \n",
      "  Epoch 5 \tbatch 91 \tloss 0.462553411246 \ttrain acc 77.30 \tval acc 74.67 \n",
      "  Epoch 5 \tbatch 92 \tloss 0.500613577993 \ttrain acc 76.59 \tval acc 74.45 \n",
      "  Epoch 5 \tbatch 93 \tloss 0.503269036201 \ttrain acc 72.96 \tval acc 74.49 \n",
      "  Epoch 5 \tbatch 94 \tloss 0.494920847864 \ttrain acc 75.45 \tval acc 74.59 \n",
      "  Epoch 5 \tbatch 95 \tloss 0.52485705307 \ttrain acc 73.60 \tval acc 74.82 \n",
      "  Epoch 5 \tbatch 96 \tloss 0.511993777724 \ttrain acc 74.30 \tval acc 74.84 \n",
      "  Epoch 5 \tbatch 97 \tloss 0.525387523473 \ttrain acc 73.60 \tval acc 74.80 \n",
      "  Epoch 5 \tbatch 98 \tloss 0.478874266904 \ttrain acc 75.89 \tval acc 74.75 \n",
      "  Epoch 5 \tbatch 99 \tloss 0.479137415181 \ttrain acc 75.77 \tval acc 74.71 \n",
      "  Epoch 5 \tbatch 100 \tloss 0.475699897919 \ttrain acc 77.36 \tval acc 74.83 \n",
      "  Epoch 5 \tbatch 101 \tloss 0.516166283464 \ttrain acc 73.53 \tval acc 74.63 \n",
      "  Epoch 5 \tbatch 102 \tloss 0.509642136721 \ttrain acc 74.23 \tval acc 74.42 \n",
      "  Epoch 5 \tbatch 103 \tloss 0.49481150915 \ttrain acc 74.43 \tval acc 74.50 \n",
      "  Epoch 5 \tbatch 104 \tloss 0.507516520029 \ttrain acc 74.04 \tval acc 74.71 \n",
      "  Epoch 5 \tbatch 105 \tloss 0.491172332309 \ttrain acc 74.81 \tval acc 74.72 \n",
      "  Epoch 5 \tbatch 106 \tloss 0.522281127514 \ttrain acc 72.77 \tval acc 74.73 \n",
      "  Epoch 5 \tbatch 107 \tloss 0.461702259579 \ttrain acc 78.12 \tval acc 74.80 \n",
      "  Epoch 5 \tbatch 108 \tloss 0.479563907809 \ttrain acc 75.83 \tval acc 74.74 \n",
      "  Epoch 5 \tbatch 109 \tloss 0.489348900916 \ttrain acc 75.38 \tval acc 74.55 \n",
      "Epoch 6 of 20 took 110.794s\n",
      "  training loss:\t\t0.497208\n",
      "  training accuracy:\t\t75.02 %\n",
      "  validation loss:\t\t0.509009\n",
      "  validation accuracy:\t\t74.55 %\n",
      "  Epoch 6 \tbatch 1 \tloss 0.509745666901 \ttrain acc 73.41 \tval acc 74.57 \n",
      "  Epoch 6 \tbatch 2 \tloss 0.51034776008 \ttrain acc 73.92 \tval acc 74.68 \n",
      "  Epoch 6 \tbatch 3 \tloss 0.499383825878 \ttrain acc 76.15 \tval acc 74.71 \n",
      "  Epoch 6 \tbatch 4 \tloss 0.512184795789 \ttrain acc 73.98 \tval acc 74.82 \n",
      "  Epoch 6 \tbatch 5 \tloss 0.462708399559 \ttrain acc 76.85 \tval acc 74.54 \n",
      "  Epoch 6 \tbatch 6 \tloss 0.502183047819 \ttrain acc 73.66 \tval acc 74.46 \n",
      "  Epoch 6 \tbatch 7 \tloss 0.510332208891 \ttrain acc 74.87 \tval acc 74.63 \n",
      "  Epoch 6 \tbatch 8 \tloss 0.518874075342 \ttrain acc 73.15 \tval acc 74.74 \n",
      "  Epoch 6 \tbatch 9 \tloss 0.491465454806 \ttrain acc 74.94 \tval acc 74.80 \n",
      "  Epoch 6 \tbatch 10 \tloss 0.498462413669 \ttrain acc 73.66 \tval acc 74.90 \n",
      "  Epoch 6 \tbatch 11 \tloss 0.482724449689 \ttrain acc 75.51 \tval acc 75.28 \n",
      "  Epoch 6 \tbatch 12 \tloss 0.466423226936 \ttrain acc 76.98 \tval acc 75.22 \n",
      "  Epoch 6 \tbatch 13 \tloss 0.482221596081 \ttrain acc 75.32 \tval acc 75.31 \n",
      "  Epoch 6 \tbatch 14 \tloss 0.506214820548 \ttrain acc 75.00 \tval acc 75.13 \n",
      "  Epoch 6 \tbatch 15 \tloss 0.523711055087 \ttrain acc 73.79 \tval acc 75.05 \n",
      "  Epoch 6 \tbatch 16 \tloss 0.482695790401 \ttrain acc 77.36 \tval acc 74.92 \n",
      "  Epoch 6 \tbatch 17 \tloss 0.529680367966 \ttrain acc 73.41 \tval acc 74.98 \n",
      "  Epoch 6 \tbatch 18 \tloss 0.505676272211 \ttrain acc 72.45 \tval acc 74.85 \n",
      "  Epoch 6 \tbatch 19 \tloss 0.476552513345 \ttrain acc 76.08 \tval acc 74.77 \n",
      "  Epoch 6 \tbatch 20 \tloss 0.479667746381 \ttrain acc 75.89 \tval acc 74.74 \n",
      "  Epoch 6 \tbatch 21 \tloss 0.510986467353 \ttrain acc 73.53 \tval acc 74.75 \n",
      "  Epoch 6 \tbatch 22 \tloss 0.512711367237 \ttrain acc 75.06 \tval acc 74.65 \n",
      "  Epoch 6 \tbatch 23 \tloss 0.46715168311 \ttrain acc 76.79 \tval acc 74.60 \n",
      "  Epoch 6 \tbatch 24 \tloss 0.521144643727 \ttrain acc 73.47 \tval acc 74.62 \n",
      "  Epoch 6 \tbatch 25 \tloss 0.482648825777 \ttrain acc 74.87 \tval acc 74.84 \n",
      "  Epoch 6 \tbatch 26 \tloss 0.477662448889 \ttrain acc 76.47 \tval acc 74.92 \n",
      "  Epoch 6 \tbatch 27 \tloss 0.505400383954 \ttrain acc 73.98 \tval acc 74.88 \n",
      "  Epoch 6 \tbatch 28 \tloss 0.442886084059 \ttrain acc 79.40 \tval acc 74.84 \n",
      "  Epoch 6 \tbatch 29 \tloss 0.49761508961 \ttrain acc 75.06 \tval acc 75.03 \n",
      "  Epoch 6 \tbatch 30 \tloss 0.52868074789 \ttrain acc 72.83 \tval acc 75.19 \n",
      "  Epoch 6 \tbatch 31 \tloss 0.512937733348 \ttrain acc 73.98 \tval acc 75.02 \n",
      "  Epoch 6 \tbatch 32 \tloss 0.516218118142 \ttrain acc 73.21 \tval acc 74.99 \n",
      "  Epoch 6 \tbatch 33 \tloss 0.482217897444 \ttrain acc 77.30 \tval acc 75.15 \n",
      "  Epoch 6 \tbatch 34 \tloss 0.485695590749 \ttrain acc 76.79 \tval acc 75.22 \n",
      "  Epoch 6 \tbatch 35 \tloss 0.53434362479 \ttrain acc 71.11 \tval acc 74.89 \n",
      "  Epoch 6 \tbatch 36 \tloss 0.461754503932 \ttrain acc 76.72 \tval acc 74.75 \n",
      "  Epoch 6 \tbatch 37 \tloss 0.487365843083 \ttrain acc 77.68 \tval acc 74.82 \n",
      "  Epoch 6 \tbatch 38 \tloss 0.498597291712 \ttrain acc 74.94 \tval acc 74.77 \n",
      "  Epoch 6 \tbatch 39 \tloss 0.506240631401 \ttrain acc 74.43 \tval acc 74.54 \n",
      "  Epoch 6 \tbatch 40 \tloss 0.458421378957 \ttrain acc 77.81 \tval acc 74.36 \n",
      "  Epoch 6 \tbatch 41 \tloss 0.472466423173 \ttrain acc 76.85 \tval acc 74.22 \n",
      "  Epoch 6 \tbatch 42 \tloss 0.490184046959 \ttrain acc 75.70 \tval acc 74.44 \n",
      "  Epoch 6 \tbatch 43 \tloss 0.487482157105 \ttrain acc 75.57 \tval acc 74.64 \n",
      "  Epoch 6 \tbatch 44 \tloss 0.512489400068 \ttrain acc 72.45 \tval acc 74.90 \n",
      "  Epoch 6 \tbatch 45 \tloss 0.497052779124 \ttrain acc 75.26 \tval acc 74.92 \n",
      "  Epoch 6 \tbatch 46 \tloss 0.492028011392 \ttrain acc 75.06 \tval acc 74.76 \n",
      "  Epoch 6 \tbatch 47 \tloss 0.489462987401 \ttrain acc 75.13 \tval acc 74.81 \n",
      "  Epoch 6 \tbatch 48 \tloss 0.486663044816 \ttrain acc 76.08 \tval acc 74.82 \n",
      "  Epoch 6 \tbatch 49 \tloss 0.483606843892 \ttrain acc 76.21 \tval acc 74.80 \n",
      "  Epoch 6 \tbatch 50 \tloss 0.510607094087 \ttrain acc 72.00 \tval acc 74.68 \n",
      "  Epoch 6 \tbatch 51 \tloss 0.487672051124 \ttrain acc 75.38 \tval acc 74.54 \n",
      "  Epoch 6 \tbatch 52 \tloss 0.475464680195 \ttrain acc 77.55 \tval acc 74.47 \n",
      "  Epoch 6 \tbatch 53 \tloss 0.51001116159 \ttrain acc 74.43 \tval acc 74.52 \n",
      "  Epoch 6 \tbatch 54 \tloss 0.525328890075 \ttrain acc 72.45 \tval acc 74.69 \n",
      "  Epoch 6 \tbatch 55 \tloss 0.475224661524 \ttrain acc 75.83 \tval acc 74.79 \n",
      "  Epoch 6 \tbatch 56 \tloss 0.473660747033 \ttrain acc 75.83 \tval acc 74.84 \n",
      "  Epoch 6 \tbatch 57 \tloss 0.516364290396 \ttrain acc 74.74 \tval acc 74.97 \n",
      "  Epoch 6 \tbatch 58 \tloss 0.496760204781 \ttrain acc 75.06 \tval acc 74.95 \n",
      "  Epoch 6 \tbatch 59 \tloss 0.501413146229 \ttrain acc 74.36 \tval acc 75.05 \n",
      "  Epoch 6 \tbatch 60 \tloss 0.483083420776 \ttrain acc 76.72 \tval acc 75.14 \n",
      "  Epoch 6 \tbatch 61 \tloss 0.483301031083 \ttrain acc 76.08 \tval acc 75.27 \n",
      "  Epoch 6 \tbatch 62 \tloss 0.483120312443 \ttrain acc 75.51 \tval acc 75.22 \n",
      "  Epoch 6 \tbatch 63 \tloss 0.493100657791 \ttrain acc 75.89 \tval acc 74.97 \n",
      "  Epoch 6 \tbatch 64 \tloss 0.493800613203 \ttrain acc 75.19 \tval acc 74.91 \n",
      "  Epoch 6 \tbatch 65 \tloss 0.497846633252 \ttrain acc 75.51 \tval acc 74.74 \n",
      "  Epoch 6 \tbatch 66 \tloss 0.4869321839 \ttrain acc 76.08 \tval acc 74.79 \n",
      "  Epoch 6 \tbatch 67 \tloss 0.454413586246 \ttrain acc 78.25 \tval acc 74.72 \n",
      "  Epoch 6 \tbatch 68 \tloss 0.509595062698 \ttrain acc 74.49 \tval acc 74.77 \n",
      "  Epoch 6 \tbatch 69 \tloss 0.498445711491 \ttrain acc 74.74 \tval acc 74.74 \n",
      "  Epoch 6 \tbatch 70 \tloss 0.501962201991 \ttrain acc 73.98 \tval acc 74.73 \n",
      "  Epoch 6 \tbatch 71 \tloss 0.488468821734 \ttrain acc 75.64 \tval acc 74.84 \n",
      "  Epoch 6 \tbatch 72 \tloss 0.469980170015 \ttrain acc 77.04 \tval acc 74.93 \n",
      "  Epoch 6 \tbatch 73 \tloss 0.494265988231 \ttrain acc 75.70 \tval acc 75.06 \n",
      "  Epoch 6 \tbatch 74 \tloss 0.500423920896 \ttrain acc 75.00 \tval acc 75.03 \n",
      "  Epoch 6 \tbatch 75 \tloss 0.498766290867 \ttrain acc 75.19 \tval acc 75.15 \n",
      "  Epoch 6 \tbatch 76 \tloss 0.517699640619 \ttrain acc 74.30 \tval acc 75.36 \n",
      "  Epoch 6 \tbatch 77 \tloss 0.495044036918 \ttrain acc 75.06 \tval acc 75.24 \n",
      "  Epoch 6 \tbatch 78 \tloss 0.511723910641 \ttrain acc 76.15 \tval acc 75.20 \n",
      "  Epoch 6 \tbatch 79 \tloss 0.489018032918 \ttrain acc 76.15 \tval acc 75.04 \n",
      "  Epoch 6 \tbatch 80 \tloss 0.497014027583 \ttrain acc 73.98 \tval acc 74.87 \n",
      "  Epoch 6 \tbatch 81 \tloss 0.48164372249 \ttrain acc 76.79 \tval acc 74.88 \n",
      "  Epoch 6 \tbatch 82 \tloss 0.478955202503 \ttrain acc 75.45 \tval acc 74.98 \n",
      "  Epoch 6 \tbatch 83 \tloss 0.483612993034 \ttrain acc 75.45 \tval acc 74.90 \n",
      "  Epoch 6 \tbatch 84 \tloss 0.490154204252 \ttrain acc 75.70 \tval acc 74.95 \n",
      "  Epoch 6 \tbatch 85 \tloss 0.510470094909 \ttrain acc 73.41 \tval acc 74.96 \n",
      "  Epoch 6 \tbatch 86 \tloss 0.474327696513 \ttrain acc 76.72 \tval acc 74.79 \n",
      "  Epoch 6 \tbatch 87 \tloss 0.493030645734 \ttrain acc 75.19 \tval acc 74.85 \n",
      "  Epoch 6 \tbatch 88 \tloss 0.466628350828 \ttrain acc 78.00 \tval acc 74.70 \n",
      "  Epoch 6 \tbatch 89 \tloss 0.465943414915 \ttrain acc 76.91 \tval acc 74.74 \n",
      "  Epoch 6 \tbatch 90 \tloss 0.529363135735 \ttrain acc 72.90 \tval acc 74.74 \n",
      "  Epoch 6 \tbatch 91 \tloss 0.462364815939 \ttrain acc 76.85 \tval acc 74.80 \n",
      "  Epoch 6 \tbatch 92 \tloss 0.491026599735 \ttrain acc 76.47 \tval acc 74.38 \n",
      "  Epoch 6 \tbatch 93 \tloss 0.499715419296 \ttrain acc 74.74 \tval acc 74.39 \n",
      "  Epoch 6 \tbatch 94 \tloss 0.491456459692 \ttrain acc 76.53 \tval acc 74.48 \n",
      "  Epoch 6 \tbatch 95 \tloss 0.521779368727 \ttrain acc 72.90 \tval acc 74.53 \n",
      "  Epoch 6 \tbatch 96 \tloss 0.507718752905 \ttrain acc 74.62 \tval acc 74.54 \n",
      "  Epoch 6 \tbatch 97 \tloss 0.522402288787 \ttrain acc 73.85 \tval acc 74.67 \n",
      "  Epoch 6 \tbatch 98 \tloss 0.477383296879 \ttrain acc 76.72 \tval acc 74.62 \n",
      "  Epoch 6 \tbatch 99 \tloss 0.472527356356 \ttrain acc 75.83 \tval acc 74.68 \n",
      "  Epoch 6 \tbatch 100 \tloss 0.465401347151 \ttrain acc 77.93 \tval acc 74.70 \n",
      "  Epoch 6 \tbatch 101 \tloss 0.510478158186 \ttrain acc 74.49 \tval acc 74.63 \n",
      "  Epoch 6 \tbatch 102 \tloss 0.506128904785 \ttrain acc 74.74 \tval acc 74.72 \n",
      "  Epoch 6 \tbatch 103 \tloss 0.48750976763 \ttrain acc 75.19 \tval acc 74.82 \n",
      "  Epoch 6 \tbatch 104 \tloss 0.506068844199 \ttrain acc 74.62 \tval acc 74.81 \n",
      "  Epoch 6 \tbatch 105 \tloss 0.488560923372 \ttrain acc 75.19 \tval acc 74.90 \n",
      "  Epoch 6 \tbatch 106 \tloss 0.522330241604 \ttrain acc 72.90 \tval acc 74.84 \n",
      "  Epoch 6 \tbatch 107 \tloss 0.45913237305 \ttrain acc 77.49 \tval acc 74.75 \n",
      "  Epoch 6 \tbatch 108 \tloss 0.475843906315 \ttrain acc 76.08 \tval acc 74.73 \n",
      "  Epoch 6 \tbatch 109 \tloss 0.484822454337 \ttrain acc 75.06 \tval acc 74.76 \n",
      "Epoch 7 of 20 took 108.211s\n",
      "  training loss:\t\t0.493584\n",
      "  training accuracy:\t\t75.26 %\n",
      "  validation loss:\t\t0.505607\n",
      "  validation accuracy:\t\t74.76 %\n",
      "  Epoch 7 \tbatch 1 \tloss 0.501021920534 \ttrain acc 74.49 \tval acc 74.64 \n",
      "  Epoch 7 \tbatch 2 \tloss 0.505221354179 \ttrain acc 74.68 \tval acc 74.49 \n",
      "  Epoch 7 \tbatch 3 \tloss 0.494102965514 \ttrain acc 76.02 \tval acc 74.74 \n",
      "  Epoch 7 \tbatch 4 \tloss 0.512903656997 \ttrain acc 74.43 \tval acc 74.82 \n",
      "  Epoch 7 \tbatch 5 \tloss 0.460019757661 \ttrain acc 77.30 \tval acc 74.74 \n",
      "  Epoch 7 \tbatch 6 \tloss 0.502886493161 \ttrain acc 74.11 \tval acc 74.76 \n",
      "  Epoch 7 \tbatch 7 \tloss 0.502211447637 \ttrain acc 75.13 \tval acc 74.64 \n",
      "  Epoch 7 \tbatch 8 \tloss 0.515953514332 \ttrain acc 73.28 \tval acc 74.46 \n",
      "  Epoch 7 \tbatch 9 \tloss 0.487095431256 \ttrain acc 75.19 \tval acc 74.67 \n",
      "  Epoch 7 \tbatch 10 \tloss 0.502158135811 \ttrain acc 74.11 \tval acc 74.84 \n",
      "  Epoch 7 \tbatch 11 \tloss 0.482135867528 \ttrain acc 75.89 \tval acc 75.12 \n",
      "  Epoch 7 \tbatch 12 \tloss 0.462279574331 \ttrain acc 76.91 \tval acc 74.95 \n",
      "  Epoch 7 \tbatch 13 \tloss 0.478033822059 \ttrain acc 76.15 \tval acc 74.75 \n",
      "  Epoch 7 \tbatch 14 \tloss 0.504642710817 \ttrain acc 75.57 \tval acc 74.67 \n",
      "  Epoch 7 \tbatch 15 \tloss 0.520667006338 \ttrain acc 73.60 \tval acc 74.67 \n",
      "  Epoch 7 \tbatch 16 \tloss 0.481605032324 \ttrain acc 76.53 \tval acc 74.57 \n",
      "  Epoch 7 \tbatch 17 \tloss 0.53001023873 \ttrain acc 72.90 \tval acc 74.86 \n",
      "  Epoch 7 \tbatch 18 \tloss 0.502065728171 \ttrain acc 73.15 \tval acc 74.85 \n",
      "  Epoch 7 \tbatch 19 \tloss 0.476046010585 \ttrain acc 76.66 \tval acc 74.77 \n",
      "  Epoch 7 \tbatch 20 \tloss 0.475342269133 \ttrain acc 75.96 \tval acc 74.58 \n",
      "  Epoch 7 \tbatch 21 \tloss 0.504894525222 \ttrain acc 74.04 \tval acc 74.50 \n",
      "  Epoch 7 \tbatch 22 \tloss 0.511924617594 \ttrain acc 74.49 \tval acc 74.42 \n",
      "  Epoch 7 \tbatch 23 \tloss 0.472147410904 \ttrain acc 76.85 \tval acc 74.60 \n",
      "  Epoch 7 \tbatch 24 \tloss 0.523107363989 \ttrain acc 72.45 \tval acc 74.63 \n",
      "  Epoch 7 \tbatch 25 \tloss 0.483525216783 \ttrain acc 75.06 \tval acc 74.73 \n",
      "  Epoch 7 \tbatch 26 \tloss 0.472835326287 \ttrain acc 76.79 \tval acc 74.72 \n",
      "  Epoch 7 \tbatch 27 \tloss 0.504639182097 \ttrain acc 74.11 \tval acc 74.75 \n",
      "  Epoch 7 \tbatch 28 \tloss 0.444794449413 \ttrain acc 79.85 \tval acc 74.80 \n",
      "  Epoch 7 \tbatch 29 \tloss 0.496892764251 \ttrain acc 74.94 \tval acc 74.74 \n",
      "  Epoch 7 \tbatch 30 \tloss 0.526905183548 \ttrain acc 73.66 \tval acc 74.98 \n",
      "  Epoch 7 \tbatch 31 \tloss 0.507385081791 \ttrain acc 75.13 \tval acc 75.17 \n",
      "  Epoch 7 \tbatch 32 \tloss 0.514927056172 \ttrain acc 73.34 \tval acc 75.27 \n",
      "  Epoch 7 \tbatch 33 \tloss 0.47045837078 \ttrain acc 77.49 \tval acc 75.17 \n",
      "  Epoch 7 \tbatch 34 \tloss 0.477579487623 \ttrain acc 77.23 \tval acc 74.97 \n",
      "  Epoch 7 \tbatch 35 \tloss 0.527110089292 \ttrain acc 71.36 \tval acc 74.73 \n",
      "  Epoch 7 \tbatch 36 \tloss 0.458771682239 \ttrain acc 76.98 \tval acc 74.53 \n",
      "  Epoch 7 \tbatch 37 \tloss 0.485430466961 \ttrain acc 77.49 \tval acc 74.42 \n",
      "  Epoch 7 \tbatch 38 \tloss 0.49077276529 \ttrain acc 75.83 \tval acc 74.45 \n",
      "  Epoch 7 \tbatch 39 \tloss 0.50402455646 \ttrain acc 74.23 \tval acc 74.53 \n",
      "  Epoch 7 \tbatch 40 \tloss 0.453560001098 \ttrain acc 77.30 \tval acc 74.48 \n",
      "  Epoch 7 \tbatch 41 \tloss 0.467068524295 \ttrain acc 76.53 \tval acc 74.50 \n",
      "  Epoch 7 \tbatch 42 \tloss 0.484757876438 \ttrain acc 75.26 \tval acc 74.64 \n",
      "  Epoch 7 \tbatch 43 \tloss 0.477250030612 \ttrain acc 75.83 \tval acc 74.91 \n",
      "  Epoch 7 \tbatch 44 \tloss 0.51301757866 \ttrain acc 73.60 \tval acc 74.79 \n",
      "  Epoch 7 \tbatch 45 \tloss 0.494719728981 \ttrain acc 75.19 \tval acc 74.75 \n",
      "  Epoch 7 \tbatch 46 \tloss 0.486967095903 \ttrain acc 75.51 \tval acc 74.72 \n",
      "  Epoch 7 \tbatch 47 \tloss 0.484158142206 \ttrain acc 75.96 \tval acc 74.74 \n",
      "  Epoch 7 \tbatch 48 \tloss 0.480684415661 \ttrain acc 76.34 \tval acc 74.70 \n",
      "  Epoch 7 \tbatch 49 \tloss 0.48149266666 \ttrain acc 76.59 \tval acc 74.76 \n",
      "  Epoch 7 \tbatch 50 \tloss 0.504706940097 \ttrain acc 73.02 \tval acc 74.78 \n",
      "  Epoch 7 \tbatch 51 \tloss 0.478985681579 \ttrain acc 75.64 \tval acc 74.82 \n",
      "  Epoch 7 \tbatch 52 \tloss 0.47037936463 \ttrain acc 77.49 \tval acc 74.79 \n",
      "  Epoch 7 \tbatch 53 \tloss 0.50182823001 \ttrain acc 75.38 \tval acc 74.79 \n",
      "  Epoch 7 \tbatch 54 \tloss 0.52025172044 \ttrain acc 73.02 \tval acc 74.73 \n",
      "  Epoch 7 \tbatch 55 \tloss 0.471393521292 \ttrain acc 76.85 \tval acc 74.61 \n",
      "  Epoch 7 \tbatch 56 \tloss 0.47232661111 \ttrain acc 76.47 \tval acc 74.65 \n",
      "  Epoch 7 \tbatch 57 \tloss 0.513506017108 \ttrain acc 73.92 \tval acc 74.83 \n",
      "  Epoch 7 \tbatch 58 \tloss 0.494969289262 \ttrain acc 74.94 \tval acc 74.89 \n",
      "  Epoch 7 \tbatch 59 \tloss 0.489807808445 \ttrain acc 74.74 \tval acc 75.01 \n",
      "  Epoch 7 \tbatch 60 \tloss 0.474535109029 \ttrain acc 77.30 \tval acc 75.12 \n",
      "  Epoch 7 \tbatch 61 \tloss 0.472762311773 \ttrain acc 75.77 \tval acc 75.00 \n",
      "  Epoch 7 \tbatch 62 \tloss 0.482441346293 \ttrain acc 75.45 \tval acc 75.00 \n",
      "  Epoch 7 \tbatch 63 \tloss 0.49330816296 \ttrain acc 75.19 \tval acc 74.96 \n",
      "  Epoch 7 \tbatch 64 \tloss 0.483564333165 \ttrain acc 75.26 \tval acc 74.87 \n",
      "  Epoch 7 \tbatch 65 \tloss 0.485243079827 \ttrain acc 75.64 \tval acc 74.66 \n",
      "  Epoch 7 \tbatch 66 \tloss 0.480580090381 \ttrain acc 75.57 \tval acc 74.54 \n",
      "  Epoch 7 \tbatch 67 \tloss 0.451429053705 \ttrain acc 77.68 \tval acc 74.69 \n",
      "  Epoch 7 \tbatch 68 \tloss 0.509243747163 \ttrain acc 73.79 \tval acc 74.66 \n",
      "  Epoch 7 \tbatch 69 \tloss 0.493074348081 \ttrain acc 74.62 \tval acc 74.58 \n",
      "  Epoch 7 \tbatch 70 \tloss 0.505122228721 \ttrain acc 73.92 \tval acc 74.65 \n",
      "  Epoch 7 \tbatch 71 \tloss 0.484495689242 \ttrain acc 76.02 \tval acc 74.68 \n",
      "  Epoch 7 \tbatch 72 \tloss 0.467370862955 \ttrain acc 77.23 \tval acc 74.79 \n",
      "  Epoch 7 \tbatch 73 \tloss 0.490288786194 \ttrain acc 75.57 \tval acc 74.71 \n",
      "  Epoch 7 \tbatch 74 \tloss 0.495713718579 \ttrain acc 74.62 \tval acc 74.93 \n",
      "  Epoch 7 \tbatch 75 \tloss 0.489691051956 \ttrain acc 75.83 \tval acc 74.85 \n",
      "  Epoch 7 \tbatch 76 \tloss 0.511652505795 \ttrain acc 74.43 \tval acc 74.74 \n",
      "  Epoch 7 \tbatch 77 \tloss 0.490082139065 \ttrain acc 75.45 \tval acc 74.82 \n",
      "  Epoch 7 \tbatch 78 \tloss 0.511569936285 \ttrain acc 75.19 \tval acc 74.89 \n",
      "  Epoch 7 \tbatch 79 \tloss 0.485877169454 \ttrain acc 75.96 \tval acc 74.90 \n",
      "  Epoch 7 \tbatch 80 \tloss 0.492571732748 \ttrain acc 73.92 \tval acc 74.92 \n",
      "  Epoch 7 \tbatch 81 \tloss 0.477211406085 \ttrain acc 76.59 \tval acc 74.80 \n",
      "  Epoch 7 \tbatch 82 \tloss 0.471271124858 \ttrain acc 75.64 \tval acc 74.72 \n",
      "  Epoch 7 \tbatch 83 \tloss 0.480020862641 \ttrain acc 75.64 \tval acc 74.66 \n",
      "  Epoch 7 \tbatch 84 \tloss 0.492497958225 \ttrain acc 75.64 \tval acc 74.58 \n",
      "  Epoch 7 \tbatch 85 \tloss 0.50746180801 \ttrain acc 73.34 \tval acc 74.53 \n",
      "  Epoch 7 \tbatch 86 \tloss 0.472402814064 \ttrain acc 76.47 \tval acc 74.56 \n",
      "  Epoch 7 \tbatch 87 \tloss 0.487126855497 \ttrain acc 75.57 \tval acc 74.51 \n",
      "  Epoch 7 \tbatch 88 \tloss 0.468579972587 \ttrain acc 77.68 \tval acc 74.47 \n",
      "  Epoch 7 \tbatch 89 \tloss 0.463948789029 \ttrain acc 76.79 \tval acc 74.47 \n",
      "  Epoch 7 \tbatch 90 \tloss 0.525098574636 \ttrain acc 74.17 \tval acc 74.83 \n",
      "  Epoch 7 \tbatch 91 \tloss 0.464534448603 \ttrain acc 76.59 \tval acc 74.85 \n",
      "  Epoch 7 \tbatch 92 \tloss 0.491453381292 \ttrain acc 76.98 \tval acc 74.71 \n",
      "  Epoch 7 \tbatch 93 \tloss 0.492956301435 \ttrain acc 75.00 \tval acc 74.44 \n",
      "  Epoch 7 \tbatch 94 \tloss 0.481559786684 \ttrain acc 76.28 \tval acc 74.31 \n",
      "  Epoch 7 \tbatch 95 \tloss 0.524172765558 \ttrain acc 73.09 \tval acc 74.27 \n",
      "  Epoch 7 \tbatch 96 \tloss 0.505921215005 \ttrain acc 75.06 \tval acc 74.31 \n",
      "  Epoch 7 \tbatch 97 \tloss 0.522842918669 \ttrain acc 74.36 \tval acc 74.55 \n",
      "  Epoch 7 \tbatch 98 \tloss 0.475608925385 \ttrain acc 76.53 \tval acc 74.68 \n",
      "  Epoch 7 \tbatch 99 \tloss 0.469379074833 \ttrain acc 75.57 \tval acc 74.73 \n",
      "  Epoch 7 \tbatch 100 \tloss 0.462210670081 \ttrain acc 77.36 \tval acc 74.59 \n",
      "  Epoch 7 \tbatch 101 \tloss 0.508154450054 \ttrain acc 74.68 \tval acc 74.62 \n",
      "  Epoch 7 \tbatch 102 \tloss 0.506427503824 \ttrain acc 75.26 \tval acc 74.66 \n",
      "  Epoch 7 \tbatch 103 \tloss 0.483005432442 \ttrain acc 76.40 \tval acc 74.69 \n",
      "  Epoch 7 \tbatch 104 \tloss 0.502357679297 \ttrain acc 74.23 \tval acc 74.73 \n",
      "  Epoch 7 \tbatch 105 \tloss 0.482850664781 \ttrain acc 75.89 \tval acc 74.74 \n",
      "  Epoch 7 \tbatch 106 \tloss 0.514692794525 \ttrain acc 73.92 \tval acc 74.71 \n",
      "  Epoch 7 \tbatch 107 \tloss 0.459207161332 \ttrain acc 77.74 \tval acc 74.58 \n",
      "  Epoch 7 \tbatch 108 \tloss 0.471982920193 \ttrain acc 76.28 \tval acc 74.60 \n",
      "  Epoch 7 \tbatch 109 \tloss 0.47746531328 \ttrain acc 76.08 \tval acc 74.44 \n",
      "Epoch 8 of 20 took 108.029s\n",
      "  training loss:\t\t0.489994\n",
      "  training accuracy:\t\t75.43 %\n",
      "  validation loss:\t\t0.505964\n",
      "  validation accuracy:\t\t74.44 %\n",
      "  Epoch 8 \tbatch 1 \tloss 0.492684008651 \ttrain acc 75.13 \tval acc 74.55 \n",
      "  Epoch 8 \tbatch 2 \tloss 0.500935681454 \ttrain acc 74.94 \tval acc 74.64 \n",
      "  Epoch 8 \tbatch 3 \tloss 0.492756281501 \ttrain acc 76.34 \tval acc 74.56 \n",
      "  Epoch 8 \tbatch 4 \tloss 0.502679416451 \ttrain acc 74.43 \tval acc 74.43 \n",
      "  Epoch 8 \tbatch 5 \tloss 0.448835111828 \ttrain acc 78.00 \tval acc 74.54 \n",
      "  Epoch 8 \tbatch 6 \tloss 0.499665882656 \ttrain acc 74.87 \tval acc 74.54 \n",
      "  Epoch 8 \tbatch 7 \tloss 0.500762096107 \ttrain acc 75.38 \tval acc 74.56 \n",
      "  Epoch 8 \tbatch 8 \tloss 0.512829757653 \ttrain acc 73.72 \tval acc 74.60 \n",
      "  Epoch 8 \tbatch 9 \tloss 0.480917697368 \ttrain acc 75.77 \tval acc 74.42 \n",
      "  Epoch 8 \tbatch 10 \tloss 0.495890540453 \ttrain acc 74.36 \tval acc 74.62 \n",
      "  Epoch 8 \tbatch 11 \tloss 0.479077908722 \ttrain acc 75.77 \tval acc 74.73 \n",
      "  Epoch 8 \tbatch 12 \tloss 0.462247828781 \ttrain acc 76.98 \tval acc 74.80 \n",
      "  Epoch 8 \tbatch 13 \tloss 0.475579113001 \ttrain acc 76.15 \tval acc 74.70 \n",
      "  Epoch 8 \tbatch 14 \tloss 0.498922218389 \ttrain acc 76.02 \tval acc 74.55 \n",
      "  Epoch 8 \tbatch 15 \tloss 0.519079136273 \ttrain acc 73.15 \tval acc 74.45 \n",
      "  Epoch 8 \tbatch 16 \tloss 0.482061864183 \ttrain acc 75.96 \tval acc 74.44 \n",
      "  Epoch 8 \tbatch 17 \tloss 0.53154172637 \ttrain acc 73.09 \tval acc 74.58 \n",
      "  Epoch 8 \tbatch 18 \tloss 0.501075481692 \ttrain acc 73.66 \tval acc 74.53 \n",
      "  Epoch 8 \tbatch 19 \tloss 0.474855491527 \ttrain acc 76.15 \tval acc 74.76 \n",
      "  Epoch 8 \tbatch 20 \tloss 0.474098164729 \ttrain acc 75.96 \tval acc 74.68 \n",
      "  Epoch 8 \tbatch 21 \tloss 0.502547622369 \ttrain acc 74.23 \tval acc 74.70 \n",
      "  Epoch 8 \tbatch 22 \tloss 0.507985911505 \ttrain acc 75.32 \tval acc 74.59 \n",
      "  Epoch 8 \tbatch 23 \tloss 0.468016356207 \ttrain acc 76.59 \tval acc 74.46 \n",
      "  Epoch 8 \tbatch 24 \tloss 0.522826561387 \ttrain acc 73.60 \tval acc 74.50 \n",
      "  Epoch 8 \tbatch 25 \tloss 0.486317927993 \ttrain acc 75.45 \tval acc 74.58 \n",
      "  Epoch 8 \tbatch 26 \tloss 0.476550030931 \ttrain acc 76.21 \tval acc 74.65 \n",
      "  Epoch 8 \tbatch 27 \tloss 0.504151918058 \ttrain acc 74.23 \tval acc 74.62 \n",
      "  Epoch 8 \tbatch 28 \tloss 0.445826683689 \ttrain acc 79.27 \tval acc 74.61 \n",
      "  Epoch 8 \tbatch 29 \tloss 0.495108664709 \ttrain acc 74.49 \tval acc 74.42 \n",
      "  Epoch 8 \tbatch 30 \tloss 0.526345873324 \ttrain acc 73.53 \tval acc 74.40 \n",
      "  Epoch 8 \tbatch 31 \tloss 0.5078240203 \ttrain acc 75.57 \tval acc 74.35 \n",
      "  Epoch 8 \tbatch 32 \tloss 0.506299879277 \ttrain acc 73.41 \tval acc 74.43 \n",
      "  Epoch 8 \tbatch 33 \tloss 0.467512068065 \ttrain acc 77.74 \tval acc 74.43 \n",
      "  Epoch 8 \tbatch 34 \tloss 0.478150721397 \ttrain acc 76.66 \tval acc 74.63 \n",
      "  Epoch 8 \tbatch 35 \tloss 0.524901513875 \ttrain acc 71.43 \tval acc 74.79 \n",
      "  Epoch 8 \tbatch 36 \tloss 0.452116771262 \ttrain acc 77.10 \tval acc 74.63 \n",
      "  Epoch 8 \tbatch 37 \tloss 0.48575034018 \ttrain acc 77.74 \tval acc 74.78 \n",
      "  Epoch 8 \tbatch 38 \tloss 0.484348226768 \ttrain acc 76.34 \tval acc 74.54 \n",
      "  Epoch 8 \tbatch 39 \tloss 0.502320127596 \ttrain acc 74.49 \tval acc 74.34 \n",
      "  Epoch 8 \tbatch 40 \tloss 0.451394235879 \ttrain acc 77.81 \tval acc 74.40 \n",
      "  Epoch 8 \tbatch 41 \tloss 0.46054423139 \ttrain acc 77.49 \tval acc 74.38 \n",
      "  Epoch 8 \tbatch 42 \tloss 0.480686782998 \ttrain acc 75.64 \tval acc 74.44 \n",
      "  Epoch 8 \tbatch 43 \tloss 0.478955459095 \ttrain acc 75.89 \tval acc 74.41 \n",
      "  Epoch 8 \tbatch 44 \tloss 0.514206105797 \ttrain acc 72.51 \tval acc 74.45 \n",
      "  Epoch 8 \tbatch 45 \tloss 0.488092619669 \ttrain acc 75.45 \tval acc 74.52 \n",
      "  Epoch 8 \tbatch 46 \tloss 0.483948356829 \ttrain acc 75.96 \tval acc 74.66 \n",
      "  Epoch 8 \tbatch 47 \tloss 0.483483912393 \ttrain acc 76.15 \tval acc 74.74 \n",
      "  Epoch 8 \tbatch 48 \tloss 0.476392818835 \ttrain acc 76.72 \tval acc 74.83 \n",
      "  Epoch 8 \tbatch 49 \tloss 0.476999969225 \ttrain acc 76.34 \tval acc 74.76 \n",
      "  Epoch 8 \tbatch 50 \tloss 0.493978707559 \ttrain acc 73.60 \tval acc 74.80 \n",
      "  Epoch 8 \tbatch 51 \tloss 0.472326504057 \ttrain acc 76.02 \tval acc 74.59 \n",
      "  Epoch 8 \tbatch 52 \tloss 0.472102035328 \ttrain acc 77.42 \tval acc 74.54 \n",
      "  Epoch 8 \tbatch 53 \tloss 0.499877136864 \ttrain acc 75.26 \tval acc 74.49 \n",
      "  Epoch 8 \tbatch 54 \tloss 0.516677467021 \ttrain acc 73.72 \tval acc 74.49 \n",
      "  Epoch 8 \tbatch 55 \tloss 0.470879023171 \ttrain acc 76.79 \tval acc 74.49 \n",
      "  Epoch 8 \tbatch 56 \tloss 0.469102153793 \ttrain acc 76.91 \tval acc 74.70 \n",
      "  Epoch 8 \tbatch 57 \tloss 0.50861984316 \ttrain acc 74.43 \tval acc 74.68 \n",
      "  Epoch 8 \tbatch 58 \tloss 0.489255525489 \ttrain acc 75.89 \tval acc 74.57 \n",
      "  Epoch 8 \tbatch 59 \tloss 0.494268900745 \ttrain acc 74.81 \tval acc 74.58 \n",
      "  Epoch 8 \tbatch 60 \tloss 0.474756832701 \ttrain acc 76.66 \tval acc 74.70 \n",
      "  Epoch 8 \tbatch 61 \tloss 0.470248964849 \ttrain acc 76.53 \tval acc 74.73 \n",
      "  Epoch 8 \tbatch 62 \tloss 0.477168991699 \ttrain acc 75.06 \tval acc 74.71 \n",
      "  Epoch 8 \tbatch 63 \tloss 0.489154755196 \ttrain acc 75.06 \tval acc 74.80 \n",
      "  Epoch 8 \tbatch 64 \tloss 0.479349073997 \ttrain acc 75.06 \tval acc 74.74 \n",
      "  Epoch 8 \tbatch 65 \tloss 0.488545166742 \ttrain acc 75.70 \tval acc 74.56 \n",
      "  Epoch 8 \tbatch 66 \tloss 0.476997695458 \ttrain acc 76.53 \tval acc 74.54 \n",
      "  Epoch 8 \tbatch 67 \tloss 0.449470874059 \ttrain acc 77.61 \tval acc 74.60 \n",
      "  Epoch 8 \tbatch 68 \tloss 0.505945065008 \ttrain acc 73.92 \tval acc 74.57 \n",
      "  Epoch 8 \tbatch 69 \tloss 0.492954002874 \ttrain acc 75.32 \tval acc 74.76 \n",
      "  Epoch 8 \tbatch 70 \tloss 0.501181677262 \ttrain acc 74.04 \tval acc 74.85 \n",
      "  Epoch 8 \tbatch 71 \tloss 0.486041448515 \ttrain acc 76.34 \tval acc 74.92 \n",
      "  Epoch 8 \tbatch 72 \tloss 0.466972983433 \ttrain acc 77.74 \tval acc 74.73 \n",
      "  Epoch 8 \tbatch 73 \tloss 0.489817941544 \ttrain acc 75.70 \tval acc 74.61 \n",
      "  Epoch 8 \tbatch 74 \tloss 0.491464253752 \ttrain acc 74.74 \tval acc 74.64 \n",
      "  Epoch 8 \tbatch 75 \tloss 0.491007755741 \ttrain acc 75.32 \tval acc 74.54 \n",
      "  Epoch 8 \tbatch 76 \tloss 0.511116739298 \ttrain acc 74.74 \tval acc 74.62 \n",
      "  Epoch 8 \tbatch 77 \tloss 0.487675511593 \ttrain acc 75.77 \tval acc 74.59 \n",
      "  Epoch 8 \tbatch 78 \tloss 0.507580486801 \ttrain acc 75.89 \tval acc 74.55 \n",
      "  Epoch 8 \tbatch 79 \tloss 0.48104267751 \ttrain acc 75.57 \tval acc 74.59 \n",
      "  Epoch 8 \tbatch 80 \tloss 0.4910872254 \ttrain acc 73.66 \tval acc 74.63 \n",
      "  Epoch 8 \tbatch 81 \tloss 0.473779782863 \ttrain acc 76.85 \tval acc 74.44 \n",
      "  Epoch 8 \tbatch 82 \tloss 0.466321691212 \ttrain acc 76.66 \tval acc 74.31 \n",
      "  Epoch 8 \tbatch 83 \tloss 0.479162712118 \ttrain acc 76.53 \tval acc 74.25 \n",
      "  Epoch 8 \tbatch 84 \tloss 0.486931238209 \ttrain acc 75.45 \tval acc 74.15 \n",
      "  Epoch 8 \tbatch 85 \tloss 0.502324387542 \ttrain acc 74.36 \tval acc 74.10 \n",
      "  Epoch 8 \tbatch 86 \tloss 0.471582080639 \ttrain acc 76.53 \tval acc 74.14 \n",
      "  Epoch 8 \tbatch 87 \tloss 0.486063388349 \ttrain acc 76.15 \tval acc 74.20 \n",
      "  Epoch 8 \tbatch 88 \tloss 0.463809450932 \ttrain acc 78.38 \tval acc 74.18 \n",
      "  Epoch 8 \tbatch 89 \tloss 0.461928718593 \ttrain acc 77.10 \tval acc 74.46 \n",
      "  Epoch 8 \tbatch 90 \tloss 0.519813510834 \ttrain acc 74.23 \tval acc 74.58 \n",
      "  Epoch 8 \tbatch 91 \tloss 0.459523315335 \ttrain acc 76.66 \tval acc 74.63 \n",
      "  Epoch 8 \tbatch 92 \tloss 0.489490306664 \ttrain acc 77.04 \tval acc 74.78 \n",
      "  Epoch 8 \tbatch 93 \tloss 0.495099775838 \ttrain acc 74.43 \tval acc 74.59 \n",
      "  Epoch 8 \tbatch 94 \tloss 0.481427537822 \ttrain acc 75.57 \tval acc 74.47 \n",
      "  Epoch 8 \tbatch 95 \tloss 0.513163614621 \ttrain acc 73.15 \tval acc 74.10 \n",
      "  Epoch 8 \tbatch 96 \tloss 0.495240217141 \ttrain acc 75.32 \tval acc 74.17 \n",
      "  Epoch 8 \tbatch 97 \tloss 0.518019301775 \ttrain acc 74.81 \tval acc 74.14 \n",
      "  Epoch 8 \tbatch 98 \tloss 0.469466747952 \ttrain acc 76.85 \tval acc 74.30 \n",
      "  Epoch 8 \tbatch 99 \tloss 0.474298425199 \ttrain acc 75.45 \tval acc 74.38 \n",
      "  Epoch 8 \tbatch 100 \tloss 0.468724829839 \ttrain acc 77.68 \tval acc 74.33 \n",
      "  Epoch 8 \tbatch 101 \tloss 0.505232642695 \ttrain acc 74.49 \tval acc 74.33 \n",
      "  Epoch 8 \tbatch 102 \tloss 0.50749230617 \ttrain acc 75.45 \tval acc 74.40 \n",
      "  Epoch 8 \tbatch 103 \tloss 0.482904668985 \ttrain acc 75.13 \tval acc 74.57 \n",
      "  Epoch 8 \tbatch 104 \tloss 0.50131018567 \ttrain acc 74.55 \tval acc 74.69 \n",
      "  Epoch 8 \tbatch 105 \tloss 0.485907343201 \ttrain acc 76.15 \tval acc 74.61 \n",
      "  Epoch 8 \tbatch 106 \tloss 0.518957132552 \ttrain acc 73.34 \tval acc 74.54 \n",
      "  Epoch 8 \tbatch 107 \tloss 0.460168459627 \ttrain acc 78.19 \tval acc 74.50 \n",
      "  Epoch 8 \tbatch 108 \tloss 0.473139506226 \ttrain acc 75.96 \tval acc 74.36 \n",
      "  Epoch 8 \tbatch 109 \tloss 0.483077063338 \ttrain acc 75.77 \tval acc 74.39 \n",
      "Epoch 9 of 20 took 108.656s\n",
      "  training loss:\t\t0.487680\n",
      "  training accuracy:\t\t75.59 %\n",
      "  validation loss:\t\t0.505351\n",
      "  validation accuracy:\t\t74.39 %\n",
      "  Epoch 9 \tbatch 1 \tloss 0.495502466277 \ttrain acc 75.19 \tval acc 74.32 \n",
      "  Epoch 9 \tbatch 2 \tloss 0.500320592256 \ttrain acc 74.68 \tval acc 74.25 \n",
      "  Epoch 9 \tbatch 3 \tloss 0.492641084614 \ttrain acc 75.38 \tval acc 74.40 \n",
      "  Epoch 9 \tbatch 4 \tloss 0.502524381195 \ttrain acc 73.98 \tval acc 74.50 \n",
      "  Epoch 9 \tbatch 5 \tloss 0.456203925136 \ttrain acc 77.55 \tval acc 74.54 \n",
      "  Epoch 9 \tbatch 6 \tloss 0.496635952085 \ttrain acc 75.00 \tval acc 74.64 \n",
      "  Epoch 9 \tbatch 7 \tloss 0.497005417928 \ttrain acc 76.28 \tval acc 74.55 \n",
      "  Epoch 9 \tbatch 8 \tloss 0.508252051483 \ttrain acc 74.17 \tval acc 74.61 \n",
      "  Epoch 9 \tbatch 9 \tloss 0.474348089737 \ttrain acc 75.83 \tval acc 74.60 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-68f74cbc0189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_problem_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_problem_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_acc_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_problem_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mallow_gc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_gc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_gc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0m\u001b[1;32m    834\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m    835\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training!!!\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in utils.iterate_minibatches(X_train, next_problem_train, truth_train, batchsize, shuffle=False):\n",
    "        X_, next_problem_, truth_ = batch\n",
    "        err, acc = train_acc_fn(X_, next_problem_, truth_)\n",
    "        train_err += err\n",
    "        train_acc += acc\n",
    "        train_batches += 1\n",
    "        val_loss, val_acc = check_val_loss_acc(X_val, next_problem_val, truth_val, batchsize)\n",
    "        print(\"  Epoch {} \\tbatch {} \\tloss {} \\ttrain acc {:.2f} \\tval acc {:.2f} \".format(epoch, train_batches, err, acc * 100, val_acc) )\n",
    "    train_acc = train_acc/train_batches * 100\n",
    "    train_accuracies.append(train_acc)\n",
    "    train_loss = train_err/train_batches\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    val_loss, val_acc = check_val_loss_acc(X_val, next_problem_val, truth_val, batchsize)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} %\".format(train_acc))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_loss))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_acc))\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "print(\"Testing...\")\n",
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in utils.iterate_minibatches(X_test, next_problem_test, truth_test, batchsize, shuffle=False):\n",
    "    X_, next_problem_, truth_ = batch\n",
    "    err, acc = compute_cost_acc(X_, next_problem_, truth_)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAaMAAAE+CAYAAAAgbX9pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzsnXecVOXVx78/QEXBhl1EURAUG2rE8qohaiLGGkuUJEY0\n",
       "ltdEUyyvmpgsG00hxZIYExVLorF3EzUaFaOx00QFFRVBsIJiL8h5/zh3mNnZ2d3Z3blzd2bP9/O5\n",
       "n51bnueeGZj7m/M85zlHZkYQBEEQZEmPrA0IgiAIghCjIAiCIHNCjIIgCILMCTEKgiAIMifEKAiC\n",
       "IMicEKMgCIIgc3plbUB7kRSx6EEQdHvMTFnbUElqToyg/v4RgiAI2kM9/iiPYbogCIIgc0KMgiAI\n",
       "gswJMQqCIAgyJ8SoQkiaJWnXrO0IglpF0khJc7K2Iw0kPSVp56zt6MrUZABDF8WSLQiCoAlmtmnW\n",
       "NnR1wjMKgiBoA0nxwz1lQowqjKSlJZ0jaW6ynS1p6eTcqpL+IeltSfMl/aeg3SmSXpH0rqQZknZJ\n",
       "jkvSqZJmSnpL0jWSVk7O9ZZ0RXL8bUmPSVo9m3ceBEv+H19XdOxcSecmrw+X9Ezy//wFSUe3o+9z\n",
       "Jc2WtFDSE5J2LDjXQ9KPk+/Ju8n5dZJzm0i6O/nOvSbp1OT4ZZLOKOijyTBhMvT+f5KeBN6T1LPg\n",
       "u/iupKcl7Vdk41EF7+9pScML+to1eR3f6RKEGFUWAacDI4Atkm1EcgzgRGAOsCqwOnAagKShwPeA\n",
       "L5jZCsBXgFlJm+8D+wA7A2sBbwN/Ss4dBqwArAP0A44BPkrrzQVBGVwFfFVSXwBJPYGDgL8n518H\n",
       "9kz+nx8OnC1pyzL7fgz/Tq0MXAlcl/uhh3+3DgH2KOj7Q0nLA/8Gbse/P4OBe5I25QytHwLsAaxk\n",
       "Zp8DM4Edk3s0AldIWiN5rwcBDcChyfl9gAUl7hXf6RLUn+tZqcVgHV9Y+w3gODN7y81RI3AB8DPg\n",
       "U/w/30AzewH4b9Lmc2AZYBNJ881sdkF/xyT9zSvo72VJhyb9rQJsaGbTgMkdtDmoM9RYme+BNbTv\n",
       "e2BmsyVNAr4GXA7sAnxoZo8l528vuPY/ku4CdqKM/7tm9veC3bMknQ4MBaYBRwInmdnzybXTACSN\n",
       "BuaZ2dlJu0+Bxwv6ae39GfAHM5tbYMP1Ba+vlXQa/oPztsSGcWY2MTn/Qgv9xne6BPUnRtlnZ1gb\n",
       "eLlgf3ZyDOC3wFjgLkkAF5rZODObKemHyblNJP0LOMHMXgUGAjdJWlzQ5yLcs7ocGABcLWkl4Arg\n",
       "J2a2KKX3FtQI7RWRCnMlMBr///kN8l4RkvbAvYcN8ZGZ5YAny+lU0knAEfj3yXAPYtXk9DpAqYf/\n",
       "AODFjryJhCbRfZK+DfwI/14C9C3DhmIGEt/pZsQwXeWZR/4/KsC6yTHM7H0zO8nMBuFu+gm5uSEz\n",
       "u8rMdgLWw79o45L2s4FRZrZywbacmb1qZovM7OdmtgmwA7AX8O1qvMkgaIXrgZGS+gP74eKEpGWA\n",
       "G4DfAKub2cr48FmbwilpJ+Bk4CAzWylpu7Cg7Rx8CK6Y2cAGLXT7AS6GOdYscc0SD1PSesCF+JB6\n",
       "v8SGp8qwoZRN8Z0uIsSo8lwFnJ4EK6yKD89dDiBpL0mD5W7Ru/jw3OeShkjaJfmyfgJ8nJwD+Avw\n",
       "S0nrJn2sJmmf5PVISZsl4/LvAZ8VtAuCTDCzN4EJwGXAi2b2bHJq6WR7C1iceElfKbPb5XHv4S15\n",
       "kNDPcM8ox3jgjNz3S9LmkvoB/wDWkvQDSctIWl7SiKTNFHx+a2VJawI/bMOGPrg4vQX0kHQ4UBiy\n",
       "PR44SdJWiQ2Dc9/bIuI7XYIQo8piwJnAE/jQw5PJ6zOT84OBu/H/ZA8BfzKz+/H5ol8BbwKv4m7/\n",
       "aUmbc4Fb8aG9d4GH8TFq8F9y1+G/EJ/BHwCXp/bugqB8rgR2Tf4CYGbv4ZP31+IT+6OBW4ratTTX\n",
       "dWeyPYcH93yEexg5zkr6vQv/PlwE9Daz94EvA3vj363ngJFJm8uBqUl/dwJXt3J/zOwZ4Pf4d/A1\n",
       "XIgeLDh/PfCL5D2/C9yIB1sUE9/pEsisttZpSrLI2h0EQXemHp+D4RkFQRAEmRNiFARBEGROiFEQ\n",
       "BEGQOSFGQRAEQeaEGAVBEASZE2IUBEEQZE6IURAEQZA5IUZBEARB5oQYdTEk/TnJRtyt6K7vOwgC\n",
       "JzIwVBBJs4AjzOzerG0JgqB+6crPwY4SnlFlMVrJQKw6L10sKf4/BUHQIeLhUSEkXY6Xi7hN0nuS\n",
       "TpI0UNJiSUdIehmvOImk6yS9KukdSfdLGlbQz5JSyEkG31cknSDpdUnzJI1pxYbD1UpJZ0n7Spoi\n",
       "L9s8U9LuyfF+ki6Vl0lfIOmm5PgYSQ8U9bFY0gYFtv5Z0u2S3sfLBuwpaXJyj9mSGora7yjpIXlJ\n",
       "5dlJfZhSJaD3Smx9W9J/JW1WcK5kifYgCGqXEKMKYWaH4lmE9zKz5c3sdwWndwY2AnZP9v+JZ/Be\n",
       "DZhEQfExmpdCXgNPlb828B3gT5JWbMGMFks6J2nz/wqcaGYrJjbNStpdDvQGhuEFvs5qx1sfDZxh\n",
       "Zn3xyrXvA99K7rEncKykfRMb1sPr15yLZyYfjmdNbvK+E5svBo7CSy9fANwqaSm1XqI9CIIape7E\n",
       "SMIqsVXYrLFm9pGZfQJgZpeZ2Qdm9hnQCGwhafnCt1Hw+jPg52b2uZndgT/sh5a6iZndbmYvJa//\n",
       "g6fT3yk5/R3gYjO7Jzk/z8yelbQWMAr4XzNbmBT3eqBU/y1ws5k9nPT5iZndb2ZPJ/vT8LT8X0yu\n",
       "/QZwt5ldk7yfBWY2tUSfRwMXmNnj5vwNr/O0PV7TJleifSkzm21mnankGQRBF6DuxMgMVWKrsFlL\n",
       "ShdL6iHp18kw2ULgpeTUqqWbMt/MCssTf4iXOm6GpD0kPSJpvqS3ga8CqySnWyvLvMDMFrbj/eQw\n",
       "mpdl3lbSfZLekPQOcEyBDeWWgF4PODEZons7eS/rAGuZ2Qt4EbSxwOuSrkoENQiCGqbuxChjWvKo\n",
       "Co9/Ey85vmsylLV+clwtXF8Warukc0slkecA/VoY+mtSljmphtkWVwI3A+uY2Up4VcucDbOBQWX0\n",
       "MRv4RVFZ5r5mdg20WqI9CIIaJcSosrxO2w/bvviQ0wJJfYBfFp0XrUTktUJbJZ0vBg6XlzfvIam/\n",
       "pKFm9ipwB3C+pJWSeZmdkzZT8eGwLST1xr2RYltLvb+3zezTZJ7qGwXnrgR2k3SQpF6SVpG0RYn3\n",
       "fRHwv5JGyOmTBEb0Vesl2oMgqFFCjCrLr4DTk6GlE5JjxV7O34CXgbnAU3jJ4cJrigMYyvKS2irp\n",
       "bGaPkwQ1AO/g5YzXTU4fis9NzcAF9ftJm+eAn+NRgM8CD7RhK8B3gZ/Lyyn/FLimwIbZ+NDhicB8\n",
       "YDKweXFfZjYRD144L3kvzwPfTq5rrUR7EAQ1Six6DYIgqDHq8TkYnlEQBEGQOSFGQRAEQeaEGAVB\n",
       "EASZE2IUBEEQZE6IURAEQZA5IUZBEARB5tRkSQNJtRWPHgRBELRKza0zCoIgCOqPGKYLgiAIMifE\n",
       "KAiCIMicEKMgCIIgc0KMgiAIgsxJTYwkhkpMLtgWSp4NWuJ4iekST0lRiyYIgiA1pFFIM5CeRzql\n",
       "xPmRSAuRJifb6cnxoQXHJifXfD81M6sRTSfRAy+ZMAIv8PZj4KtmfCaxmhlvpm5EEARBd0PqiZd/\n",
       "2Q1/Bj8OjMZsesE1I4ETMNunlX7yz3CzOS1e1wmqNUy3GzDTjDnAscCvzPgMIIQoCIIgNUYAMzGb\n",
       "hdlnwNXAviWua6scxW7AC2kJEVRPjA4BrkpebwjsLPGIxASJL1TJhiAIgu5Gf6BQQF5JjhViwA5I\n",
       "U5FuRxpWop9D8ErNqZG6GEksDewNXJcc6gWsbMZ2wMl4ZdIgCIKg8pQzDzMJGIDZFsAfgZubnJWK\n",
       "n+GpUI10QHsAEwuG414BbgQw43GJxRKrmDG/sFGk/AmCIGg/RRVg5wIDCvYH4M/gwgbvFby+A+l8\n",
       "pH6YLUiO7gFMxCzVKZVqiNFo8kN04Kq7C3C/xBBg6WIhytHVy+pKGmtmY7O2oy3CzsoSdlaWWrCz\n",
       "FmyEkj/inwA2RBoIzAMOxp/JhY3WAN7AzJBGACoQImj+DE+FVMVIog8+8XVUweFLgEskpgGfAt9O\n",
       "04YgCIJui9kipOOAfwE9gYsxm450THL+AuBA4FikRcCH+PyQI5V6hqdCqmJkxgfAqkXHPgMOTfO+\n",
       "QRAEQYLZHcAdRccuKHj9J+BPLbRt9gxPi8jA0DkmZG1AmUzI2oAymZC1AWUyIWsDymRC1gaUyYSs\n",
       "DSiDCVkbUO902RISkqyrzxkFQRB0JWr5uRmeURAEQZA5IUZBEARB5oQYBUEQBJkTYhQEQRBkTohR\n",
       "EARBkDkhRkEQBEHmVCMdUBAEQVAhJHoCawPrAusV/a1ZYp1REARBF0JiOUoLzXrJthbwFvAyMLvp\n",
       "X91Wq8/NEKMgCIIqISE8vU6huBQLTl+8BlGh2BS+fsWMT0v3X7vPzRCjIAiCCiGxFF68rtibKRSc\n",
       "jygtMrm/b5iVVYeoxP1r97kZYhQEQVAmEsvT8vDZusAawGuUFpmXgTlmvNe850rZV7vPzRCjIAiC\n",
       "BIllcGHZoMS2HrA0rXs185LKBJlQy8/NEKMgCLoNyZzNapQWmw1wz2YO8GLR9hIuNvM7OoRWDWr5\n",
       "uRliFARBXSHRGxhIy4LzCfACzQXnRTw4YFH1ra4MtfzcDDEKgqCmSLybNWhZbFbDh81Kic1LZryT\n",
       "gdlVoeRzUxoFnINXeh2P2bii8yOBW/DPB+AGzM5Mzq0EjAc2AQw4ArNHUrG9S4sRLIvZx1nbEgRB\n",
       "dZFYlrx3M4imYrM+Xh67lNjkvJvPq2919jQTI6kn8CxeOnwu8DgwGrPpBdeMBE7AbJ8SHf4VuB+z\n",
       "S5B6AX0wW5iG7V09A8P+wJVZGxEEQeWRWAHYlOZiswGwCj5HUygy95P3bt7NwuYaZAQwE7NZAEhX\n",
       "A/sC04uuaz4KJa0I7ITZYQCYLQJSESLo+mJ0NCFGQVDTJOlrBgGbF2xbAKsDzwDP4SIzAbgkeT2v\n",
       "u3o3FaY/HpCR4xVg26JrDNgBaSruPZ2E2TO4B/om0qX4v9dE4AeYfZiGoV1djDZCGorZs1kbEgRB\n",
       "20isTFPR2Ryfb3gTmAo8CVwB/B/wQghO55APsY1s5ZJy5mEmAQMw+xBpD+BmYAiuD1sBx2H2ONI5\n",
       "wKnAzzpldAt09TmjcUBPzE7O2p4gCPJI9AI2xH8xFwrPysA08sLzJPCUWXrDO0GeEnNG2wFjMRuV\n",
       "7J8GLG4WxNC0k5eArfE1VQ9jtn5yfEfgVMz2SsP2ru4ZjQceQjods0+yNiYIuiMSq5IfWsuJzsb4\n",
       "kE5OcMYnf2eZsTgjU4PmPAFsiDQQmAccDIxucoW0BvAGZoY0AhBmC5Jzc5CGYPYcHgTxdFqGpiZG\n",
       "EkOBqwsObYC7dysDR+JuO8BpZtxZshOzmUjTgP2Aa9KyNQgCkFgaGErTeZ3NgeXIi87DwF+Ap814\n",
       "PyNTg3IxW4R0HPAvPLT7YsymIx2TnL8AOBA4FmkRHqV4SEEPxwN/R1oaX5t1eFqmVmWYTqIH/itq\n",
       "BHAE8J4ZZ7XeJnE3pYOBozHbNXVDg6CbILEmzQMKhgCzyAvPk/hw25yunHUgyFPL6zOrNUy3GzDT\n",
       "jDnJgrX2fFg3A39EGozZzHTMC4L6JMm1NozmQQW9yM/rTAD+gHs7H2VjadDdqZZndAnwhBnnSzTg\n",
       "rt5CfDzzxFIropsovPQ7YBFmp6ZubBDUKEkanOH4CMSI5PUgfHilMKDgSTx0OrydOqOWPaPUxSgZ\n",
       "h54LDDPjTYnVyc8XnQGsZcZ3mreTAY0Au8Iq/4RvLgNrYlayqFQQdCeSoe+h5IVnBO4BPQs8iq+0\n",
       "nwRMNyOCf7oJIUat3UDsCxxrxqgS5wYCt5mxWfNzzUIUJwDnYXZ9etYGQddEYi1ccLZN/n4BmA88\n",
       "lmyPAlPMSGVBYlAb1LIYVWPOaDRwVW5HYi0zXk12v4avSSiHC4GjgBCjoK6R6IuLTc7j2RaPaHsU\n",
       "F56zgMfNlowwBEHNk6pnJNEHzy+1fq66ocTf8LFsw2uEHGPG683bNvOMeuNpLUZg9lJqRgdBFUkW\n",
       "j25KU69nA3yOJ+fxPAa8GHM8QVvUsmfUtTMwNE+FfjbwIWY/ycaqIOg4SSTpQJp6PMPxcgc50XkM\n",
       "mGZGzI0G7SbEKAVaEKNhwL+B9TDLrLRvEJSDRD9gG/IezwhgEXnheRSYGKlygkoRYpQCLX6o0gPA\n",
       "7zG7ufpWBUFpSoRVb4sXgJtIgfiYMTczI4O6J8QoBVoRo28Dh2D21epbFQRLwqqH0NTjyYVVF87z\n",
       "zIis1M1Ro3rgw5Ub42UKngYesQaLBbedJMQoBVoRo2XxmhxbYfZy1Q0Luh1JPZ7hwC7ArsB2wAKa\n",
       "zvNMjrDqpqhRy+CivTGwUfJ3Yzzb91t4gbeXgc3wrBAT8WwQEwhx6hAhRinQ6ocq/QF4G7OG6loV\n",
       "dAeSQIOhuPDsgteLeR24F7gHeDDCqvOoUSuSF5pC0RmAR8xOL9qetQZ7v6iPvsAO+Gc9khCnDhFi\n",
       "lAJtiNFmwB3AwKQUbhB0Col1yXs+u+CBBvfgAnSvGfMyNC9z1CgBa9FccDYGVgBm0FRwZgAzraFj\n",
       "gUYtiNMk8uL0cIhTc0KMUqDND1V6GPglZrdVz6qgXpBYDfgSeQFaibzncy9ehbRrfjlSRI3qia9z\n",
       "KhacjYBPyQtNofC8Yg2Wag2jEKfyCDFKgTLE6HBgf8z2rp5VQa0isTywM3nPZ33gAVx87sGrkXab\n",
       "onBq1LLk53MKt8H4kGTx0Np0a7D52VjbnBCn0oQYpUAZYtQHz8iwOWavVM2woCZIQq23J+/5bI4H\n",
       "GuQ8nyfMqPu1ampUP5p7ORsDa+PZvItF5zlrsA+ysbbjqFF9aCpOW9ANxSnEKAXK+lClPwGvY/bz\n",
       "6lgVdFWSiLetyXs+2wHPkPd8HqrnWj1q1FK4yAwv2DbBc9qVGlp7saPzObVAdxWnFpIFjALOwSu9\n",
       "jsdsXNH5kcAtwIvJkRsxOyM5Nwt4F/gc+AyzEanZXuNiNBy4FVgfs1jP0Y1IIt6G4eKzKz4E9wr5\n",
       "eZ/76zWzQRK9tgVNhWcjPEx6SrJNBZ4C5lpDF/2SV5HuIk4lcnr2xNe/7YaX8nkcGI3Z9IJrRgIn\n",
       "YLZPiQ5fArbGbEGqhlO9Sq/pYDYF6TVgd+D2rM0J0kVifZpGvH2IC8/VwNGlEu7WMkkE2wCais5w\n",
       "YHU82/0UfK3TBcBTtTi8Vi2Sz+buZCsWpzOALdSouhMnfEH2TMxmASBdDeyLe8eFtPbDvyrDfrXt\n",
       "GfmFRwJ7Yva19K0KqonEGrjo5ARoOQoi3syom+ztLQyzDQc+Ie/t5LaZ1hAjAZWkDM/pEWuwLr+o\n",
       "uYRndCCwO2ZHJfvfArbF7PiCa74I3IiPLMwFTsLsmeTci3hV7s+BCzC7KDXb60CM+uKBDJtg1q3X\n",
       "gtQ6EisCXyQ/9LYOcD/5eZ9n6iHcupVhtlnkh9imAFOtwV7LyMxuTQlx2gZYDHwEfJz8bWmr1PmP\n",
       "2xpilQ+xjSw41FAkRgcAo9oQo+WBzzH7EGkP4FzMhiTn1sLsVaTVcK/yeMweaOPj6xC1L0Z+8QXA\n",
       "bMx+ka5VQaWR6A/sDxwIbAU8Qt77mWRGzS5qbmOY7UmaejtP1cIv7+5K8m+5DLBsC1vvVs6Ve03x\n",
       "+aVxz7h8MRvLd4vEaDtgLGajkv3TgMXNghiavNkW5omkBuB9zH5f9gfXDupFjLbGK8AOwtJdfBd0\n",
       "niTbwQG4AG2MB6HcANxtxsdZ2tZR1Kilce+mWHg+pvkw2wsxzBa0RZJQtlCg2ha8sfyxSIx64QEM\n",
       "uwLz8OUNxQEMawBvYGZII4BrMRuItBzQE7P3kqU0dwGNmN2VyvutCzHyBhOB09L6oILOIbEBeQEa\n",
       "BNyM/4C4t9YKyalRK5EfZsv9LRxmWxLRZg1WV0EVQdemhdDuPciHdl+M2a+QjgHA7AKk7wHH4imw\n",
       "PsQj6x5B2gCfSwIPdvs7Zr9KzfY6EqP/BXbD7MD0rArag8QQ8gK0DnATLkD3V3PBaTLEshywPNC3\n",
       "lb/lnFseH67JRbPFMFvQZYhFrynQATFaAV9nsREWv0azQmJjXHwOxOdGbsAF6IFya/skQ16dEYzi\n",
       "v8vhw2XvA++18Le1c4XXvA/Mj2G2oCsSYpQCHfpQpfHA861OzgUVJVl8uil5AVoRF5/rgYdbE6Bk\n",
       "THw4sEeybYyLh3ABKEcoyhGRD0I8gu5AiFEKdFCMRgBXAkMikCE9EgEaTl6AliEvQI+1lnA0yZX2\n",
       "FVx8dgfeAe7ES4JMAt61Bvsk1TcQBHVKiFEKdFCMhI/fn4DZPakY1k1JBOgL5AUI4DpcgCa2tP4n\n",
       "8X62Iu/9bIqvHboDuNMa7MVS7YIgaD8hRinQ4Q/VI0N2xuzgylvVvZDoAWxLXoA+Ji9AU1sRoFVp\n",
       "6v3Mx8XnDuABa7CaDN8Ogq5OiFGpjsVQPGdYjg2An5rxh+T8icBvgVXNaJaErxNitBJe6ngIZlEa\n",
       "up0k2a93wMXnAHwYLTcE93QpAUoKsm1N07mfCeS9n1nVsD0IujshRm3dxH9hzwVGmDFHYgBwETAU\n",
       "2LqiYuSNLwWexux3nTC72yDRC9gJF6D98eJq1wM3mDVLqOhtGrUa7vXsgXtBb5D3fh6MeZ8gqD61\n",
       "LEbVytq9G17GeU6yfxbwf3gNjTS4CLgM6fd01XHIjJFYCs9pdSDwNWA2LkA7m/F8s+vd+9mGvPcz\n",
       "FE/bcwdwmjXY7OpYHgRBPVItMToEj3JDYl/gFTOeVHr6/TDwKZ50c0Jqd6kxJJbG04IciKeRn4kL\n",
       "0LalMmCrUWuQ936+DLyKi88pwH+twWoqc0IQBF2X1IfpkgfgXLwQ2gfAfcCXzXhX4iXgC2bMb95O\n",
       "BjQWHJpgZhPacePv49lpv9kJ82ue5PPfHRegvfHqp9cDN5rRxJtRo3rh9U9y3s9gPGHpHcC/rMHm\n",
       "EARBl6WWh+mqIUb7AseaMUpiM+DfeP4j8BQxubmkN5q26+SHKvXDy+gOwqyZ2NU7EhsCRwJj8ESJ\n",
       "1+IC1KTMhhq1JjCKvPczh/zcz0P1XJo6COqNEKPWbiCuBu4w468lzr1EGgEM+U4uByZhdnan+qkR\n",
       "JJYB9gOOxtfz/BW4qHAOKPF+tiPv/ayP/0C4E498m1ttu4MgqAwhRi11Lvrg+eLWN+O9EudfxIfp\n",
       "0hKjnYALgWH1HMiQJCQ9CjgMr5NzIXCLGZ8AqFFrk/d+dsOzS+e8n0fC+wmC+iDEKAUqJEbC50iO\n",
       "wuzBihjWRZDojUfBHY2v67kMGG/GzCXXNGoz4AxgZ9z7ya37ebXqBgdBkDq1LEbViqbLBi8WdSH+\n",
       "wK4LMZLYCPeCvg1MBv4E3FpYE0iN2hAP/tgV+DUw2hrsowzMDYIgKIv69oy8o1XxEOb1MXu70/1l\n",
       "QOIFHYCL6hDgUtwLapLXTY0aAPwM95jOAc61Bms2PBoEQX1Sy55Rj6wNSB2zt4DbgW9lbUp7kRgm\n",
       "cTYe4XYocC6wrhk/LhQiNWoNNepcPEnsm8AQa7AzQ4iCIEAahTQD6XmkU0qcH4m0EGlysp1edL5n\n",
       "cvy2NM2s72G6PBcCf0Q6r6sHMkgsi68JOhovz30JHvpealHqysDJwDHAFcAm1mCvVdHcIAi6MlJP\n",
       "4Dw8cGku8DjSrZgVp/m6H7N9WujlB/jc+/LpGdp9xOh+vObOdnh2hi6HxKb4XNA3gceA3wP/LFWe\n",
       "W43qi/8H+RFeynvLSMcTBEEJRgAzsSRZsXQ1nn2lWIxKD+1J6wBfBX4BnJCWkdBdxMgDGS7CvY0u\n",
       "I0YSywEH4XYNBC7G1129XPL6RvUGjsXT8dwLbG8N1iyPXBAEQUJ/oDBzyit4WZhCDNgBaSruPZ2E\n",
       "2TPJubPx0ZcV0ja0e4iR81fgWaQfYrYwS0OSTBRHA98AHgHGAbebsajk9Y1aCjgc+CleDfUr1mBP\n",
       "VsncIAi6KJJG4gmPW6KcaYlJwADMPkTaA7gZGIK0F/AGZpPx+6RK9xEjszeQ7sKHwc6v9u2TBcBf\n",
       "x0VoAO4FbVmcH65JG8+UPRoYi9doOtAa7NH0rQ2CoBZI8nVOyO1Laii6ZC7+vMkxAPeOCjt5r+D1\n",
       "HUjnI62C1zXbB+mrQG9gBaS/YfbtCr6FJdR/aHfTTnfD52KGVyuQQWILXIBGA//FgynuaMkLAlCj\n",
       "hKf1OQN4F/iJNdh9VTA3CIIaptlzU+qF56bcFZiHz0ePbhLAIK2Be0CGNAK4FrOBRR1/ER++2zst\n",
       "27uPZ+TcC/TF6/I8ltZNJPoCB+MitDYwHtiioJ5T6XYuQl8BzsT/bU4BbreGLvqLIQiCro3ZIqTj\n",
       "gH8BPYGLMZuOdExy/gI8evdYpEV4EutDWuotTVO7l2fkHZ+KZ/I+qvJdsyUuQAcDD+Be0J1mfN5m\n",
       "20btiEesrI4vXL3BGmxxpW0MgqB+qeVFr91RjNbEwxrXw+zdznfH8vgviaNxIRkPXGpWNC7bUvtG\n",
       "bY17QhvhKXyusAZrcQgvCIKgJUKMUiDVD1W6AbgrcVE72AW98fLpo/EJxAuBu8rxggDUqGH4nND2\n",
       "uEc03hrsk47aEwRBUMti1N3mjHJcCPwS6LAY4eOsmwObFBesaw01agM8Om4U8FvgUGuwD1ttFARB\n",
       "UOd0VzG6G7gAaWvMJnawjzHAH8oVIjWqP3A6vsj1j8Bga+j8MGEQBEE90D3FyGxxkpHhKKDdYiSx\n",
       "LrAlcGub1zZqNeBUXLwuBoZaQ/crgx4EQdAa3VOMnEuBp5FOwuz9drY9FLjWjI9bukCNWhE4Efge\n",
       "cDWwmTVY2cN5QRAE3YnuK0Zm85DuxyPhxpfbTEJ4ee9DS55vVB/geFyI/gFsbQ1JksIgCIKgJN1X\n",
       "jJwLgQbaIUZ49NtiihbNqlHL4OHdp+FVZXeyBptRITuDIAjqmjaL60n8VmIFiaUk7pF4SyrtFdQg\n",
       "/wLWQhomLW/oAAAgAElEQVTejjZjgMvMfDWyGtVLjfoO8BywO7CnNdjXQ4iCIAjKp811RhJTzdhC\n",
       "4mvAXnhNiwfM2DxVw6oVL++JBVfH7HttX8qyeOLBzRmreXji00Y859NPrMEeStXWIAiCVqj3dUa5\n",
       "a/YCrjdjoZRujqIqcwkwBelkrM31PvsBjzNWKwNXAcviAQr3RP64IAiCjlOOGN0mMQP4GDhWYvXk\n",
       "dX1gNgfpIdzLuazVa3svOIJv7vUJcA+eP+4ia7CyMi4EQRAELVNWOiCJfsBCMz5P6vIsb8ZrbbQZ\n",
       "ioc059gAf4Cvgpe9NWA+MKZUNuuqupvS3sBpmO1Q8nSjxGubHclyC/5CnzeupOdnJ1qDvVEV24Ig\n",
       "CMqklofpypkz6oPPE61rxlESGwJDzfhH2TcRPfC5lhHAO2a8lxw/Hi+tcGTzNlUVo17ALGAUZk81\n",
       "OdWoQcB5vL/6Vtx5zqM2bfQ+VbEpCIKgndSyGLUZTYcvDv0Ur/oHPln/i3beZzfgBTPm5IQooS/w\n",
       "Vjv7qjxmi/C5oyVlJdSo3mrUz4BHMd3HWXMW8NTocZnZGARBUMeUI0aDzBiHCxJmfNCB+xwCXJnb\n",
       "kfiFxGx88eivO9BfGlwMfBNpWTXqy8CTwHBgKxoX38/ipXsBES0XBEFtIY1CmoH0PNIpJc6PRFqI\n",
       "NDnZTk+O90Z6FGkK0jNIv0rTzHICGD5JQpoBkBgElF3qQGJpYG+8aikAZvwE+InEqcDZwOGl22ps\n",
       "we6EpN57Opi9/OFSerJxJPfhdYm+bw32DwCN5VTgr7m1RUEQBDWB1BM4Dx+dmgs8jnRrk7Ljzv2Y\n",
       "NZ2CMPsY6UuYfZhMZTyItCNmD6ZhajliNBa4E1hH4krgf/CFn+WyBzDRjDdLnLsSuL2lhmY2th33\n",
       "6TBqVC/guIO+xhd++W/e+c2ObJQr65DULToYT4waBEFQS4wAZmJJSjLpajyArFiMSs8z5Ze7LI2X\n",
       "LV+QhpFQxjCdGXcBB+Dey5XA1mbc1457jMbX5ACQBEDk2BeY3I6+Ko4atT3wBLD3I+uw/eC36WVj\n",
       "Wa/gkr2ByWbMzsbCIAiCDtMfmkQrv5IcK8SAHZCmIt2ONGzJGakH0hTgdeA+zJ4p665SP6R2JUZo\n",
       "UYwkNk7+bg2sC7yabOtKbFWePfTB3cMbCw7/SmKaxBRgJJ5QtOqoUauoURcBNwDjgN1mn2VP4wEb\n",
       "RxVcOgb4a/UtDIIg6DTlTC1MAgZgtgVea+3mfGtbjNlwYB1gZ6SRLfYi3Y+0AlI/vDTPeKSzyzW0\n",
       "tWG6E/CH8u8p/Ya+1FbnSbDDqkXHDizXuDRQo3rgAvMr4BpgY2uwhQWXXAw8ivRjYSvjUYRfr7qh\n",
       "QRAEbSAXh5GtXDIXGFCwPwD3jvKYvVfw+g6k85H6Ybag4PhCpH8CXwAmtHCvFTF7F+lI4G+YNSBN\n",
       "K/e9tChGZu4dmLX6RmsKNWpz4HxgKWAPa7BJzS4yexFpMrA/sDZwUwcjCIMgCFIlCeqakNuX59os\n",
       "5AlgQ6SB+LKcg/GpkzzSGsAbmBnSCECYLUBaFViE2TtIywJfxnNxtkRPpLXwH++n50ws9720GcAg\n",
       "8T3gSjPeTvZXBkabcX65N8kaNWp5PBDjUOCnwPg20vhcaJ5zbjXgu+lbGARBkAJmi5COwysU9AQu\n",
       "xmw60jHJ+QuAA4FjkRYBH+JLcQDWAv6K1AOf0rkcs3taudvPk/v8F7PHkAYBz5dratlZu4uOTTGj\n",
       "PWUX2k0lVhKrUcKDL87G88n9X1lpfKSlP2WpeVsy+aNn2GQ9MxZ3xo4gCIJqUO8ZGHok6XwAkOiJ\n",
       "D3N1adSowXjY+Fjgm9ZgY8rOJ2f26S3s+/I4TpkdQhQEQVAG0lCke5CeTvY3X7KAtpzmZXhGv8Oj\n",
       "6S7AY9GPAWabpRsF11GFV6N64wtsj8ej5M6xBvusffdmmaHMePUpNv28F5+vg1nZi3yDIAiyIlPP\n",
       "SPoPcDLwF8y2RBLwFGablNO8nEWvp+DltI9N9u+mfWW6q4Ya9RXgT3gqny2twZplAy+TPZ9loyd7\n",
       "8fnn+FqoaytlYxAEQZ2yHGaPokQLPSCibEegTTEy43Pgz8nWJVGj+uPzQl8AjrMGazGrQ5mMwWsb\n",
       "fYQLcYhREARB67yJNHjJnnQgvja1LMoZphsC/BIYBkty1JkZG7Tb1HZQjruZpPE5HvgJHrL9K2uw\n",
       "jzp3X9YAngUGGPoUX728PWYvdKbfIAiCtMl4mG4QcCGwPfAO8BLwzSWpiNqgnGG6S4EG4Cx8oesY\n",
       "PEQwU9SoHXBv7U3gf6zBnq1Q198AbvFSFwbS34AjgdMq1H8QBEH94T/Yd0Xqi69Veq+tJoWUE023\n",
       "rBn/BmTGLDPGAnu239LKkKTxGQ9ch2dR+HIFhQi8rEVh+p+LgMORlq7gPYIgCOoL6YdIKwAfAOcg\n",
       "TULavdzm5XhGHyfh3DMljsNX8fbpmLUdJ0njczg+ZHg1MKwojU/n7yGGAytTmO7C7FmkGXjC1Bsq\n",
       "eb8gCII64gjMzkkEqB/wbeByfCFsm5QjRj8AlgO+D5wBrIB7D1UjSePzZ3x4cJQ1WFqZvg/D6xYV\n",
       "ry26CA9kCDEKgiAoTW6uak88W8NTSyLrymncWgBD4hGNM+OkTpnYASQZY1kBz4X0LTzX0XhrsFQW\n",
       "oSZFAF8BdjBjZtHJ3nggwwjMXkrj/kEQBJ0l4wCGy/B8nhsAW+DOw32YbV1O81Y9IzM+l9hRQhlV\n",
       "OZ2Or2vaxBqsVHG+SrIH8GwzIYJcxcMrgO+QTwAYBEEQ5DkCL0L6AmYfIK1CC1W8S1HOMN0U4BaJ\n",
       "6/AkeuCh3Te20qZSjLYGe6AK94HmgQvFXAT8G6kRa19GhyAIgm7A9sBUzN5HOhTYCjin3MblrDO6\n",
       "LHnZ5EKz8hWvI1TT3ZRYFZgJrGdGy0ER0oPA7zC7ucVrgiAIMiLjYbppwObJdhmeqefrmH2xnObl\n",
       "ZGAY0wnzaoXRwD9bFSLnQrzgYIhREARBUxYlKYD2A/6E2Xik75TbuJx6RpcWHTIAM45on51dmjHA\n",
       "qWVcdx1wNtK6mM1O16QgCIKa4j2kH+MBZzshtavCQzmLXv8J/CPZ7gFWhPqpfCqxGbA6cG+bF5t9\n",
       "BFyJBzIEQRAEeQ4GPsHXG70G9Ad+W27jNueMmjXw2kb/NWP7djVsJ9Ua+0xKZHxqxo/LbLAZcAcw\n",
       "ELNFadoWBEHQHko+N6VReCBBT2A8ZuOKzo8EbgFeTI7cgNmZSAOAv+E/1g24ELM/tGHAmsA2yfWP\n",
       "YWXWkKM8z6iYIXg57ppHohfuUrYWRdcUs2n4mqNRKZkVBEFQGXyo7Dz8eTUMGI20cYkr78dsy2Q7\n",
       "Mzn2GfCjpB7RdsD3Wmibu9fXgUeBg4CvA48hHVSuqeXMGb1PPpLOgNfxGkf1wO7AS2a0N7fdhXhG\n",
       "hn9U3qQgCIKKMQKYuSRztnQ1XqNtetF1zUehfKjtteT1+0jT8UWtxW1znA5ss8QbklbDp3auK8fQ\n",
       "Nj0jM/qasXyyrWDGhmZ1kxZnDCwJXW8P1wI7Iq1TUWuCIAgqS398JCfHK8mxQgzYAWkq0u1Iw5r1\n",
       "Ig3EF7Q+2sq9hFdRyDGfUiLXAm2KkcTXJFYq2F9JYr9yOpcYKjG5YFso8QOJ30pMl5gqcaPEiuUa\n",
       "XCkk+gFfAa5pd2OzD/BkramutQqCIOgk5QQFTAIGYLYF8EeKl654SYjrgR9g9n4r/dwJ/AtpDNLh\n",
       "wO34/HpZlLPodaoZWxQdm2LG8HJvkrTpAczF3caNgHvMWCzxawCzpqHVaQcwSHwX2NmMQzrYwebA\n",
       "fXidp/OwymYQD4IgaAt58MHIgkMNTZ6b0nbAWMxGJfunAYubBTE07fQlYGvMFiAthU9H3IFZ69kU\n",
       "JAH7AzviIvgAZjeV+17KSQdUShA6UlxvN+AFM+bQ1G18FDigA/11lsPwooEdw+xJpB2BHwMvIJ0P\n",
       "nIPZggrZFwRB0CpmNoGCkjeSip9pTwAbJsNs8/Dw69FNrpDWAN5IFqyOwAvjLUjE5WLgmTaFKDEG\n",
       "r2zQoWmccqLpJkqcJTFIYrDE2cDEDtzrEHyNTjFH4O5c1ZAYBgzAk7B2HLPpmB2KR5qsDTyPNA5p\n",
       "9c5bGQRB0El8+clxeE2hZ4BrMJuOdAzSMclVBwLTkKbgIeC50aL/waONv4Q0OdmaRxFL7yO918L2\n",
       "brmmljNM1xf4KbBrcuhu4Eyz8he+JuUZ5gLDzPITXBI/AbYya+4ZSTK8fESOCcmvgE4jMQ7ArMJR\n",
       "gdK6eKThaDw+/7eYza3oPYIgCFog09x0naTdi147dBOxL3CsWX5tjsQYPM/brmZ83LxNOh9qUqNp\n",
       "NvBlM56pdP/JTdYGTsKj9a4Bxi0JrQyCIEiJWhajcqLp/l0UTddPKq+MbAGjgasK+hgFnAzsW0qI\n",
       "UubLwNzUhAjAbB5mJwBDgXeAiUiXIG2Y2j2DIAhqmHLmjFY1453cjhkLgDXKvYFEHzx4obD+0R+B\n",
       "vsDdScj3+eX2VwHaqltUOczexOw0YEPgZeAhpL8jbVKV+wdBENQI5cwZTQT2N+PlZH8gcKMZW6Vq\n",
       "WAruZuLhvQxsYMb8SvZdpgErAN8FfgQ8APwCs8lVtyMIgrqkrofpgJ8AD0hcLnEF8B8oM6lo1+Pr\n",
       "wF2ZCBGA2buY/RqvEf9f4B9ItyFtm4k9QRAEXYSyAhgkVsdzsU0BegNvmPGfVA1LxzN6CPilWRfJ\n",
       "KSf1xkPbTwGeBc7ELNXPNQiC+qWWPaNyhumOAr6Pr8uZjK+pediMXVI1rMIfqsQQ3KsbYMZnleq3\n",
       "IkhLA4fiHudc4Azg31Qj1DEIgrqhlsWonGG6H+ApfGaZ8SU8WV4tpr45DPh7lxMiALNPMbsYj767\n",
       "EPgD8DDSXskq6CAIgrqmHDH62IyPACR6mzEDf2jWDMnaom9TrSi6jmK2CLMrgE2B3wO/ACYhHYDU\n",
       "kdpTQRAENUE5D7g5EivjmVzvlrgVmJWqVZXnS8CbZjyZtSFlYfY5ZtfhXmgDcCqeruMbSOXkEwyC\n",
       "IKgp2pWBQWIksAJwpxmfpmWU36tyY59JFOBjZrReMrer4kN1X8HTMq0B/BK4ArOuN+QYBEFm1PKc\n",
       "UVXSAXWESn2oEivg6X8Gm/FW5y3LEBelL+KiNAj4NXApZp9kalcQBF2CWhaj7jAPcRBwX80LEXiK\n",
       "drMJmO0KfAPYGy9f8QOk5TK2LgiCoMN0BzE6jI6VFu/amD2E2Z54PfuRwItIpyAtn61hQRAE7aeu\n",
       "xUhiMF5VtuzStzWH2UTMvoYngB2Oi9LPkFZqo2UQBEGXoa7FCA/nvjLtYIsugdk0zEbjJX83wIfv\n",
       "foG0asaWBUEQtEndipFED2phbVGlMXsWszHANsCqwHNI5yGNjLDwIOiGSKOQZiA9j9S8oKg/GxYW\n",
       "VHM9veDcJUivI01L28y6FSM86mwhnk+v+2H2ImbHAFsAr+OLaF9Dugxpvwh4CIJugNQTOA8YBQwD\n",
       "RiNtXOLK+zHbMtnOLDh+adI2depZjA4DLjOja8auVwuzOZidgdnWwFbAROB4XJhuRhoTQ3lBULeM\n",
       "AGZiNitZl3g1HvRUTOlwcLMHgLfTMy9PXYqRRF9gP+DKrG3pUpjNxuyPSWj4QOAG8uHhE5B+iLR+\n",
       "pjYGQVBJ+gNzCvZfSY4VYsAOSFORbkcaVjXrCqjXOYQDgP+Y8XrWhnRZzBYAlwOXIy2LV+PdD/gJ\n",
       "0lw8/dPNwNTIHh4EXRNJI/GlHS1Rznd3EjAAsw+R9sC/90M6b137qFcxGoOPkwblYPYRcBtwWzLG\n",
       "vAMuTDcCPZBywvQgZouyMzQIgkLMbAIwIbcvqaHokrl4+Z8cA3DvqLCT9wpe34F0PlK/5Adr1ai7\n",
       "YTqJ9YHNoIsU0Ks1PEnrA5idiKcc2huYT9MAiH0jACIIaoIngA2RBiZ10w4Gbm1yhbTGklI10ghA\n",
       "1RYiqEMxwovUXW1G5GvrLJ5+aFpBAMTWeADE94FXkW5COgxplWwNDYKgJD6ScRzwL+AZ4BrMpiMd\n",
       "g3RMctWBeFWAKcA5wCFL2ktXAQ8BQ5DmIB2elql1lShVQsBM4GAznkjHsgAAqR+wJz6ctxs+7nwz\n",
       "cAtmszK0LAi6LbWcKLXexGgn4M/AZt0+pLuaNA2A2Acfk87NMz0ZARBBUB1CjFKgg2J0MTDDjN+m\n",
       "ZFbQFk0DIL6Gr1/ICdN/IwAiCNIjxKilzsVQfJFVjg2An+ERHo14EtNtzJjUvG37PlSJPvgv8mFm\n",
       "vNopw4PK4JOim+LCtB+wLh5YcjNwN2YfZmhdENQdIUbl3Mhzxc3FVwT3ARYDFwAnVkiMvgV8w4yv\n",
       "VsjkoNJI6+Grv/cDvgDcgwvTPzCbn6VpQVAP1LIYVTOabjfgBTPmmDHDjOcq3P8Y6rFuUT1h9jJm\n",
       "f8BsF2B94CZcnF5Cug/p+0gDWu8kCIJ6pJpidAgppeeRWBfYkuL4+aDrYjYfs79htj+wJh5SuiUw\n",
       "BenRpFDg4GyNDIKgWlQlA4PE0vjiyebpy1ttp7EFuxOS1calOBS41oyPO2RgkC0+d3QLcAvSUnjG\n",
       "9QOAB5HewHPo3Qg8FZF5QVCfVGXOSGJf4FizpqnIJe6jk3NGydqiZ4FDzXi0UjYHXQCPzNse2B8X\n",
       "p09wUboBeCKEKQiaEnNGbTMauKqFc5394LbHgyEe62Q/QVfDUxM9iNkJeJbx0fi/9RXAy0jnIO2c\n",
       "iFYQBDVM6p5REnL9MrC+Ge8lx74G/AGvRLoQmGzGHk3ble0ZXQC8ZMavK2580DXxkPFh5D2mtfCo\n",
       "vBuA+5K6LUHQ7ahlz6imF71KLIuHi29uVpSJNug+SINwYdofT33/D3w4764kI3kQdAtCjFKgTDEa\n",
       "DYwxY/cqmRV0daT+eOaH/fHKtnfhHtPtTVLlB0EdEmKUAmWK0Z3A38yiomtQAmk1PFfeAcCOeN2X\n",
       "G4Fbs0iRHwRpE2KUAm19qBL9gWnAOmZEWpmgdaQVgb1wj2k3PODlBuBmzF7L0rQgqBQhRilQhhid\n",
       "Agwy4+gqmhXUA1IfYHfcY/oq8DS5tUxmL2dpWhB0hhCjFGjtQ03WFj0DHGnGf6trWVBXSMsAu+DC\n",
       "tC8e+ZkTpmezNC0I2ksti1GtVnrdBs8e8VDWhgQ1jtknmN2B2ZF4iPjJwNrAvUhPI/0cafiSssxB\n",
       "UGtIo5BmID2P1DwLjjQSaSHS5GQ7vey2lTSzRj2j84F5ZpxZZbOC7oLUA88wf0CyGfnsD49htjhD\n",
       "64KgJM2em74g/Fl8nnQu8DgwGrPpBdeMBE7AbJ+iztpuW0FqzjOS6A0cDPwta1uCOsZsMWaPYHYy\n",
       "MAgXpI+A8cDbSPcijUM6EGm98JyCLsoIYCZms5LF4Ffjw9HFlPr/W27bilBzYoQnXJ1ixuysDQm6\n",
       "CWaG2RTMfobZprg4/QZ4H/g28AjwOtI/kRqQvpqElQdB1vQH5hTsv5IcK8SAHZCmIt2ONKwdbStG\n",
       "VbJ2V5gxRN2iIEvM3gLuTLZceqL++FzmNsCPgG2Q3sGHNnLbRMzezcLkoD6RD7GNbOWScuZhJgED\n",
       "MPsQaQ88tdaQzlvXPmpKjCTWAnYAvp61LUGwBJ94fSXZbgJyc06DyQvUmcAWSHPwNU45gZqKWZQ+\n",
       "CTpEUlZnQm5fUkPRJXOBwoKVA6AodVphZhKzO5DOR+qXXNd62wpSUwEMEicBw8w4IiOzgqDjSL2A\n",
       "TfCx+JxIDQWm09SDegazRVmZGdQuJQIYeuFBCLsC8/AfQsUBDGsAb2BmSCOAazEbWFbbClIznlGy\n",
       "tugw4HtZ2xIEHcIFZmqyXQSAtCwwHBemkXhoeX+kKTQVqBeiflPQbswWIR0H/AvoCVyM2XSkY5Lz\n",
       "FwAHAsciLQI+xKtyt9w2JWrGM5LYGrgOGGxGhNUG9Yu0ErA1ee9pG6Av8AQuTD7MZzYvMxuDLkkt\n",
       "L3qtJTH6I/CWGY0ZmhUE2eBDKTlhyg3zfUJT7+mJSADbvQkxSoHCD1ViGXzibIQZL2VrWRB0ATyC\n",
       "byBNvaetgDfIe0+PAZOiplP3IcQoBYrEaH/g+2athjAGQffGV8wPJe89jcAr4s4AHi3YnosMEvVJ\n",
       "iFEKFInRLcBNZrG+KAjahdQb2BLYtmBbGfeeHsW9p0cxeyMzG4OKEWKUArkPVWJ14DlggBlRqTMI\n",
       "Oou0Ou415cRpBPA2Tb2nSbH+qfYIMUqBAjH6IbClGYdlbVMQ1CW+QHcITb2njfEyLYUC9XwM73Vt\n",
       "QoxSoECMpgAnmHFv1jYFQbfB1z9tRVOBWpHcsF5u89RIQRchxCgFJBnYlsAtwPqxtigIMsbDywvF\n",
       "aRvgLZp6T1NieC87QoxSIBGjc4D3zfhp1vYEQVCED+9tRFOBGgo8RVOBmhnZI6pDiFGpjsVQvP5F\n",
       "jg2AnwJXANcA6wGzgK+b8U7z9jKwN4D/MWNmKkYGQVBZpOXw7BGFAtWX5sN78zOzsY4JMWrrJqIH\n",
       "nj12BHA8nknhNxKnACubcWrzNjKwB83YKXUDgyBID2ktmorTF/DFucXDe59kZmOdEGLU1k3EV4Cf\n",
       "mrGTxAzgi2a8LrEmMMGMjZq3kYEdZcb41A0MgqB6+OLcjWkaWj4EH97LeVCPEdF77SbEqK2biEuA\n",
       "J8w4X+JtM1ZOjgtYkNtv2kYGtpIZC1M3MAiCbJH64NF7ufVPI/DovcLFuY9h9npmNtYAIUat3UAs\n",
       "jQ/RDTPjzUIxSs4vMKNf83YyaJIUdUJSSCoIgu6AtCYesZcTp22AhTT1niZi9mFmNnYxQoxau4HY\n",
       "FzjWjFHJ/gxgpBmvJZVb72tpmK5WP9QgCFLAo/c2pKn3tCmeoSWf2gimY/Z5VmZmSS0/N6shRlcD\n",
       "d5jx12T/N8B8M8ZJnAqs1FIAQ61+qEEQVAnPvbcFeXHaFlgDmEihB2WWWrnsrkQtPzdTFSOJPsDL\n",
       "+KLV95Jj/YBrgXVpI7S7Vj/UIAgyROpH0+G9bYFPyZfVeBSv/fRuZjamRMnnpjQKOAev1joes3Et\n",
       "NN4GeBg4GLMbkmM/AI4EBFyE2bmp2d5V16KFGAVBUBHytZ8KxWk4/mO4cP5pGmafZWNkZWj23PTI\n",
       "xWeB3fC5+8eB0c3Kh/t1d+Nlxy/F7AakTYGrcGH/DLgT+F/MXkjD9l5pdBoEQdBl8F/cLyWbL8SX\n",
       "lgI2I1/36XhgINIUCqP34KUazx4xAs+AMQsA6WpgX2B60XXHA9fjwpNjY3yB8sdJ2/uB/YHfpmFo\n",
       "iFEQBN0P94AmJdtfAJBWwBfkjgAOBs4ClkJ6DPcongNeSLb5NSJS/YE5Bfuv4J5hHqk/LlC74GKU\n",
       "e1/TgDOTYc+PgT1xgU6FEKMgCAIgmUO6N9kcaR1cnLbGH9iDgUGAkHLClNtmJn/nViuaT9JIaLUC\n",
       "djmCeQ5wKmaWDGn6MJ/ZDKRxwF3AB8BkSC9hdcwZBUEQtAd/YPfDRanUtgo+H1VKrF5KM+1RiTmj\n",
       "7YCxmI1K9k8DFjcJYpBeJCdAsCo+b3QUZrcWdf5LYDZmf0nF9hCjIAiCCuLJYjegtFCtC7xOc6Hy\n",
       "zaxZZHH7bt1MjHrhAQy7AvPwYbbmAQz56y8FbsPsxmR/dczeQFoX+BewbVpRiDFMFwRBUEk8I8RT\n",
       "ydYUF4cB5MVpMD5P4/vSx7QkVPBqu+epzBYhHYcLSU/gYsymIx2TnL+gjR6uR1oFj6b7bprh8OEZ\n",
       "BUEQdAV8+G91SntUg4E+wIuUFqqXMfuslp+bIUZBEAS1gLQ8Lc9TrQ28ItigVp+bIUZBEAS1jrQ0\n",
       "sJ7guVp9boYYBUEQ1Am1/NzskbUBQRAEQRBiFARBEGROiFEQBEGQOSFGQRAEQeaEGAVBEASZE2IU\n",
       "BEEQZE6IURAEQZA5IUZBEARB5oQYBUEQBJkTYhQEQRBkTohREARBkDkhRkEQBEHmpCpGEitJXC8x\n",
       "XeIZie0ktpB4WOJJiVsllk/ThiAIgm6NNAppBtLzSKe0ct02SIuQDig4dhrS00jTkK5EWiYtM9P2\n",
       "jM4FbjdjY2BzYDowHvg/MzYHbgJOTtmG1JA0MmsbyiHsrCxhZ2WpBTtrwcaSSD2B84BRwDBgNNLG\n",
       "LVw3Driz4NhA4ChgK8w2wyvFHpKWqamJkcSKwE5mXAJgxiIzFgIbmvFActm/gQNa6qMGGJm1AWUy\n",
       "MmsDymRk1gaUycisDSiTkVkbUCYjszagDEZmbUAHGQHMxGwWZp8BVwP7lrjueOB64M2CY+/i5caX\n",
       "S8qlLwfMTcvQND2j9YE3JS6VmCRxkUQf4GlpyYdxEF4PPgiCIKg8/YE5BfuvJMfySP1xgfpzcsSL\n",
       "3JktAH4PzAbmAe9g9u+0DE1TjHoBWwHnm7EV8AFwCnAE8F2JJ4C+wKcp2hAEQdCdKad66jnAqXil\n",
       "VSUbSIOAHwID8bLmfZG+mY6ZKVZ6lVgTeNiM9ZP9HYFTzdir4JohwOVm/9/evcbYVZVhHP8/FQot\n",
       "rTZSIhIatBFM1aQ0kFopXoKVgAHURMEqlZh4+QBKJJpYQ/D2QY2JmvgFNKW2thBCQSvxRrhIIEZo\n",
       "S6cgLV5QkhZoJQbRpgJp5/HDXoPT8UwH0N21xnl+yaR7ztn79D1n5uz3rDX7XS9v/s/j1WYL2oiI\n",
       "hh3U6VVaAnwJ+5zy/UpgGPsbo/b5EyMJCOYC+4BPAEcBZ2N/rOy3AliCfWkfcR/Rx4MC2OyW2Clx\n",
       "is3vgWV0U3TH2TwpMQ24kn8PDcccPzlb50ZENGQzcHK5GOFx4CJg+UF72POf35ZWA7dgb0RaCFyF\n",
       "NAN4hu4cfl9fgfaWjIpPAeslpgOPAB8FLpEYyaw32fyg5xgiIqYmez/SZcAv6a6GW4W9A+mT5f5r\n",
       "DnHsNqS1dAltGLgf+F5fofY2TRcREfFCNbcCg6RzJD0s6Q86VIFWRZKulbRH0oO1YzkUSfMk3Snp\n",
       "IUm/lfTp2jENIuloSfdKGpK0XdLXasc0Hkkvk7RV0i21YxmPpEclPVDi7G1a5b8laY6kDZJ2lJ/7\n",
       "kv5jopcAAAT6SURBVNoxjSXp9eV1HPl6uuH30cryXn9Q0nXqsUC1D02NjNQVXv2Obm7yMWATsNz2\n",
       "jqqBjSHprcBeYK27YrAmSToeON72kKRZwBbgva29ngCSZtrep66e4R7gs7bvqR3XWJKuAE4DZtu+\n",
       "oHY8g0j6M3Cau0tzmyVpDXCX7WvLz/0Y20/Xjms8kqbRnZcW29450f6Hk7q/Cd0BLLD9rKQbgJ/Z\n",
       "XlM1sBehtZHRYuCPth/1oQu0qrJ9N/BU7TgmYnu37aGyvZduBYwT6kY1mO19ZXM63dx2cydSSScC\n",
       "76ZbRaT1C2yajk9SKYp3KYr3/pYTUbEMeKS1RFQ8X6Cqw1Cg2ofWktHEBVrxkpRPTouAe+tGMpik\n",
       "aZKGgD3Anba3145pgG/TLV81XDuQCRi4TdJmSR+vHcw4SlG8Vku6X9L3Jc2sHdQEPghcVzuIQTyg\n",
       "QNU9Fqj2obVk1M6c4f+RMkW3Abi8jJCaY3vY9qnAicDbWlsLTNJ5wF9sb6XxUQew1PYi4Fzg0jKt\n",
       "3JpRRfEeKYr/fN2QxidpOnA+cGPtWAbRgAJV9Vig2ofWktFjHLw80Dy60VG8RJKOBG4C1tn+ce14\n",
       "JlKman4KnF47ljHOAC4of4+5HjhL3WWvzbH9RPn3SbrFiBfXjWigXcAu25vK9xvoklOrzgW2lNe0\n",
       "RacDv7b9V9v7gZvpfmcnjdaS0WbgZEmvKZ9ELgJ+UjmmSUuSgFXAdtvfqR3PeCTNlTSnbM8A3gVs\n",
       "rRvVwWx/wfY826+lm665w/ZHasc1lqSZkmaX7WOAs4Hmrvq0vRvYKemUctMy4KGKIU1kOd2HkFY9\n",
       "DCyRNKO875cBLU51j6vvotcXxfZ+jSnQavTKr+uBtwPHStoJXGV7deWwBlkKXAw8IGnk5L7S9i8O\n",
       "cUwNrwbWlKuVpgE/tH175Zgm0uqU8quAH3XnI44A1tu+tW5I4ypF8RpdFN+cktSX0bVTaJLtbTqM\n",
       "Bap9aOrS7oiImJpam6aLiIgpKMkoIiKqSzKKiIjqkowiIqK6JKOIiKguySgiIqpLMor4H5H0jpZb\n",
       "S0S0LMkoIiKqSzKKKUfSxaWZ31ZJV5eGeXslfas0IbxN0tyy76mSfiNpm6SbRy1b9Lqy35CkLZLm\n",
       "063KMEvSjaVh3LpR/+fXS+OzbZK+WeeZR7QrySimFEkLgAuBM8rK1geAD9P1f9lk+03AXcAXyyFr\n",
       "gc/ZXki3xtvI7euB75aVxt8CPEG3mvci4HLgDcB8SUslHUvX1PCN5XG+ehieasSkkmQUU8076Tq1\n",
       "bi7r9Z1F11tnGLih7LMOOFPSy4FXlGaKAGvo2lvMAk6wvRHA9nO2/1n2uc/24+7W2RoCTgL+Bjwj\n",
       "aZWk9wEj+0ZEkWQUU9Ea24vK1wLbXx5zvxi8EOoL6WP07KjtA8CRtg/QtXHYAJwHtLZQbUR1SUYx\n",
       "1dwOvF/ScQCSXinpJLr3wgfKPh8C7rb9d+ApSWeW21cAvyoNCndJek95jKNK64uByqrPc2z/HLgC\n",
       "WNjHE4uYzJpqIRHRN9s7JF0J3FpaVjwHXEbXaXRxuW8PXS8tgEuAq0tL7NFtDlYA10j6SnmMC+lG\n",
       "U2NHVAZmAxslHU03uvpMX88vYrJKC4kIQNI/bM+uHUfEVJVpuohOPpVFVJSRUUREVJeRUUREVJdk\n",
       "FBER1SUZRUREdUlGERFRXZJRRERUl2QUERHV/QvIJcaLHbAFwwAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x101cf3bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss and the accuracies for both training and validation sets for each epoch\n",
    "visualize.plot_loss_acc(DATA_SET + '_train', train_losses, train_accuracies, val_accuracies, learning_rate, reg_strength, num_epochs, num_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
