{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImplements an RNN on a synthetic data set, following the architecture \\ndescribed in \"Deep Knowledge Tracing\" by Chris Piech et al.\\nThe RNN implementation is based on min-char-rnn.py by Andrej Karpathy (@karpathy).\\nBSD License\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implements an RNN on a synthetic data set, following the architecture \n",
    "described in \"Deep Knowledge Tracing\" by Chris Piech et al.\n",
    "The RNN implementation is based on min-char-rnn.py by Andrej Karpathy (@karpathy).\n",
    "BSD License\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "synthetic_data_set = \"syntheticDetailed/naive_c5_q50_s4000_v0.csv\"\n",
    "code_org_data_set = \"data/code-org-problem4.csv\"\n",
    "\n",
    "DATA_SET = code_org_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Vectorization done!\n",
      "X_train\n",
      "(2000, 19, 40)\n",
      "[[[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ...,  True False False]\n",
      "  [False False False ..., False  True False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ...,  True False False]\n",
      "  [False False False ..., False  True False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ...,  True False False]\n",
      "  [False False False ..., False  True False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ...,  True False False]\n",
      "  [False False False ..., False  True False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ...,  True False False]\n",
      "  [False False False ..., False  True False]]]\n",
      "y_train\n",
      "(2000, 19)\n",
      "[[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      " [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      " [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      " [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      " [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]]\n"
     ]
    }
   ],
   "source": [
    "# Read in the data set\n",
    "# This function can be moved to utils.py\n",
    "data_array = np.array(list(csv.reader(open(DATA_SET,\"rb\"),delimiter=','))).astype('int')\n",
    "num_samples = data_array.shape[0]\n",
    "num_problems = data_array.shape[1]\n",
    "\n",
    "# time steps is number of problems - 1 because we cannot predict on the last problem.\n",
    "num_timesteps = num_problems - 1 \n",
    "# Split data into train and test (half and half)\n",
    "train = data_array[0:num_samples/2,:]\n",
    "test = data_array[num_samples/2:num_samples,:]\n",
    "\n",
    "num_train = train.shape[0]\n",
    "num_test = test.shape[0]\n",
    "\n",
    "\n",
    "print('Vectorization...')\n",
    "X_train = np.zeros((num_train, num_timesteps, num_problems * 2), dtype=np.bool)\n",
    "y_train = np.zeros((num_train, num_timesteps), dtype=np.int)\n",
    "\n",
    "# Create 3-dimensional input tensor with one-hot encodings for each sample\n",
    "# the dimension of each vector for a student i and time t is 2 * num_problems\n",
    "# where the first half corresponds to the correctly answered problems and the\n",
    "# second half to the incorrectly answered ones.\n",
    "for i in xrange(num_train):\n",
    "    \n",
    "    # for the first time step. Done separately so we can populate the output \n",
    "    # tensor at the same time, which is shifted back by 1.\n",
    "\n",
    "    for t in xrange(0,num_timesteps):\n",
    "        p = t # since timestep t corresponds to problem p where t=p\n",
    "        if train[i,p] == 1:\n",
    "            X_train[i, t, p] = 1 \n",
    "        else:\n",
    "            X_train[i, t, num_problems + p] = 1\n",
    "        # this is a special case for the synthetic data set, where the next problem \n",
    "        # is just the current problem index + 1\n",
    "        y_train[i,t] = p + 1\n",
    "correctness = train\n",
    "\n",
    "print (\"Vectorization done!\")\n",
    "\n",
    "print (\"X_train\")\n",
    "print (X_train.shape)\n",
    "print (X_train[:5])\n",
    "\n",
    "print (\"y_train\")\n",
    "print (y_train.shape)\n",
    "print (y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 200 # size of hidden layer of neurons\n",
    "learning_rate = 1e-1\n",
    "epochs = 1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, num_problems * 2)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(num_problems, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((num_problems, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, correctness, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps, ps_denom = {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = inputs[t,:].reshape((num_problems * 2, 1))\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps_denom[t] = np.sum(np.exp(ys[t]))\n",
    "        ps[t] = np.exp(ys[t]) / ps_denom[t] # probabilities for next chars\n",
    "\n",
    "        # softmax (cross-entropy loss)\n",
    "        if correctness[targets[t]] == 1:\n",
    "            loss += -np.log(ps[t][targets[t],0]) \n",
    "        else:\n",
    "            loss += -np.log(1-ps[t][targets[t],0]) \n",
    "        # backward pass: compute gradients going backwards\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "        dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        if correctness[targets[t]] == 1:\n",
    "            dy[targets[t]] -= 1 # backprop into y\n",
    "        else:\n",
    "            for p in xrange(num_problems):\n",
    "                if p != targets[t]:\n",
    "                    dy[p] -= np.exp(ys[t][p]) / (ps_denom[t] - np.exp(ys[t][targets[t]]))\n",
    "\n",
    "\n",
    "\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(ps, targets, correctness):\n",
    "    \"\"\"\n",
    "    Computes the accuracy using the predictions at each time step.\n",
    "    For each t, if probability of next problem is > 0.5 for correct, or <= 0.5 \n",
    "    for incorrect, then count this as correct prediction.\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    for t in xrange(num_timesteps):\n",
    "        predicted_prob = ps[t][targets[t],0] \n",
    "        if (predicted_prob >= 0.5 and correctness[targets[t]] == 1) or (predicted_prob < 0.5 and correctness[targets[t]] == 0):\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / float(num_timesteps)\n",
    "    return accuracy\n",
    "\n",
    "def forward_pass(inputs):\n",
    "    xs, hs, ys, ps, ps_denom = {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = inputs[t,:].reshape((num_problems * 2, 1))\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps_denom[t] = np.sum(np.exp(ys[t]))\n",
    "        ps[t] = np.exp(ys[t]) / ps_denom[t] # probabilities for next chars\n",
    "    return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, iter 0, loss: 41.018464, acc: 0.947368\n",
      "epoch 0, iter 1, loss: 28.953734, acc: 0.947368\n",
      "epoch 0, iter 2, loss: 27.736009, acc: 0.894737\n",
      "epoch 0, iter 3, loss: 20.189130, acc: 0.894737\n",
      "epoch 0, iter 4, loss: 14.974193, acc: 0.947368\n",
      "epoch 0, iter 5, loss: 24.800078, acc: 0.894737\n",
      "epoch 0, iter 6, loss: 29.037327, acc: 0.789474\n",
      "epoch 0, iter 7, loss: 44.571268, acc: 0.894737\n",
      "epoch 0, iter 8, loss: 42.842446, acc: 0.894737\n",
      "epoch 0, iter 9, loss: 40.433066, acc: 0.842105\n",
      "epoch 0, iter 10, loss: 37.019689, acc: 0.894737\n",
      "epoch 0, iter 11, loss: 32.261606, acc: 0.894737\n",
      "epoch 0, iter 12, loss: 32.893370, acc: 0.947368\n",
      "epoch 0, iter 13, loss: 29.302648, acc: 0.894737\n",
      "epoch 0, iter 14, loss: 27.850372, acc: 0.947368\n",
      "epoch 0, iter 15, loss: 21.804223, acc: 1.000000\n",
      "epoch 0, iter 16, loss: 15.336892, acc: 1.000000\n",
      "epoch 0, iter 17, loss: 10.778842, acc: 0.842105\n",
      "epoch 0, iter 18, loss: 18.462329, acc: 0.842105\n",
      "epoch 0, iter 19, loss: 18.028410, acc: 0.947368\n",
      "epoch 0, iter 20, loss: 18.074587, acc: 0.842105\n",
      "epoch 0, iter 21, loss: 16.736481, acc: 0.947368\n",
      "epoch 0, iter 22, loss: 20.466133, acc: 1.000000\n",
      "epoch 0, iter 23, loss: 14.362039, acc: 1.000000\n",
      "epoch 0, iter 24, loss: 10.237540, acc: 0.894737\n",
      "epoch 0, iter 25, loss: 14.301792, acc: 0.947368\n",
      "epoch 0, iter 26, loss: 17.276813, acc: 0.947368\n",
      "epoch 0, iter 27, loss: 15.123029, acc: 1.000000\n",
      "epoch 0, iter 28, loss: 10.620004, acc: 1.000000\n",
      "epoch 0, iter 29, loss: 7.434971, acc: 0.947368\n",
      "epoch 0, iter 30, loss: 6.674665, acc: 1.000000\n",
      "epoch 0, iter 31, loss: 4.816606, acc: 1.000000\n",
      "epoch 0, iter 32, loss: 3.371694, acc: 1.000000\n",
      "epoch 0, iter 33, loss: 2.360726, acc: 1.000000\n",
      "epoch 0, iter 34, loss: 1.652666, acc: 1.000000\n",
      "epoch 0, iter 35, loss: 2.229193, acc: 1.000000\n",
      "epoch 0, iter 36, loss: 3.226881, acc: 0.789474\n",
      "epoch 0, iter 37, loss: 23.361623, acc: 1.000000\n",
      "epoch 0, iter 38, loss: 16.356043, acc: 0.947368\n",
      "epoch 0, iter 39, loss: 15.322970, acc: 0.894737\n",
      "epoch 0, iter 40, loss: 21.863410, acc: 0.894737\n",
      "epoch 0, iter 41, loss: 22.856089, acc: 0.947368\n",
      "epoch 0, iter 42, loss: 16.649281, acc: 0.894737\n",
      "epoch 0, iter 43, loss: 14.728861, acc: 0.947368\n",
      "epoch 0, iter 44, loss: 17.765021, acc: 0.947368\n",
      "epoch 0, iter 45, loss: 18.647253, acc: 1.000000\n",
      "epoch 0, iter 46, loss: 13.228201, acc: 0.894737\n",
      "epoch 0, iter 47, loss: 12.586976, acc: 1.000000\n",
      "epoch 0, iter 48, loss: 14.981509, acc: 0.894737\n",
      "epoch 0, iter 49, loss: 16.690583, acc: 0.947368\n",
      "epoch 0, iter 50, loss: 12.512590, acc: 0.947368\n",
      "epoch 0, iter 51, loss: 11.388184, acc: 0.894737\n",
      "epoch 0, iter 52, loss: 14.237552, acc: 0.894737\n",
      "epoch 0, iter 53, loss: 10.587112, acc: 0.894737\n",
      "epoch 0, iter 54, loss: 8.354221, acc: 0.894737\n",
      "epoch 0, iter 55, loss: 9.418465, acc: 0.947368\n",
      "epoch 0, iter 56, loss: 10.851501, acc: 0.947368\n",
      "epoch 0, iter 57, loss: 14.222110, acc: 0.894737\n",
      "epoch 0, iter 58, loss: 11.056637, acc: 0.947368\n",
      "epoch 0, iter 59, loss: 8.076911, acc: 0.894737\n",
      "epoch 0, iter 60, loss: 9.779306, acc: 0.947368\n",
      "epoch 0, iter 61, loss: 10.143354, acc: 0.842105\n",
      "epoch 0, iter 62, loss: 8.745087, acc: 0.894737\n",
      "epoch 0, iter 63, loss: 9.292487, acc: 0.947368\n",
      "epoch 0, iter 64, loss: 10.572778, acc: 0.947368\n",
      "epoch 0, iter 65, loss: 10.685132, acc: 1.000000\n",
      "epoch 0, iter 66, loss: 7.943720, acc: 0.947368\n",
      "epoch 0, iter 67, loss: 6.238125, acc: 0.947368\n",
      "epoch 0, iter 68, loss: 6.486066, acc: 0.842105\n",
      "epoch 0, iter 69, loss: 6.442960, acc: 0.947368\n",
      "epoch 0, iter 70, loss: 7.757982, acc: 0.894737\n",
      "epoch 0, iter 71, loss: 9.135543, acc: 0.894737\n",
      "epoch 0, iter 72, loss: 8.687148, acc: 0.842105\n",
      "epoch 0, iter 73, loss: 8.224898, acc: 0.947368\n",
      "epoch 0, iter 74, loss: 6.010047, acc: 0.894737\n",
      "epoch 0, iter 75, loss: 7.577340, acc: 1.000000\n",
      "epoch 0, iter 76, loss: 5.367918, acc: 0.894737\n",
      "epoch 0, iter 77, loss: 5.477216, acc: 0.947368\n",
      "epoch 0, iter 78, loss: 6.130587, acc: 0.842105\n",
      "epoch 0, iter 79, loss: 5.777981, acc: 1.000000\n",
      "epoch 0, iter 80, loss: 4.106161, acc: 0.947368\n",
      "epoch 0, iter 81, loss: 2.917337, acc: 0.947368\n",
      "epoch 0, iter 82, loss: 2.942816, acc: 0.842105\n",
      "epoch 0, iter 83, loss: 7.228376, acc: 0.894737\n",
      "epoch 0, iter 84, loss: 8.907048, acc: 0.842105\n",
      "epoch 0, iter 85, loss: 7.825695, acc: 1.000000\n",
      "epoch 0, iter 86, loss: 6.078286, acc: 0.894737\n",
      "epoch 0, iter 87, loss: 7.376842, acc: 0.947368\n",
      "epoch 0, iter 88, loss: 5.213944, acc: 0.947368\n",
      "epoch 0, iter 89, loss: 4.655498, acc: 0.894737\n",
      "epoch 0, iter 90, loss: 4.799068, acc: 0.947368\n",
      "epoch 0, iter 91, loss: 7.208811, acc: 0.894737\n",
      "epoch 0, iter 92, loss: 8.305380, acc: 0.947368\n",
      "epoch 0, iter 93, loss: 6.332161, acc: 0.947368\n",
      "epoch 0, iter 94, loss: 4.878441, acc: 0.947368\n",
      "epoch 0, iter 95, loss: 5.759633, acc: 0.947368\n",
      "epoch 0, iter 96, loss: 5.925100, acc: 0.947368\n",
      "epoch 0, iter 97, loss: 4.732445, acc: 0.947368\n",
      "epoch 0, iter 98, loss: 3.885519, acc: 0.947368\n",
      "epoch 0, iter 99, loss: 4.337058, acc: 0.947368\n",
      "epoch 0, iter 100, loss: 4.573939, acc: 0.947368\n",
      "epoch 0, iter 101, loss: 3.813429, acc: 0.947368\n",
      "epoch 0, iter 102, loss: 3.354904, acc: 0.947368\n",
      "epoch 0, iter 103, loss: 3.670548, acc: 0.947368\n",
      "epoch 0, iter 104, loss: 4.022271, acc: 0.947368\n",
      "epoch 0, iter 105, loss: 3.494477, acc: 0.947368\n",
      "epoch 0, iter 106, loss: 3.778367, acc: 0.947368\n",
      "epoch 0, iter 107, loss: 3.706370, acc: 0.947368\n",
      "epoch 0, iter 108, loss: 2.736450, acc: 0.947368\n",
      "epoch 0, iter 109, loss: 4.176995, acc: 1.000000\n",
      "epoch 0, iter 110, loss: 3.385642, acc: 0.947368\n",
      "epoch 0, iter 111, loss: 5.221310, acc: 0.947368\n",
      "epoch 0, iter 112, loss: 3.866041, acc: 1.000000\n",
      "epoch 0, iter 113, loss: 2.930735, acc: 0.947368\n",
      "epoch 0, iter 114, loss: 2.273158, acc: 1.000000\n",
      "epoch 0, iter 115, loss: 1.980058, acc: 0.947368\n",
      "epoch 0, iter 116, loss: 2.962739, acc: 0.947368\n",
      "epoch 0, iter 117, loss: 2.269206, acc: 1.000000\n",
      "epoch 0, iter 118, loss: 2.156798, acc: 1.000000\n",
      "epoch 0, iter 119, loss: 1.785541, acc: 0.947368\n",
      "epoch 0, iter 120, loss: 2.628252, acc: 0.947368\n",
      "epoch 0, iter 121, loss: 3.239718, acc: 0.947368\n",
      "epoch 0, iter 122, loss: 2.866622, acc: 0.947368\n",
      "epoch 0, iter 123, loss: 2.497734, acc: 0.947368\n",
      "epoch 0, iter 124, loss: 1.948361, acc: 1.000000\n",
      "epoch 0, iter 125, loss: 2.505937, acc: 1.000000\n",
      "epoch 0, iter 126, loss: 2.024345, acc: 0.947368\n",
      "epoch 0, iter 127, loss: 2.184965, acc: 0.947368\n",
      "epoch 0, iter 128, loss: 2.996309, acc: 0.947368\n",
      "epoch 0, iter 129, loss: 3.136916, acc: 0.947368\n",
      "epoch 0, iter 130, loss: 2.702079, acc: 0.947368\n",
      "epoch 0, iter 131, loss: 2.575794, acc: 0.947368\n",
      "epoch 0, iter 132, loss: 2.003146, acc: 1.000000\n",
      "epoch 0, iter 133, loss: 1.586745, acc: 0.947368\n",
      "epoch 0, iter 134, loss: 1.339331, acc: 1.000000\n",
      "epoch 0, iter 135, loss: 1.199057, acc: 0.947368\n",
      "epoch 0, iter 136, loss: 2.249950, acc: 0.947368\n",
      "epoch 0, iter 137, loss: 1.746148, acc: 1.000000\n",
      "epoch 0, iter 138, loss: 1.700417, acc: 1.000000\n",
      "epoch 0, iter 139, loss: 1.412526, acc: 0.947368\n",
      "epoch 0, iter 140, loss: 2.108003, acc: 0.947368\n",
      "epoch 0, iter 141, loss: 2.974681, acc: 0.947368\n",
      "epoch 0, iter 142, loss: 2.785223, acc: 0.947368\n",
      "epoch 0, iter 143, loss: 2.345817, acc: 1.000000\n",
      "epoch 0, iter 144, loss: 2.637751, acc: 1.000000\n",
      "epoch 0, iter 145, loss: 2.233457, acc: 0.947368\n",
      "epoch 0, iter 146, loss: 2.323837, acc: 0.947368\n",
      "epoch 0, iter 147, loss: 1.788987, acc: 1.000000\n",
      "epoch 0, iter 148, loss: 2.127028, acc: 1.000000\n",
      "epoch 0, iter 149, loss: 1.700294, acc: 0.947368\n",
      "epoch 0, iter 150, loss: 1.836164, acc: 0.947368\n",
      "epoch 0, iter 151, loss: 3.053232, acc: 0.947368\n",
      "epoch 0, iter 152, loss: 3.016313, acc: 0.947368\n",
      "epoch 0, iter 153, loss: 2.607811, acc: 0.947368\n",
      "epoch 0, iter 154, loss: 2.568739, acc: 0.947368\n",
      "epoch 0, iter 155, loss: 1.997069, acc: 1.000000\n",
      "epoch 0, iter 156, loss: 2.193114, acc: 1.000000\n",
      "epoch 0, iter 157, loss: 1.797461, acc: 0.947368\n",
      "epoch 0, iter 158, loss: 1.482766, acc: 1.000000\n",
      "epoch 0, iter 159, loss: 2.282967, acc: 1.000000\n",
      "epoch 0, iter 160, loss: 1.912730, acc: 0.947368\n",
      "epoch 0, iter 161, loss: 1.766101, acc: 0.947368\n",
      "epoch 0, iter 162, loss: 2.548229, acc: 0.947368\n",
      "epoch 0, iter 163, loss: 2.718237, acc: 0.947368\n",
      "epoch 0, iter 164, loss: 2.427693, acc: 0.947368\n",
      "epoch 0, iter 165, loss: 1.987651, acc: 1.000000\n",
      "epoch 0, iter 166, loss: 1.587167, acc: 0.947368\n",
      "epoch 0, iter 167, loss: 1.403735, acc: 1.000000\n",
      "epoch 0, iter 168, loss: 1.247188, acc: 0.947368\n",
      "epoch 0, iter 169, loss: 1.786306, acc: 0.947368\n",
      "epoch 0, iter 170, loss: 2.482390, acc: 0.947368\n",
      "epoch 0, iter 171, loss: 2.224606, acc: 0.947368\n",
      "epoch 0, iter 172, loss: 1.992322, acc: 0.947368\n",
      "epoch 0, iter 173, loss: 2.128410, acc: 0.947368\n",
      "epoch 0, iter 174, loss: 2.423516, acc: 0.947368\n",
      "epoch 0, iter 175, loss: 2.239188, acc: 0.947368\n",
      "epoch 0, iter 176, loss: 2.112886, acc: 0.947368\n",
      "epoch 0, iter 177, loss: 2.138447, acc: 0.947368\n",
      "epoch 0, iter 178, loss: 2.291867, acc: 0.947368\n",
      "epoch 0, iter 179, loss: 1.864517, acc: 1.000000\n",
      "epoch 0, iter 180, loss: 1.868905, acc: 1.000000\n",
      "epoch 0, iter 181, loss: 1.580184, acc: 0.947368\n",
      "epoch 0, iter 182, loss: 1.816415, acc: 0.947368\n",
      "epoch 0, iter 183, loss: 2.136073, acc: 0.947368\n",
      "epoch 0, iter 184, loss: 2.119202, acc: 0.947368\n",
      "epoch 0, iter 185, loss: 1.934225, acc: 0.947368\n",
      "epoch 0, iter 186, loss: 2.018846, acc: 0.947368\n",
      "epoch 0, iter 187, loss: 2.031058, acc: 0.947368\n",
      "epoch 0, iter 188, loss: 2.030312, acc: 0.947368\n",
      "epoch 0, iter 189, loss: 1.922234, acc: 0.947368\n",
      "epoch 0, iter 190, loss: 1.964873, acc: 0.947368\n",
      "epoch 0, iter 191, loss: 1.950068, acc: 0.947368\n",
      "epoch 0, iter 192, loss: 1.950901, acc: 0.947368\n",
      "epoch 0, iter 193, loss: 1.904188, acc: 0.947368\n",
      "epoch 0, iter 194, loss: 2.008332, acc: 0.947368\n",
      "epoch 0, iter 195, loss: 1.931311, acc: 0.947368\n",
      "epoch 0, iter 196, loss: 1.899157, acc: 0.947368\n",
      "epoch 0, iter 197, loss: 1.791273, acc: 0.947368\n",
      "epoch 0, iter 198, loss: 1.833271, acc: 0.947368\n",
      "epoch 0, iter 199, loss: 1.744584, acc: 0.947368\n",
      "epoch 0, iter 200, loss: 1.803818, acc: 0.947368\n",
      "epoch 0, iter 201, loss: 1.753160, acc: 0.947368\n",
      "epoch 0, iter 202, loss: 1.805357, acc: 0.947368\n",
      "epoch 0, iter 203, loss: 1.726932, acc: 0.947368\n",
      "epoch 0, iter 204, loss: 1.718413, acc: 0.947368\n",
      "epoch 0, iter 205, loss: 1.623506, acc: 0.947368\n",
      "epoch 0, iter 206, loss: 1.380821, acc: 1.000000\n",
      "epoch 0, iter 207, loss: 1.387296, acc: 1.000000\n",
      "epoch 0, iter 208, loss: 1.209551, acc: 0.947368\n",
      "epoch 0, iter 209, loss: 1.233753, acc: 0.947368\n",
      "epoch 0, iter 210, loss: 1.652652, acc: 0.947368\n",
      "epoch 0, iter 211, loss: 1.530149, acc: 0.947368\n",
      "epoch 0, iter 212, loss: 1.461336, acc: 0.947368\n",
      "epoch 0, iter 213, loss: 1.418657, acc: 0.947368\n",
      "epoch 0, iter 214, loss: 1.583083, acc: 0.947368\n",
      "epoch 0, iter 215, loss: 1.476004, acc: 0.947368\n",
      "epoch 0, iter 216, loss: 1.457897, acc: 0.947368\n",
      "epoch 0, iter 217, loss: 1.371431, acc: 0.947368\n",
      "epoch 0, iter 218, loss: 1.445777, acc: 1.000000\n",
      "epoch 0, iter 219, loss: 1.342853, acc: 0.947368\n",
      "epoch 0, iter 220, loss: 1.391226, acc: 0.947368\n",
      "epoch 0, iter 221, loss: 1.310147, acc: 0.947368\n",
      "epoch 0, iter 222, loss: 1.392215, acc: 1.000000\n",
      "epoch 0, iter 223, loss: 1.322911, acc: 1.000000\n",
      "epoch 0, iter 224, loss: 1.334035, acc: 1.000000\n",
      "epoch 0, iter 225, loss: 1.114074, acc: 0.947368\n",
      "epoch 0, iter 226, loss: 1.230686, acc: 0.947368\n",
      "epoch 0, iter 227, loss: 1.432891, acc: 0.947368\n",
      "epoch 0, iter 228, loss: 1.263836, acc: 0.947368\n",
      "epoch 0, iter 229, loss: 1.143063, acc: 1.000000\n",
      "epoch 0, iter 230, loss: 1.013304, acc: 0.947368\n",
      "epoch 0, iter 231, loss: 1.047409, acc: 0.947368\n",
      "epoch 0, iter 232, loss: 1.405720, acc: 1.000000\n",
      "epoch 0, iter 233, loss: 1.257688, acc: 1.000000\n",
      "epoch 0, iter 234, loss: 1.192591, acc: 1.000000\n",
      "epoch 0, iter 235, loss: 1.125073, acc: 0.947368\n",
      "epoch 0, iter 236, loss: 1.234518, acc: 1.000000\n",
      "epoch 0, iter 237, loss: 1.134843, acc: 0.947368\n",
      "epoch 0, iter 238, loss: 1.177202, acc: 1.000000\n",
      "epoch 0, iter 239, loss: 1.095504, acc: 0.947368\n",
      "epoch 0, iter 240, loss: 1.036176, acc: 0.947368\n",
      "epoch 0, iter 241, loss: 1.006060, acc: 1.000000\n",
      "epoch 0, iter 242, loss: 0.896320, acc: 1.000000\n",
      "epoch 0, iter 243, loss: 0.890566, acc: 0.947368\n",
      "epoch 0, iter 244, loss: 1.255021, acc: 1.000000\n",
      "epoch 0, iter 245, loss: 1.122926, acc: 1.000000\n",
      "epoch 0, iter 246, loss: 1.080435, acc: 1.000000\n",
      "epoch 0, iter 247, loss: 1.005333, acc: 0.947368\n",
      "epoch 0, iter 248, loss: 1.107926, acc: 1.000000\n",
      "epoch 0, iter 249, loss: 1.011612, acc: 0.947368\n",
      "epoch 0, iter 250, loss: 1.049971, acc: 1.000000\n",
      "epoch 0, iter 251, loss: 0.968447, acc: 0.947368\n",
      "epoch 0, iter 252, loss: 1.034427, acc: 1.000000\n",
      "epoch 0, iter 253, loss: 0.949223, acc: 0.947368\n",
      "epoch 0, iter 254, loss: 1.001933, acc: 1.000000\n",
      "epoch 0, iter 255, loss: 0.917601, acc: 0.947368\n",
      "epoch 0, iter 256, loss: 0.979232, acc: 1.000000\n",
      "epoch 0, iter 257, loss: 0.894333, acc: 0.947368\n",
      "epoch 0, iter 258, loss: 0.950920, acc: 1.000000\n",
      "epoch 0, iter 259, loss: 0.864783, acc: 0.947368\n",
      "epoch 0, iter 260, loss: 0.932756, acc: 1.000000\n",
      "epoch 0, iter 261, loss: 0.854319, acc: 0.947368\n",
      "epoch 0, iter 262, loss: 0.921156, acc: 1.000000\n",
      "epoch 0, iter 263, loss: 1.103938, acc: 0.947368\n",
      "epoch 0, iter 264, loss: 1.081717, acc: 1.000000\n",
      "epoch 0, iter 265, loss: 1.808079, acc: 0.947368\n",
      "epoch 0, iter 266, loss: 1.547799, acc: 1.000000\n",
      "epoch 0, iter 267, loss: 1.253995, acc: 0.947368\n",
      "epoch 0, iter 268, loss: 1.185516, acc: 1.000000\n",
      "epoch 0, iter 269, loss: 1.062142, acc: 0.947368\n",
      "epoch 0, iter 270, loss: 1.055882, acc: 1.000000\n",
      "epoch 0, iter 271, loss: 1.194082, acc: 0.947368\n",
      "epoch 0, iter 272, loss: 1.118144, acc: 1.000000\n",
      "epoch 0, iter 273, loss: 0.918939, acc: 1.000000\n",
      "epoch 0, iter 274, loss: 0.951354, acc: 0.947368\n",
      "epoch 0, iter 275, loss: 1.073137, acc: 0.947368\n",
      "epoch 0, iter 276, loss: 1.045112, acc: 1.000000\n",
      "epoch 0, iter 277, loss: 0.885732, acc: 1.000000\n",
      "epoch 0, iter 278, loss: 0.892307, acc: 1.000000\n",
      "epoch 0, iter 279, loss: 0.838845, acc: 1.000000\n",
      "epoch 0, iter 280, loss: 0.863126, acc: 1.000000\n",
      "epoch 0, iter 281, loss: 0.777139, acc: 1.000000\n",
      "epoch 0, iter 282, loss: 0.810735, acc: 1.000000\n",
      "epoch 0, iter 283, loss: 0.998951, acc: 0.947368\n",
      "epoch 0, iter 284, loss: 0.961725, acc: 1.000000\n",
      "epoch 0, iter 285, loss: 1.698405, acc: 0.947368\n",
      "epoch 0, iter 286, loss: 1.527929, acc: 0.947368\n",
      "epoch 0, iter 287, loss: 1.604014, acc: 0.947368\n",
      "epoch 0, iter 288, loss: 1.287993, acc: 1.000000\n",
      "epoch 0, iter 289, loss: 1.024315, acc: 0.947368\n",
      "epoch 0, iter 290, loss: 1.245668, acc: 0.947368\n",
      "epoch 0, iter 291, loss: 1.326902, acc: 1.000000\n",
      "epoch 0, iter 292, loss: 1.144010, acc: 1.000000\n",
      "epoch 0, iter 293, loss: 0.962894, acc: 0.947368\n",
      "epoch 0, iter 294, loss: 0.963650, acc: 0.947368\n",
      "epoch 0, iter 295, loss: 0.899948, acc: 1.000000\n",
      "epoch 0, iter 296, loss: 0.772659, acc: 1.000000\n",
      "epoch 0, iter 297, loss: 0.721274, acc: 0.947368\n",
      "epoch 0, iter 298, loss: 1.006023, acc: 1.000000\n",
      "epoch 0, iter 299, loss: 0.889522, acc: 1.000000\n",
      "epoch 0, iter 300, loss: 0.821491, acc: 1.000000\n",
      "epoch 0, iter 301, loss: 0.751601, acc: 1.000000\n",
      "epoch 0, iter 302, loss: 0.802032, acc: 1.000000\n",
      "epoch 0, iter 303, loss: 1.031945, acc: 0.947368\n",
      "epoch 0, iter 304, loss: 0.937106, acc: 1.000000\n",
      "epoch 0, iter 305, loss: 0.773095, acc: 1.000000\n",
      "epoch 0, iter 306, loss: 0.778213, acc: 0.947368\n",
      "epoch 0, iter 307, loss: 0.917163, acc: 1.000000\n",
      "epoch 0, iter 308, loss: 0.864332, acc: 1.000000\n",
      "epoch 0, iter 309, loss: 0.732594, acc: 1.000000\n",
      "epoch 0, iter 310, loss: 0.722778, acc: 1.000000\n",
      "epoch 0, iter 311, loss: 0.694420, acc: 1.000000\n",
      "epoch 0, iter 312, loss: 0.697166, acc: 1.000000\n",
      "epoch 0, iter 313, loss: 0.630353, acc: 1.000000\n",
      "epoch 0, iter 314, loss: 0.644030, acc: 1.000000\n",
      "epoch 0, iter 315, loss: 0.610623, acc: 1.000000\n",
      "epoch 0, iter 316, loss: 0.625678, acc: 1.000000\n",
      "epoch 0, iter 317, loss: 0.936238, acc: 0.947368\n",
      "epoch 0, iter 318, loss: 1.066668, acc: 0.947368\n",
      "epoch 0, iter 319, loss: 0.899503, acc: 1.000000\n",
      "epoch 0, iter 320, loss: 0.779630, acc: 0.947368\n",
      "epoch 0, iter 321, loss: 0.883767, acc: 0.947368\n",
      "epoch 0, iter 322, loss: 1.081230, acc: 1.000000\n",
      "epoch 0, iter 323, loss: 0.889668, acc: 1.000000\n",
      "epoch 0, iter 324, loss: 1.104054, acc: 0.947368\n",
      "epoch 0, iter 325, loss: 0.978042, acc: 1.000000\n",
      "epoch 0, iter 326, loss: 1.502808, acc: 0.947368\n",
      "epoch 0, iter 327, loss: 1.212829, acc: 1.000000\n",
      "epoch 0, iter 328, loss: 1.012536, acc: 1.000000\n",
      "epoch 0, iter 329, loss: 0.861961, acc: 0.947368\n",
      "epoch 0, iter 330, loss: 0.899559, acc: 1.000000\n",
      "epoch 0, iter 331, loss: 0.777731, acc: 1.000000\n",
      "epoch 0, iter 332, loss: 0.749588, acc: 1.000000\n",
      "epoch 0, iter 333, loss: 0.671716, acc: 1.000000\n",
      "epoch 0, iter 334, loss: 0.703279, acc: 1.000000\n",
      "epoch 0, iter 335, loss: 0.633492, acc: 1.000000\n",
      "epoch 0, iter 336, loss: 0.650632, acc: 1.000000\n",
      "epoch 0, iter 337, loss: 0.596790, acc: 1.000000\n",
      "epoch 0, iter 338, loss: 0.632693, acc: 1.000000\n",
      "epoch 0, iter 339, loss: 0.576709, acc: 1.000000\n",
      "epoch 0, iter 340, loss: 0.599734, acc: 1.000000\n",
      "epoch 0, iter 341, loss: 0.555785, acc: 1.000000\n",
      "epoch 0, iter 342, loss: 0.756951, acc: 0.947368\n",
      "epoch 0, iter 343, loss: 0.683879, acc: 1.000000\n",
      "epoch 0, iter 344, loss: 0.593780, acc: 1.000000\n",
      "epoch 0, iter 345, loss: 0.970468, acc: 1.000000\n",
      "epoch 0, iter 346, loss: 1.071473, acc: 1.000000\n",
      "epoch 0, iter 347, loss: 1.896497, acc: 0.947368\n",
      "epoch 0, iter 348, loss: 1.476074, acc: 1.000000\n",
      "epoch 0, iter 349, loss: 1.154987, acc: 1.000000\n",
      "epoch 0, iter 350, loss: 1.031659, acc: 1.000000\n",
      "epoch 0, iter 351, loss: 0.868149, acc: 1.000000\n",
      "epoch 0, iter 352, loss: 0.778353, acc: 1.000000\n",
      "epoch 0, iter 353, loss: 0.671929, acc: 1.000000\n",
      "epoch 0, iter 354, loss: 0.852433, acc: 0.947368\n",
      "epoch 0, iter 355, loss: 0.752852, acc: 1.000000\n",
      "epoch 0, iter 356, loss: 0.641900, acc: 1.000000\n",
      "epoch 0, iter 357, loss: 0.581553, acc: 0.947368\n",
      "epoch 0, iter 358, loss: 0.817003, acc: 1.000000\n",
      "epoch 0, iter 359, loss: 0.701025, acc: 1.000000\n",
      "epoch 0, iter 360, loss: 0.639617, acc: 1.000000\n",
      "epoch 0, iter 361, loss: 0.571881, acc: 1.000000\n",
      "epoch 0, iter 362, loss: 0.604591, acc: 1.000000\n",
      "epoch 0, iter 363, loss: 0.541035, acc: 1.000000\n",
      "epoch 0, iter 364, loss: 0.531875, acc: 1.000000\n",
      "epoch 0, iter 365, loss: 0.927819, acc: 0.947368\n",
      "epoch 0, iter 366, loss: 0.826369, acc: 1.000000\n",
      "epoch 0, iter 367, loss: 0.665211, acc: 1.000000\n",
      "epoch 0, iter 368, loss: 0.636629, acc: 0.947368\n",
      "epoch 0, iter 369, loss: 0.684326, acc: 0.947368\n",
      "epoch 0, iter 370, loss: 0.649255, acc: 1.000000\n",
      "epoch 0, iter 371, loss: 0.535020, acc: 1.000000\n",
      "epoch 0, iter 372, loss: 0.838848, acc: 1.000000\n",
      "epoch 0, iter 373, loss: 1.147422, acc: 1.000000\n",
      "epoch 0, iter 374, loss: 1.786677, acc: 0.947368\n",
      "epoch 0, iter 375, loss: 1.355502, acc: 1.000000\n",
      "epoch 0, iter 376, loss: 1.073145, acc: 1.000000\n",
      "epoch 0, iter 377, loss: 1.157405, acc: 0.947368\n",
      "epoch 0, iter 378, loss: 1.025666, acc: 1.000000\n",
      "epoch 0, iter 379, loss: 0.804198, acc: 1.000000\n",
      "epoch 0, iter 380, loss: 0.719205, acc: 0.947368\n",
      "epoch 0, iter 381, loss: 0.877431, acc: 1.000000\n",
      "epoch 0, iter 382, loss: 1.053966, acc: 0.947368\n",
      "epoch 0, iter 383, loss: 0.835312, acc: 1.000000\n",
      "epoch 0, iter 384, loss: 0.681867, acc: 1.000000\n",
      "epoch 0, iter 385, loss: 0.642991, acc: 0.947368\n",
      "epoch 0, iter 386, loss: 0.817929, acc: 1.000000\n",
      "epoch 0, iter 387, loss: 0.666208, acc: 1.000000\n",
      "epoch 0, iter 388, loss: 0.587358, acc: 1.000000\n",
      "epoch 0, iter 389, loss: 0.535594, acc: 1.000000\n",
      "epoch 0, iter 390, loss: 0.548993, acc: 1.000000\n",
      "epoch 0, iter 391, loss: 0.481435, acc: 1.000000\n",
      "epoch 0, iter 392, loss: 0.463765, acc: 1.000000\n",
      "epoch 0, iter 393, loss: 0.435390, acc: 1.000000\n",
      "epoch 0, iter 394, loss: 0.451320, acc: 1.000000\n",
      "epoch 0, iter 395, loss: 0.413590, acc: 1.000000\n",
      "epoch 0, iter 396, loss: 0.417895, acc: 1.000000\n",
      "epoch 0, iter 397, loss: 0.396730, acc: 1.000000\n",
      "epoch 0, iter 398, loss: 0.413846, acc: 1.000000\n",
      "epoch 0, iter 399, loss: 0.386008, acc: 1.000000\n",
      "epoch 0, iter 400, loss: 0.396776, acc: 1.000000\n",
      "epoch 0, iter 401, loss: 0.377531, acc: 1.000000\n",
      "epoch 0, iter 402, loss: 0.742405, acc: 0.947368\n",
      "epoch 0, iter 403, loss: 1.111901, acc: 0.947368\n",
      "epoch 0, iter 404, loss: 0.897854, acc: 1.000000\n",
      "epoch 0, iter 405, loss: 0.719598, acc: 0.947368\n",
      "epoch 0, iter 406, loss: 0.860804, acc: 1.000000\n",
      "epoch 0, iter 407, loss: 0.852027, acc: 1.000000\n",
      "epoch 0, iter 408, loss: 0.713009, acc: 1.000000\n",
      "epoch 0, iter 409, loss: 0.575912, acc: 1.000000\n",
      "epoch 0, iter 410, loss: 0.557263, acc: 1.000000\n",
      "epoch 0, iter 411, loss: 0.872006, acc: 0.947368\n",
      "epoch 0, iter 412, loss: 0.747463, acc: 1.000000\n",
      "epoch 0, iter 413, loss: 0.597533, acc: 1.000000\n",
      "epoch 0, iter 414, loss: 0.896516, acc: 1.000000\n",
      "epoch 0, iter 415, loss: 0.964790, acc: 1.000000\n",
      "epoch 0, iter 416, loss: 0.764390, acc: 1.000000\n",
      "epoch 0, iter 417, loss: 0.620257, acc: 0.947368\n",
      "epoch 0, iter 418, loss: 0.746827, acc: 1.000000\n",
      "epoch 0, iter 419, loss: 0.653607, acc: 1.000000\n",
      "epoch 0, iter 420, loss: 0.554988, acc: 1.000000\n",
      "epoch 0, iter 421, loss: 0.469778, acc: 1.000000\n",
      "epoch 0, iter 422, loss: 0.476150, acc: 1.000000\n",
      "epoch 0, iter 423, loss: 0.435510, acc: 1.000000\n",
      "epoch 0, iter 424, loss: 0.411851, acc: 1.000000\n",
      "epoch 0, iter 425, loss: 0.371788, acc: 1.000000\n",
      "epoch 0, iter 426, loss: 0.383964, acc: 1.000000\n",
      "epoch 0, iter 427, loss: 0.362264, acc: 1.000000\n",
      "epoch 0, iter 428, loss: 0.362592, acc: 1.000000\n",
      "epoch 0, iter 429, loss: 0.337608, acc: 1.000000\n",
      "epoch 0, iter 430, loss: 0.351637, acc: 1.000000\n",
      "epoch 0, iter 431, loss: 0.334805, acc: 1.000000\n",
      "epoch 0, iter 432, loss: 0.767712, acc: 0.947368\n",
      "epoch 0, iter 433, loss: 0.652724, acc: 1.000000\n",
      "epoch 0, iter 434, loss: 0.542002, acc: 1.000000\n",
      "epoch 0, iter 435, loss: 0.482118, acc: 0.947368\n",
      "epoch 0, iter 436, loss: 0.608496, acc: 1.000000\n",
      "epoch 0, iter 437, loss: 0.511733, acc: 1.000000\n",
      "epoch 0, iter 438, loss: 0.447406, acc: 1.000000\n",
      "epoch 0, iter 439, loss: 0.400418, acc: 1.000000\n",
      "epoch 0, iter 440, loss: 0.410780, acc: 1.000000\n",
      "epoch 0, iter 441, loss: 0.367660, acc: 1.000000\n",
      "epoch 0, iter 442, loss: 0.354138, acc: 1.000000\n",
      "epoch 0, iter 443, loss: 0.330129, acc: 1.000000\n",
      "epoch 0, iter 444, loss: 0.341692, acc: 1.000000\n",
      "epoch 0, iter 445, loss: 0.317899, acc: 1.000000\n",
      "epoch 0, iter 446, loss: 0.321944, acc: 1.000000\n",
      "epoch 0, iter 447, loss: 0.304763, acc: 1.000000\n",
      "epoch 0, iter 448, loss: 0.316715, acc: 1.000000\n",
      "epoch 0, iter 449, loss: 0.298886, acc: 1.000000\n",
      "epoch 0, iter 450, loss: 0.308004, acc: 1.000000\n",
      "epoch 0, iter 451, loss: 0.292754, acc: 1.000000\n",
      "epoch 0, iter 452, loss: 0.304228, acc: 1.000000\n",
      "epoch 0, iter 453, loss: 0.288514, acc: 1.000000\n",
      "epoch 0, iter 454, loss: 0.298969, acc: 1.000000\n",
      "epoch 0, iter 455, loss: 0.284444, acc: 1.000000\n",
      "epoch 0, iter 456, loss: 0.295359, acc: 1.000000\n",
      "epoch 0, iter 457, loss: 0.280652, acc: 1.000000\n",
      "epoch 0, iter 458, loss: 0.291335, acc: 1.000000\n",
      "epoch 0, iter 459, loss: 0.277268, acc: 1.000000\n",
      "epoch 0, iter 460, loss: 0.287709, acc: 1.000000\n",
      "epoch 0, iter 461, loss: 0.273658, acc: 1.000000\n",
      "epoch 0, iter 462, loss: 0.762441, acc: 0.947368\n",
      "epoch 0, iter 463, loss: 0.628603, acc: 1.000000\n",
      "epoch 0, iter 464, loss: 0.515262, acc: 1.000000\n",
      "epoch 0, iter 465, loss: 0.445508, acc: 1.000000\n",
      "epoch 0, iter 466, loss: 0.557797, acc: 1.000000\n",
      "epoch 0, iter 467, loss: 0.463177, acc: 1.000000\n",
      "epoch 0, iter 468, loss: 0.396742, acc: 1.000000\n",
      "epoch 0, iter 469, loss: 0.350352, acc: 1.000000\n",
      "epoch 0, iter 470, loss: 0.362746, acc: 1.000000\n",
      "epoch 0, iter 471, loss: 0.941868, acc: 0.947368\n",
      "epoch 0, iter 472, loss: 0.755898, acc: 1.000000\n",
      "epoch 0, iter 473, loss: 0.588067, acc: 1.000000\n",
      "epoch 0, iter 474, loss: 0.951869, acc: 0.947368\n",
      "epoch 0, iter 475, loss: 0.888514, acc: 1.000000\n",
      "epoch 0, iter 476, loss: 0.697527, acc: 1.000000\n",
      "epoch 0, iter 477, loss: 0.556310, acc: 0.947368\n",
      "epoch 0, iter 478, loss: 0.642249, acc: 1.000000\n",
      "epoch 0, iter 479, loss: 0.549876, acc: 1.000000\n",
      "epoch 0, iter 480, loss: 0.453989, acc: 1.000000\n",
      "epoch 0, iter 481, loss: 0.377698, acc: 1.000000\n",
      "epoch 0, iter 482, loss: 0.382928, acc: 1.000000\n",
      "epoch 0, iter 483, loss: 0.347266, acc: 1.000000\n",
      "epoch 0, iter 484, loss: 0.319697, acc: 1.000000\n",
      "epoch 0, iter 485, loss: 0.284925, acc: 1.000000\n",
      "epoch 0, iter 486, loss: 0.295457, acc: 1.000000\n",
      "epoch 0, iter 487, loss: 0.278719, acc: 1.000000\n",
      "epoch 0, iter 488, loss: 0.275033, acc: 1.000000\n",
      "epoch 0, iter 489, loss: 0.254328, acc: 1.000000\n",
      "epoch 0, iter 490, loss: 0.266275, acc: 1.000000\n",
      "epoch 0, iter 491, loss: 0.254400, acc: 1.000000\n",
      "epoch 0, iter 492, loss: 0.258339, acc: 1.000000\n",
      "epoch 0, iter 493, loss: 0.242490, acc: 1.000000\n",
      "epoch 0, iter 494, loss: 0.761360, acc: 0.947368\n",
      "epoch 0, iter 495, loss: 1.207374, acc: 0.947368\n",
      "epoch 0, iter 496, loss: 2.066438, acc: 0.947368\n",
      "epoch 0, iter 497, loss: 1.503577, acc: 1.000000\n",
      "epoch 0, iter 498, loss: 1.145032, acc: 1.000000\n",
      "epoch 0, iter 499, loss: 0.986102, acc: 1.000000\n",
      "epoch 0, iter 500, loss: 0.790630, acc: 1.000000\n",
      "epoch 0, iter 501, loss: 0.605232, acc: 1.000000\n",
      "epoch 0, iter 502, loss: 1.036658, acc: 0.947368\n",
      "epoch 0, iter 503, loss: 0.842275, acc: 1.000000\n",
      "epoch 0, iter 504, loss: 0.654194, acc: 1.000000\n",
      "epoch 0, iter 505, loss: 0.522007, acc: 0.947368\n",
      "epoch 0, iter 506, loss: 0.602501, acc: 0.947368\n",
      "epoch 0, iter 507, loss: 1.006598, acc: 0.947368\n",
      "epoch 0, iter 508, loss: 0.776192, acc: 1.000000\n",
      "epoch 0, iter 509, loss: 0.595657, acc: 0.947368\n",
      "epoch 0, iter 510, loss: 1.296879, acc: 1.000000\n",
      "epoch 0, iter 511, loss: 1.126278, acc: 1.000000\n",
      "epoch 0, iter 512, loss: 0.882332, acc: 1.000000\n",
      "epoch 0, iter 513, loss: 0.673907, acc: 1.000000\n",
      "epoch 0, iter 514, loss: 0.597476, acc: 1.000000\n",
      "epoch 0, iter 515, loss: 0.520511, acc: 1.000000\n",
      "epoch 0, iter 516, loss: 0.450128, acc: 1.000000\n",
      "epoch 0, iter 517, loss: 0.370039, acc: 1.000000\n",
      "epoch 0, iter 518, loss: 0.355464, acc: 1.000000\n",
      "epoch 0, iter 519, loss: 0.327335, acc: 1.000000\n",
      "epoch 0, iter 520, loss: 0.310309, acc: 1.000000\n",
      "epoch 0, iter 521, loss: 0.273355, acc: 1.000000\n",
      "epoch 0, iter 522, loss: 0.277309, acc: 1.000000\n",
      "epoch 0, iter 523, loss: 0.263736, acc: 1.000000\n",
      "epoch 0, iter 524, loss: 0.788068, acc: 0.947368\n",
      "epoch 0, iter 525, loss: 0.629197, acc: 1.000000\n",
      "epoch 0, iter 526, loss: 0.502227, acc: 1.000000\n",
      "epoch 0, iter 527, loss: 0.426267, acc: 0.947368\n",
      "epoch 0, iter 528, loss: 0.539724, acc: 1.000000\n",
      "epoch 0, iter 529, loss: 0.437334, acc: 1.000000\n",
      "epoch 0, iter 530, loss: 0.368573, acc: 1.000000\n",
      "epoch 0, iter 531, loss: 0.322793, acc: 1.000000\n",
      "epoch 0, iter 532, loss: 0.333535, acc: 1.000000\n",
      "epoch 0, iter 533, loss: 0.290779, acc: 1.000000\n",
      "epoch 0, iter 534, loss: 0.272670, acc: 1.000000\n",
      "epoch 0, iter 535, loss: 0.252091, acc: 1.000000\n",
      "epoch 0, iter 536, loss: 0.727376, acc: 0.947368\n",
      "epoch 0, iter 537, loss: 0.584498, acc: 1.000000\n",
      "epoch 0, iter 538, loss: 1.587585, acc: 0.947368\n",
      "epoch 0, iter 539, loss: 1.174763, acc: 1.000000\n",
      "epoch 0, iter 540, loss: 1.422547, acc: 0.947368\n",
      "epoch 0, iter 541, loss: 1.054604, acc: 1.000000\n",
      "epoch 0, iter 542, loss: 0.782059, acc: 1.000000\n",
      "epoch 0, iter 543, loss: 0.603071, acc: 0.947368\n",
      "epoch 0, iter 544, loss: 0.851012, acc: 1.000000\n",
      "epoch 0, iter 545, loss: 0.651791, acc: 1.000000\n",
      "epoch 0, iter 546, loss: 0.527280, acc: 1.000000\n",
      "epoch 0, iter 547, loss: 0.429679, acc: 1.000000\n",
      "epoch 0, iter 548, loss: 0.396311, acc: 1.000000\n",
      "epoch 0, iter 549, loss: 0.332957, acc: 1.000000\n",
      "epoch 0, iter 550, loss: 0.307972, acc: 1.000000\n",
      "epoch 0, iter 551, loss: 0.273929, acc: 1.000000\n",
      "epoch 0, iter 552, loss: 0.275389, acc: 1.000000\n",
      "epoch 0, iter 553, loss: 0.247683, acc: 1.000000\n",
      "epoch 0, iter 554, loss: 0.249086, acc: 1.000000\n",
      "epoch 0, iter 555, loss: 0.231198, acc: 1.000000\n",
      "epoch 0, iter 556, loss: 0.241009, acc: 1.000000\n",
      "epoch 0, iter 557, loss: 0.222865, acc: 1.000000\n",
      "epoch 0, iter 558, loss: 0.231116, acc: 1.000000\n",
      "epoch 0, iter 559, loss: 0.217339, acc: 1.000000\n",
      "epoch 0, iter 560, loss: 0.228685, acc: 1.000000\n",
      "epoch 0, iter 561, loss: 0.213429, acc: 1.000000\n",
      "epoch 0, iter 562, loss: 0.223384, acc: 1.000000\n",
      "epoch 0, iter 563, loss: 0.210764, acc: 1.000000\n",
      "epoch 0, iter 564, loss: 0.222115, acc: 1.000000\n",
      "epoch 0, iter 565, loss: 0.208004, acc: 1.000000\n",
      "epoch 0, iter 566, loss: 0.218278, acc: 1.000000\n",
      "epoch 0, iter 567, loss: 0.206104, acc: 1.000000\n",
      "epoch 0, iter 568, loss: 0.217173, acc: 1.000000\n",
      "epoch 0, iter 569, loss: 0.203718, acc: 1.000000\n",
      "epoch 0, iter 570, loss: 0.213923, acc: 1.000000\n",
      "epoch 0, iter 571, loss: 0.915235, acc: 0.947368\n",
      "epoch 0, iter 572, loss: 0.726413, acc: 1.000000\n",
      "epoch 0, iter 573, loss: 0.555480, acc: 1.000000\n",
      "epoch 0, iter 574, loss: 0.473022, acc: 1.000000\n",
      "epoch 0, iter 575, loss: 0.479389, acc: 1.000000\n",
      "epoch 0, iter 576, loss: 0.409825, acc: 1.000000\n",
      "epoch 0, iter 577, loss: 0.327664, acc: 1.000000\n",
      "epoch 0, iter 578, loss: 0.300365, acc: 1.000000\n",
      "epoch 0, iter 579, loss: 0.293271, acc: 1.000000\n",
      "epoch 0, iter 580, loss: 0.276495, acc: 1.000000\n",
      "epoch 0, iter 581, loss: 0.235411, acc: 1.000000\n",
      "epoch 0, iter 582, loss: 0.232422, acc: 1.000000\n",
      "epoch 0, iter 583, loss: 0.228484, acc: 1.000000\n",
      "epoch 0, iter 584, loss: 0.229154, acc: 1.000000\n",
      "epoch 0, iter 585, loss: 1.035369, acc: 0.947368\n",
      "epoch 0, iter 586, loss: 0.807214, acc: 1.000000\n",
      "epoch 0, iter 587, loss: 0.611995, acc: 1.000000\n",
      "epoch 0, iter 588, loss: 0.510394, acc: 1.000000\n",
      "epoch 0, iter 589, loss: 0.476151, acc: 1.000000\n",
      "epoch 0, iter 590, loss: 0.402032, acc: 1.000000\n",
      "epoch 0, iter 591, loss: 0.322339, acc: 1.000000\n",
      "epoch 0, iter 592, loss: 0.294148, acc: 1.000000\n",
      "epoch 0, iter 593, loss: 0.276904, acc: 1.000000\n",
      "epoch 0, iter 594, loss: 0.260042, acc: 1.000000\n",
      "epoch 0, iter 595, loss: 0.224298, acc: 1.000000\n",
      "epoch 0, iter 596, loss: 0.222166, acc: 1.000000\n",
      "epoch 0, iter 597, loss: 0.214436, acc: 1.000000\n",
      "epoch 0, iter 598, loss: 0.815126, acc: 0.947368\n",
      "epoch 0, iter 599, loss: 0.633156, acc: 1.000000\n",
      "epoch 0, iter 600, loss: 1.644325, acc: 0.947368\n",
      "epoch 0, iter 601, loss: 1.208689, acc: 1.000000\n",
      "epoch 0, iter 602, loss: 1.574294, acc: 0.947368\n",
      "epoch 0, iter 603, loss: 1.910211, acc: 0.947368\n",
      "epoch 0, iter 604, loss: 1.412871, acc: 1.000000\n",
      "epoch 0, iter 605, loss: 1.052201, acc: 1.000000\n",
      "epoch 0, iter 606, loss: 0.926667, acc: 1.000000\n",
      "epoch 0, iter 607, loss: 0.789643, acc: 1.000000\n",
      "epoch 0, iter 608, loss: 0.610208, acc: 1.000000\n",
      "epoch 0, iter 609, loss: 0.468879, acc: 1.000000\n",
      "epoch 0, iter 610, loss: 0.852193, acc: 0.947368\n",
      "epoch 0, iter 611, loss: 0.693585, acc: 1.000000\n",
      "epoch 0, iter 612, loss: 1.748535, acc: 0.947368\n",
      "epoch 0, iter 613, loss: 1.269547, acc: 1.000000\n",
      "epoch 0, iter 614, loss: 0.961281, acc: 1.000000\n",
      "epoch 0, iter 615, loss: 0.733984, acc: 1.000000\n",
      "epoch 0, iter 616, loss: 0.590068, acc: 1.000000\n",
      "epoch 0, iter 617, loss: 0.459519, acc: 1.000000\n",
      "epoch 0, iter 618, loss: 0.393841, acc: 1.000000\n",
      "epoch 0, iter 619, loss: 0.330769, acc: 1.000000\n",
      "epoch 0, iter 620, loss: 0.303543, acc: 1.000000\n",
      "epoch 0, iter 621, loss: 0.259478, acc: 1.000000\n",
      "epoch 0, iter 622, loss: 0.252502, acc: 1.000000\n",
      "epoch 0, iter 623, loss: 0.228595, acc: 1.000000\n",
      "epoch 0, iter 624, loss: 0.229517, acc: 1.000000\n",
      "epoch 0, iter 625, loss: 0.207732, acc: 1.000000\n",
      "epoch 0, iter 626, loss: 0.214680, acc: 1.000000\n",
      "epoch 0, iter 627, loss: 0.906106, acc: 0.947368\n",
      "epoch 0, iter 628, loss: 0.714347, acc: 1.000000\n",
      "epoch 0, iter 629, loss: 0.544717, acc: 1.000000\n",
      "epoch 0, iter 630, loss: 0.460984, acc: 1.000000\n",
      "epoch 0, iter 631, loss: 0.470724, acc: 1.000000\n",
      "epoch 0, iter 632, loss: 0.400016, acc: 1.000000\n",
      "epoch 0, iter 633, loss: 0.318383, acc: 1.000000\n",
      "epoch 0, iter 634, loss: 0.291568, acc: 1.000000\n",
      "epoch 0, iter 635, loss: 0.746430, acc: 0.947368\n",
      "epoch 0, iter 636, loss: 0.605179, acc: 1.000000\n",
      "epoch 0, iter 637, loss: 0.471452, acc: 1.000000\n",
      "epoch 0, iter 638, loss: 0.409331, acc: 1.000000\n",
      "epoch 0, iter 639, loss: 0.518966, acc: 1.000000\n",
      "epoch 0, iter 640, loss: 1.020699, acc: 0.947368\n",
      "epoch 0, iter 641, loss: 0.768565, acc: 1.000000\n",
      "epoch 0, iter 642, loss: 0.600981, acc: 1.000000\n",
      "epoch 0, iter 643, loss: 0.530010, acc: 1.000000\n",
      "epoch 0, iter 644, loss: 0.556348, acc: 1.000000\n",
      "epoch 0, iter 645, loss: 1.402253, acc: 0.947368\n",
      "epoch 0, iter 646, loss: 1.045442, acc: 1.000000\n",
      "epoch 0, iter 647, loss: 0.779833, acc: 1.000000\n",
      "epoch 0, iter 648, loss: 0.656164, acc: 1.000000\n",
      "epoch 0, iter 649, loss: 0.554141, acc: 1.000000\n",
      "epoch 0, iter 650, loss: 1.156654, acc: 0.947368\n",
      "epoch 0, iter 651, loss: 0.862641, acc: 1.000000\n",
      "epoch 0, iter 652, loss: 0.662213, acc: 1.000000\n",
      "epoch 0, iter 653, loss: 0.533606, acc: 1.000000\n",
      "epoch 0, iter 654, loss: 0.521643, acc: 1.000000\n",
      "epoch 0, iter 655, loss: 0.410650, acc: 1.000000\n",
      "epoch 0, iter 656, loss: 0.338894, acc: 1.000000\n",
      "epoch 0, iter 657, loss: 0.292159, acc: 1.000000\n",
      "epoch 0, iter 658, loss: 0.291847, acc: 1.000000\n",
      "epoch 0, iter 659, loss: 0.249166, acc: 1.000000\n",
      "epoch 0, iter 660, loss: 0.229422, acc: 1.000000\n",
      "epoch 0, iter 661, loss: 0.210535, acc: 1.000000\n",
      "epoch 0, iter 662, loss: 0.219663, acc: 1.000000\n",
      "epoch 0, iter 663, loss: 0.198718, acc: 1.000000\n",
      "epoch 0, iter 664, loss: 0.196718, acc: 1.000000\n",
      "epoch 0, iter 665, loss: 0.185174, acc: 1.000000\n",
      "epoch 0, iter 666, loss: 0.761228, acc: 0.947368\n",
      "epoch 0, iter 667, loss: 0.593210, acc: 1.000000\n",
      "epoch 0, iter 668, loss: 0.470932, acc: 1.000000\n",
      "epoch 0, iter 669, loss: 0.386624, acc: 1.000000\n",
      "epoch 0, iter 670, loss: 0.454905, acc: 1.000000\n",
      "epoch 0, iter 671, loss: 1.117219, acc: 0.947368\n",
      "epoch 0, iter 672, loss: 0.839434, acc: 1.000000\n",
      "epoch 0, iter 673, loss: 0.630815, acc: 1.000000\n",
      "epoch 0, iter 674, loss: 0.549102, acc: 1.000000\n",
      "epoch 0, iter 675, loss: 0.521150, acc: 1.000000\n",
      "epoch 0, iter 676, loss: 0.418506, acc: 1.000000\n",
      "epoch 0, iter 677, loss: 0.328370, acc: 1.000000\n",
      "epoch 0, iter 678, loss: 0.808317, acc: 0.947368\n",
      "epoch 0, iter 679, loss: 0.661891, acc: 1.000000\n",
      "epoch 0, iter 680, loss: 0.519802, acc: 1.000000\n",
      "epoch 0, iter 681, loss: 0.411191, acc: 1.000000\n",
      "epoch 0, iter 682, loss: 0.563194, acc: 0.947368\n",
      "epoch 0, iter 683, loss: 0.478329, acc: 1.000000\n",
      "epoch 0, iter 684, loss: 1.829997, acc: 0.947368\n",
      "epoch 0, iter 685, loss: 1.322270, acc: 1.000000\n",
      "epoch 0, iter 686, loss: 1.051099, acc: 1.000000\n",
      "epoch 0, iter 687, loss: 0.789728, acc: 1.000000\n",
      "epoch 0, iter 688, loss: 0.606698, acc: 1.000000\n",
      "epoch 0, iter 689, loss: 0.466228, acc: 1.000000\n",
      "epoch 0, iter 690, loss: 0.410493, acc: 1.000000\n",
      "epoch 0, iter 691, loss: 0.337300, acc: 1.000000\n",
      "epoch 0, iter 692, loss: 0.293518, acc: 1.000000\n",
      "epoch 0, iter 693, loss: 0.247474, acc: 1.000000\n",
      "epoch 0, iter 694, loss: 0.246913, acc: 1.000000\n",
      "epoch 0, iter 695, loss: 0.220419, acc: 1.000000\n",
      "epoch 0, iter 696, loss: 0.212846, acc: 1.000000\n",
      "epoch 0, iter 697, loss: 0.191113, acc: 1.000000\n",
      "epoch 0, iter 698, loss: 0.202398, acc: 1.000000\n",
      "epoch 0, iter 699, loss: 0.187601, acc: 1.000000\n",
      "epoch 0, iter 700, loss: 0.805157, acc: 0.947368\n",
      "epoch 0, iter 701, loss: 0.624706, acc: 1.000000\n",
      "epoch 0, iter 702, loss: 1.580566, acc: 0.947368\n",
      "epoch 0, iter 703, loss: 1.153664, acc: 1.000000\n",
      "epoch 0, iter 704, loss: 0.860926, acc: 1.000000\n",
      "epoch 0, iter 705, loss: 0.644464, acc: 1.000000\n",
      "epoch 0, iter 706, loss: 0.545238, acc: 1.000000\n",
      "epoch 0, iter 707, loss: 0.427347, acc: 1.000000\n",
      "epoch 0, iter 708, loss: 0.357064, acc: 1.000000\n",
      "epoch 0, iter 709, loss: 0.291811, acc: 1.000000\n",
      "epoch 0, iter 710, loss: 0.281113, acc: 1.000000\n",
      "epoch 0, iter 711, loss: 0.241310, acc: 1.000000\n",
      "epoch 0, iter 712, loss: 0.228050, acc: 1.000000\n",
      "epoch 0, iter 713, loss: 0.201255, acc: 1.000000\n",
      "epoch 0, iter 714, loss: 0.211146, acc: 1.000000\n",
      "epoch 0, iter 715, loss: 0.191353, acc: 1.000000\n",
      "epoch 0, iter 716, loss: 0.192965, acc: 1.000000\n",
      "epoch 0, iter 717, loss: 0.176317, acc: 1.000000\n",
      "epoch 0, iter 718, loss: 0.189944, acc: 1.000000\n",
      "epoch 0, iter 719, loss: 0.175640, acc: 1.000000\n",
      "epoch 0, iter 720, loss: 0.181338, acc: 1.000000\n",
      "epoch 0, iter 721, loss: 0.167735, acc: 1.000000\n",
      "epoch 0, iter 722, loss: 0.181351, acc: 1.000000\n",
      "epoch 0, iter 723, loss: 0.168831, acc: 1.000000\n",
      "epoch 0, iter 724, loss: 0.175757, acc: 1.000000\n",
      "epoch 0, iter 725, loss: 0.163356, acc: 1.000000\n",
      "epoch 0, iter 726, loss: 0.176278, acc: 1.000000\n",
      "epoch 0, iter 727, loss: 0.164547, acc: 1.000000\n",
      "epoch 0, iter 728, loss: 0.171885, acc: 1.000000\n",
      "epoch 0, iter 729, loss: 0.160159, acc: 1.000000\n",
      "epoch 0, iter 730, loss: 0.172364, acc: 1.000000\n",
      "epoch 0, iter 731, loss: 0.161122, acc: 1.000000\n",
      "epoch 0, iter 732, loss: 0.168604, acc: 1.000000\n",
      "epoch 0, iter 733, loss: 0.157370, acc: 1.000000\n",
      "epoch 0, iter 734, loss: 0.168949, acc: 1.000000\n",
      "epoch 0, iter 735, loss: 0.955269, acc: 0.947368\n",
      "epoch 0, iter 736, loss: 0.734337, acc: 1.000000\n",
      "epoch 0, iter 737, loss: 0.550851, acc: 1.000000\n",
      "epoch 0, iter 738, loss: 1.048501, acc: 0.947368\n",
      "epoch 0, iter 739, loss: 0.869428, acc: 1.000000\n",
      "epoch 0, iter 740, loss: 0.661506, acc: 1.000000\n",
      "epoch 0, iter 741, loss: 0.503153, acc: 1.000000\n",
      "epoch 0, iter 742, loss: 0.530452, acc: 1.000000\n",
      "epoch 0, iter 743, loss: 0.445019, acc: 1.000000\n",
      "epoch 0, iter 744, loss: 0.354636, acc: 1.000000\n",
      "epoch 0, iter 745, loss: 0.281536, acc: 1.000000\n",
      "epoch 0, iter 746, loss: 0.288051, acc: 1.000000\n",
      "epoch 0, iter 747, loss: 0.257744, acc: 1.000000\n",
      "epoch 0, iter 748, loss: 0.225922, acc: 1.000000\n",
      "epoch 0, iter 749, loss: 1.136577, acc: 0.947368\n",
      "epoch 0, iter 750, loss: 0.873384, acc: 1.000000\n",
      "epoch 0, iter 751, loss: 0.647289, acc: 1.000000\n",
      "epoch 0, iter 752, loss: 0.509873, acc: 1.000000\n",
      "epoch 0, iter 753, loss: 0.866048, acc: 0.947368\n",
      "epoch 0, iter 754, loss: 1.303637, acc: 0.947368\n",
      "epoch 0, iter 755, loss: 0.975444, acc: 1.000000\n",
      "epoch 0, iter 756, loss: 0.757498, acc: 1.000000\n",
      "epoch 0, iter 757, loss: 0.772317, acc: 1.000000\n",
      "epoch 0, iter 758, loss: 0.737723, acc: 1.000000\n",
      "epoch 0, iter 759, loss: 0.556135, acc: 1.000000\n",
      "epoch 0, iter 760, loss: 0.437724, acc: 1.000000\n",
      "epoch 0, iter 761, loss: 0.410334, acc: 1.000000\n",
      "epoch 0, iter 762, loss: 0.385633, acc: 1.000000\n",
      "epoch 0, iter 763, loss: 0.304581, acc: 1.000000\n",
      "epoch 0, iter 764, loss: 0.258299, acc: 1.000000\n",
      "epoch 0, iter 765, loss: 0.248476, acc: 1.000000\n",
      "epoch 0, iter 766, loss: 0.248069, acc: 1.000000\n",
      "epoch 0, iter 767, loss: 0.207765, acc: 1.000000\n",
      "epoch 0, iter 768, loss: 0.191254, acc: 1.000000\n",
      "epoch 0, iter 769, loss: 0.186674, acc: 1.000000\n",
      "epoch 0, iter 770, loss: 0.194796, acc: 1.000000\n",
      "epoch 0, iter 771, loss: 0.170944, acc: 1.000000\n",
      "epoch 0, iter 772, loss: 0.166470, acc: 1.000000\n",
      "epoch 0, iter 773, loss: 0.162302, acc: 1.000000\n",
      "epoch 0, iter 774, loss: 0.172618, acc: 1.000000\n",
      "epoch 0, iter 775, loss: 1.027646, acc: 0.947368\n",
      "epoch 0, iter 776, loss: 0.776537, acc: 1.000000\n",
      "epoch 0, iter 777, loss: 0.577338, acc: 1.000000\n",
      "epoch 0, iter 778, loss: 0.469596, acc: 1.000000\n",
      "epoch 0, iter 779, loss: 0.424899, acc: 1.000000\n",
      "epoch 0, iter 780, loss: 0.348423, acc: 1.000000\n",
      "epoch 0, iter 781, loss: 0.274162, acc: 1.000000\n",
      "epoch 0, iter 782, loss: 0.877877, acc: 0.947368\n",
      "epoch 0, iter 783, loss: 1.207830, acc: 0.947368\n",
      "epoch 0, iter 784, loss: 0.908224, acc: 1.000000\n",
      "epoch 0, iter 785, loss: 0.688245, acc: 1.000000\n",
      "epoch 0, iter 786, loss: 0.669238, acc: 1.000000\n",
      "epoch 0, iter 787, loss: 0.665399, acc: 1.000000\n",
      "epoch 0, iter 788, loss: 0.510662, acc: 1.000000\n",
      "epoch 0, iter 789, loss: 1.614360, acc: 0.947368\n",
      "epoch 0, iter 790, loss: 1.216856, acc: 1.000000\n",
      "epoch 0, iter 791, loss: 0.888149, acc: 1.000000\n",
      "epoch 0, iter 792, loss: 0.665252, acc: 1.000000\n",
      "epoch 0, iter 793, loss: 1.141899, acc: 0.947368\n",
      "epoch 0, iter 794, loss: 0.887029, acc: 1.000000\n",
      "epoch 0, iter 795, loss: 0.665627, acc: 1.000000\n",
      "epoch 0, iter 796, loss: 0.529465, acc: 1.000000\n",
      "epoch 0, iter 797, loss: 0.530263, acc: 1.000000\n",
      "epoch 0, iter 798, loss: 1.026481, acc: 0.947368\n",
      "epoch 0, iter 799, loss: 0.765556, acc: 1.000000\n",
      "epoch 0, iter 800, loss: 0.587503, acc: 1.000000\n",
      "epoch 0, iter 801, loss: 0.500679, acc: 1.000000\n",
      "epoch 0, iter 802, loss: 0.538621, acc: 1.000000\n",
      "epoch 0, iter 803, loss: 0.414718, acc: 1.000000\n",
      "epoch 0, iter 804, loss: 0.330415, acc: 1.000000\n",
      "epoch 0, iter 805, loss: 0.295902, acc: 1.000000\n",
      "epoch 0, iter 806, loss: 0.300783, acc: 1.000000\n",
      "epoch 0, iter 807, loss: 0.247399, acc: 1.000000\n",
      "epoch 0, iter 808, loss: 0.214266, acc: 1.000000\n",
      "epoch 0, iter 809, loss: 0.203783, acc: 1.000000\n",
      "epoch 0, iter 810, loss: 0.213843, acc: 1.000000\n",
      "epoch 0, iter 811, loss: 0.187238, acc: 1.000000\n",
      "epoch 0, iter 812, loss: 0.174278, acc: 1.000000\n",
      "epoch 0, iter 813, loss: 0.170425, acc: 1.000000\n",
      "epoch 0, iter 814, loss: 0.181136, acc: 1.000000\n",
      "epoch 0, iter 815, loss: 0.165094, acc: 1.000000\n",
      "epoch 0, iter 816, loss: 0.160306, acc: 1.000000\n",
      "epoch 0, iter 817, loss: 0.157522, acc: 1.000000\n",
      "epoch 0, iter 818, loss: 0.167228, acc: 1.000000\n",
      "epoch 0, iter 819, loss: 0.155841, acc: 1.000000\n",
      "epoch 0, iter 820, loss: 0.154651, acc: 1.000000\n",
      "epoch 0, iter 821, loss: 0.151547, acc: 1.000000\n",
      "epoch 0, iter 822, loss: 0.160081, acc: 1.000000\n",
      "epoch 0, iter 823, loss: 0.151048, acc: 1.000000\n",
      "epoch 0, iter 824, loss: 0.151605, acc: 1.000000\n",
      "epoch 0, iter 825, loss: 0.148001, acc: 1.000000\n",
      "epoch 0, iter 826, loss: 0.155573, acc: 1.000000\n",
      "epoch 0, iter 827, loss: 0.147887, acc: 1.000000\n",
      "epoch 0, iter 828, loss: 0.149374, acc: 1.000000\n",
      "epoch 0, iter 829, loss: 0.145377, acc: 1.000000\n",
      "epoch 0, iter 830, loss: 0.152220, acc: 1.000000\n",
      "epoch 0, iter 831, loss: 0.145383, acc: 1.000000\n",
      "epoch 0, iter 832, loss: 0.147401, acc: 1.000000\n",
      "epoch 0, iter 833, loss: 0.143147, acc: 1.000000\n",
      "epoch 0, iter 834, loss: 0.149440, acc: 1.000000\n",
      "epoch 0, iter 835, loss: 0.143178, acc: 1.000000\n",
      "epoch 0, iter 836, loss: 0.145514, acc: 1.000000\n",
      "epoch 0, iter 837, loss: 0.141113, acc: 1.000000\n",
      "epoch 0, iter 838, loss: 0.146985, acc: 1.000000\n",
      "epoch 0, iter 839, loss: 0.141132, acc: 1.000000\n",
      "epoch 0, iter 840, loss: 0.143663, acc: 1.000000\n",
      "epoch 0, iter 841, loss: 0.139191, acc: 1.000000\n",
      "epoch 0, iter 842, loss: 0.144730, acc: 1.000000\n",
      "epoch 0, iter 843, loss: 0.139182, acc: 1.000000\n",
      "epoch 0, iter 844, loss: 0.141832, acc: 1.000000\n",
      "epoch 0, iter 845, loss: 0.137340, acc: 1.000000\n",
      "epoch 0, iter 846, loss: 0.142612, acc: 1.000000\n",
      "epoch 0, iter 847, loss: 0.137297, acc: 1.000000\n",
      "epoch 0, iter 848, loss: 0.140016, acc: 1.000000\n",
      "epoch 0, iter 849, loss: 0.135536, acc: 1.000000\n",
      "epoch 0, iter 850, loss: 0.140587, acc: 1.000000\n",
      "epoch 0, iter 851, loss: 0.927179, acc: 0.947368\n",
      "epoch 0, iter 852, loss: 0.702343, acc: 1.000000\n",
      "epoch 0, iter 853, loss: 1.816487, acc: 0.947368\n",
      "epoch 0, iter 854, loss: 1.318153, acc: 1.000000\n",
      "epoch 0, iter 855, loss: 0.959111, acc: 1.000000\n",
      "epoch 0, iter 856, loss: 0.715454, acc: 1.000000\n",
      "epoch 0, iter 857, loss: 0.543881, acc: 1.000000\n",
      "epoch 0, iter 858, loss: 0.426670, acc: 1.000000\n",
      "epoch 0, iter 859, loss: 0.335568, acc: 1.000000\n",
      "epoch 0, iter 860, loss: 0.278237, acc: 1.000000\n",
      "epoch 0, iter 861, loss: 0.235924, acc: 1.000000\n",
      "epoch 0, iter 862, loss: 0.210433, acc: 1.000000\n",
      "epoch 0, iter 863, loss: 0.184443, acc: 1.000000\n",
      "epoch 0, iter 864, loss: 0.171873, acc: 1.000000\n",
      "epoch 0, iter 865, loss: 0.160141, acc: 1.000000\n",
      "epoch 0, iter 866, loss: 0.156720, acc: 1.000000\n",
      "epoch 0, iter 867, loss: 0.146870, acc: 1.000000\n",
      "epoch 0, iter 868, loss: 0.145057, acc: 1.000000\n",
      "epoch 0, iter 869, loss: 0.140376, acc: 1.000000\n",
      "epoch 0, iter 870, loss: 0.142235, acc: 1.000000\n",
      "epoch 0, iter 871, loss: 0.136603, acc: 1.000000\n",
      "epoch 0, iter 872, loss: 0.137381, acc: 1.000000\n",
      "epoch 0, iter 873, loss: 0.134211, acc: 1.000000\n",
      "epoch 0, iter 874, loss: 0.871321, acc: 0.947368\n",
      "epoch 0, iter 875, loss: 0.659915, acc: 1.000000\n",
      "epoch 0, iter 876, loss: 0.501177, acc: 1.000000\n",
      "epoch 0, iter 877, loss: 0.396078, acc: 1.000000\n",
      "epoch 0, iter 878, loss: 0.403704, acc: 1.000000\n",
      "epoch 0, iter 879, loss: 0.323472, acc: 1.000000\n",
      "epoch 0, iter 880, loss: 1.298080, acc: 0.947368\n",
      "epoch 0, iter 881, loss: 0.951986, acc: 1.000000\n",
      "epoch 0, iter 882, loss: 0.703086, acc: 1.000000\n",
      "epoch 0, iter 883, loss: 0.531532, acc: 1.000000\n",
      "epoch 0, iter 884, loss: 0.441320, acc: 1.000000\n",
      "epoch 0, iter 885, loss: 0.348132, acc: 1.000000\n",
      "epoch 0, iter 886, loss: 1.154255, acc: 0.947368\n",
      "epoch 0, iter 887, loss: 1.618907, acc: 0.947368\n",
      "epoch 0, iter 888, loss: 2.307280, acc: 0.947368\n",
      "epoch 0, iter 889, loss: 1.650142, acc: 1.000000\n",
      "epoch 0, iter 890, loss: 1.202647, acc: 1.000000\n",
      "epoch 0, iter 891, loss: 0.954941, acc: 1.000000\n",
      "epoch 0, iter 892, loss: 0.732969, acc: 1.000000\n",
      "epoch 0, iter 893, loss: 1.638665, acc: 0.947368\n",
      "epoch 0, iter 894, loss: 1.192092, acc: 1.000000\n",
      "epoch 0, iter 895, loss: 0.866367, acc: 1.000000\n",
      "epoch 0, iter 896, loss: 1.274144, acc: 0.947368\n",
      "epoch 0, iter 897, loss: 0.972044, acc: 1.000000\n",
      "epoch 0, iter 898, loss: 0.731788, acc: 1.000000\n",
      "epoch 0, iter 899, loss: 0.557487, acc: 1.000000\n",
      "epoch 0, iter 900, loss: 0.543139, acc: 1.000000\n",
      "epoch 0, iter 901, loss: 0.435922, acc: 1.000000\n",
      "epoch 0, iter 902, loss: 1.338044, acc: 0.947368\n",
      "epoch 0, iter 903, loss: 0.974439, acc: 1.000000\n",
      "epoch 0, iter 904, loss: 0.718418, acc: 1.000000\n",
      "epoch 0, iter 905, loss: 0.548587, acc: 1.000000\n",
      "epoch 0, iter 906, loss: 0.468935, acc: 1.000000\n",
      "epoch 0, iter 907, loss: 0.364931, acc: 1.000000\n",
      "epoch 0, iter 908, loss: 0.293731, acc: 1.000000\n",
      "epoch 0, iter 909, loss: 0.247937, acc: 1.000000\n",
      "epoch 0, iter 910, loss: 0.777683, acc: 0.947368\n",
      "epoch 0, iter 911, loss: 0.591065, acc: 1.000000\n",
      "epoch 0, iter 912, loss: 0.450700, acc: 1.000000\n",
      "epoch 0, iter 913, loss: 1.122352, acc: 1.000000\n",
      "epoch 0, iter 914, loss: 1.026742, acc: 1.000000\n",
      "epoch 0, iter 915, loss: 2.040662, acc: 0.947368\n",
      "epoch 0, iter 916, loss: 1.464657, acc: 1.000000\n",
      "epoch 0, iter 917, loss: 1.850890, acc: 0.947368\n",
      "epoch 0, iter 918, loss: 1.393621, acc: 1.000000\n",
      "epoch 0, iter 919, loss: 2.182318, acc: 0.947368\n",
      "epoch 0, iter 920, loss: 1.567911, acc: 1.000000\n",
      "epoch 0, iter 921, loss: 1.140395, acc: 1.000000\n",
      "epoch 0, iter 922, loss: 0.872150, acc: 1.000000\n",
      "epoch 0, iter 923, loss: 0.684343, acc: 1.000000\n",
      "epoch 0, iter 924, loss: 1.315122, acc: 0.947368\n",
      "epoch 0, iter 925, loss: 0.981422, acc: 1.000000\n",
      "epoch 0, iter 926, loss: 0.731869, acc: 1.000000\n",
      "epoch 0, iter 927, loss: 0.581770, acc: 1.000000\n",
      "epoch 0, iter 928, loss: 0.522953, acc: 1.000000\n",
      "epoch 0, iter 929, loss: 0.413508, acc: 1.000000\n",
      "epoch 0, iter 930, loss: 0.329537, acc: 1.000000\n",
      "epoch 0, iter 931, loss: 0.286388, acc: 1.000000\n",
      "epoch 0, iter 932, loss: 0.270312, acc: 1.000000\n",
      "epoch 0, iter 933, loss: 0.956411, acc: 0.947368\n",
      "epoch 0, iter 934, loss: 1.538864, acc: 0.947368\n",
      "epoch 0, iter 935, loss: 1.125602, acc: 1.000000\n",
      "epoch 0, iter 936, loss: 0.832696, acc: 1.000000\n",
      "epoch 0, iter 937, loss: 0.705347, acc: 1.000000\n",
      "epoch 0, iter 938, loss: 0.618584, acc: 1.000000\n",
      "epoch 0, iter 939, loss: 1.429239, acc: 0.947368\n",
      "epoch 0, iter 940, loss: 1.967837, acc: 0.947368\n",
      "epoch 0, iter 941, loss: 1.417325, acc: 1.000000\n",
      "epoch 0, iter 942, loss: 1.035510, acc: 1.000000\n",
      "epoch 0, iter 943, loss: 1.094487, acc: 0.947368\n",
      "epoch 0, iter 944, loss: 0.847029, acc: 1.000000\n",
      "epoch 0, iter 945, loss: 1.884543, acc: 0.947368\n",
      "epoch 0, iter 946, loss: 1.361004, acc: 0.947368\n",
      "epoch 0, iter 947, loss: 1.453350, acc: 1.000000\n",
      "epoch 0, iter 948, loss: 1.093627, acc: 1.000000\n",
      "epoch 0, iter 949, loss: 0.861812, acc: 1.000000\n",
      "epoch 0, iter 950, loss: 0.649844, acc: 1.000000\n",
      "epoch 0, iter 951, loss: 0.497423, acc: 1.000000\n",
      "epoch 0, iter 952, loss: 0.408840, acc: 1.000000\n",
      "epoch 0, iter 953, loss: 0.352156, acc: 1.000000\n",
      "epoch 0, iter 954, loss: 0.293612, acc: 1.000000\n",
      "epoch 0, iter 955, loss: 0.246199, acc: 1.000000\n",
      "epoch 0, iter 956, loss: 0.226108, acc: 1.000000\n",
      "epoch 0, iter 957, loss: 0.213027, acc: 1.000000\n",
      "epoch 0, iter 958, loss: 0.197040, acc: 1.000000\n",
      "epoch 0, iter 959, loss: 0.178148, acc: 1.000000\n",
      "epoch 0, iter 960, loss: 0.174960, acc: 1.000000\n",
      "epoch 0, iter 961, loss: 0.171854, acc: 1.000000\n",
      "epoch 0, iter 962, loss: 0.168471, acc: 1.000000\n",
      "epoch 0, iter 963, loss: 0.158427, acc: 1.000000\n",
      "epoch 0, iter 964, loss: 0.844068, acc: 0.947368\n",
      "epoch 0, iter 965, loss: 0.652750, acc: 1.000000\n",
      "epoch 0, iter 966, loss: 0.500088, acc: 1.000000\n",
      "epoch 0, iter 967, loss: 0.400110, acc: 1.000000\n",
      "epoch 0, iter 968, loss: 0.414831, acc: 1.000000\n",
      "epoch 0, iter 969, loss: 0.340357, acc: 1.000000\n",
      "epoch 0, iter 970, loss: 0.275007, acc: 1.000000\n",
      "epoch 0, iter 971, loss: 0.236319, acc: 1.000000\n",
      "epoch 0, iter 972, loss: 0.709652, acc: 0.947368\n",
      "epoch 0, iter 973, loss: 0.561598, acc: 1.000000\n",
      "epoch 0, iter 974, loss: 0.443493, acc: 1.000000\n",
      "epoch 0, iter 975, loss: 0.366840, acc: 1.000000\n",
      "epoch 0, iter 976, loss: 0.456028, acc: 1.000000\n",
      "epoch 0, iter 977, loss: 0.369430, acc: 1.000000\n",
      "epoch 0, iter 978, loss: 0.294770, acc: 1.000000\n",
      "epoch 0, iter 979, loss: 0.251926, acc: 1.000000\n",
      "epoch 0, iter 980, loss: 0.269223, acc: 1.000000\n",
      "epoch 0, iter 981, loss: 0.232847, acc: 1.000000\n",
      "epoch 0, iter 982, loss: 1.098351, acc: 0.947368\n",
      "epoch 0, iter 983, loss: 0.816242, acc: 1.000000\n",
      "epoch 0, iter 984, loss: 0.609293, acc: 1.000000\n",
      "epoch 0, iter 985, loss: 0.469804, acc: 1.000000\n",
      "epoch 0, iter 986, loss: 0.422415, acc: 1.000000\n",
      "epoch 0, iter 987, loss: 0.338411, acc: 1.000000\n",
      "epoch 0, iter 988, loss: 1.150749, acc: 0.947368\n",
      "epoch 0, iter 989, loss: 0.854211, acc: 1.000000\n",
      "epoch 0, iter 990, loss: 0.635301, acc: 1.000000\n",
      "epoch 0, iter 991, loss: 0.489559, acc: 1.000000\n",
      "epoch 0, iter 992, loss: 0.435282, acc: 1.000000\n",
      "epoch 0, iter 993, loss: 0.348207, acc: 1.000000\n",
      "epoch 0, iter 994, loss: 0.281753, acc: 1.000000\n",
      "epoch 0, iter 995, loss: 0.239006, acc: 1.000000\n",
      "epoch 0, iter 996, loss: 0.234192, acc: 1.000000\n",
      "epoch 0, iter 997, loss: 0.205228, acc: 1.000000\n",
      "epoch 0, iter 998, loss: 0.184129, acc: 1.000000\n",
      "epoch 0, iter 999, loss: 0.169333, acc: 1.000000\n",
      "epoch 0, iter 1000, loss: 0.174964, acc: 1.000000\n",
      "epoch 0, iter 1001, loss: 0.162587, acc: 1.000000\n",
      "epoch 0, iter 1002, loss: 0.156238, acc: 1.000000\n",
      "epoch 0, iter 1003, loss: 0.148972, acc: 1.000000\n",
      "epoch 0, iter 1004, loss: 0.798332, acc: 0.947368\n",
      "epoch 0, iter 1005, loss: 0.613022, acc: 1.000000\n",
      "epoch 0, iter 1006, loss: 0.470066, acc: 1.000000\n",
      "epoch 0, iter 1007, loss: 0.377762, acc: 1.000000\n",
      "epoch 0, iter 1008, loss: 0.408812, acc: 1.000000\n",
      "epoch 0, iter 1009, loss: 0.330253, acc: 1.000000\n",
      "epoch 0, iter 1010, loss: 0.265930, acc: 1.000000\n",
      "epoch 0, iter 1011, loss: 0.227785, acc: 1.000000\n",
      "epoch 0, iter 1012, loss: 0.687079, acc: 0.947368\n",
      "epoch 0, iter 1013, loss: 0.539254, acc: 1.000000\n",
      "epoch 0, iter 1014, loss: 0.424559, acc: 1.000000\n",
      "epoch 0, iter 1015, loss: 0.350215, acc: 1.000000\n",
      "epoch 0, iter 1016, loss: 0.451798, acc: 1.000000\n",
      "epoch 0, iter 1017, loss: 0.361588, acc: 1.000000\n",
      "epoch 0, iter 1018, loss: 0.287512, acc: 1.000000\n",
      "epoch 0, iter 1019, loss: 0.243792, acc: 1.000000\n",
      "epoch 0, iter 1020, loss: 0.265386, acc: 1.000000\n",
      "epoch 0, iter 1021, loss: 0.225891, acc: 1.000000\n",
      "epoch 0, iter 1022, loss: 0.192641, acc: 1.000000\n",
      "epoch 0, iter 1023, loss: 0.173988, acc: 1.000000\n",
      "epoch 0, iter 1024, loss: 0.188812, acc: 1.000000\n",
      "epoch 0, iter 1025, loss: 0.169973, acc: 1.000000\n",
      "epoch 0, iter 1026, loss: 0.155626, acc: 1.000000\n",
      "epoch 0, iter 1027, loss: 0.146538, acc: 1.000000\n",
      "epoch 0, iter 1028, loss: 0.158162, acc: 1.000000\n",
      "epoch 0, iter 1029, loss: 0.147187, acc: 1.000000\n",
      "epoch 0, iter 1030, loss: 0.141586, acc: 1.000000\n",
      "epoch 0, iter 1031, loss: 0.135706, acc: 1.000000\n",
      "epoch 0, iter 1032, loss: 0.144677, acc: 1.000000\n",
      "epoch 0, iter 1033, loss: 0.136742, acc: 1.000000\n",
      "epoch 0, iter 1034, loss: 0.893854, acc: 0.947368\n",
      "epoch 0, iter 1035, loss: 0.670966, acc: 1.000000\n",
      "epoch 0, iter 1036, loss: 0.507990, acc: 1.000000\n",
      "epoch 0, iter 1037, loss: 0.395901, acc: 1.000000\n",
      "epoch 0, iter 1038, loss: 0.393920, acc: 1.000000\n",
      "epoch 0, iter 1039, loss: 1.100574, acc: 0.947368\n",
      "epoch 0, iter 1040, loss: 0.812445, acc: 1.000000\n",
      "epoch 0, iter 1041, loss: 0.600751, acc: 1.000000\n",
      "epoch 0, iter 1042, loss: 0.955138, acc: 0.947368\n",
      "epoch 0, iter 1043, loss: 1.095556, acc: 0.947368\n",
      "epoch 0, iter 1044, loss: 0.833575, acc: 1.000000\n",
      "epoch 0, iter 1045, loss: 0.642346, acc: 1.000000\n",
      "epoch 0, iter 1046, loss: 0.678725, acc: 0.947368\n",
      "epoch 0, iter 1047, loss: 0.754421, acc: 1.000000\n",
      "epoch 0, iter 1048, loss: 1.640988, acc: 0.947368\n",
      "epoch 0, iter 1049, loss: 1.179577, acc: 1.000000\n",
      "epoch 0, iter 1050, loss: 1.704121, acc: 0.947368\n",
      "epoch 0, iter 1051, loss: 1.302261, acc: 1.000000\n",
      "epoch 0, iter 1052, loss: 0.947807, acc: 1.000000\n",
      "epoch 0, iter 1053, loss: 0.692775, acc: 1.000000\n",
      "epoch 0, iter 1054, loss: 0.600571, acc: 1.000000\n",
      "epoch 0, iter 1055, loss: 0.991199, acc: 0.947368\n",
      "epoch 0, iter 1056, loss: 0.743294, acc: 1.000000\n",
      "epoch 0, iter 1057, loss: 0.556155, acc: 1.000000\n",
      "epoch 0, iter 1058, loss: 0.475850, acc: 1.000000\n",
      "epoch 0, iter 1059, loss: 0.526337, acc: 1.000000\n",
      "epoch 0, iter 1060, loss: 1.207841, acc: 0.947368\n",
      "epoch 0, iter 1061, loss: 0.881643, acc: 1.000000\n",
      "epoch 0, iter 1062, loss: 0.659950, acc: 1.000000\n",
      "epoch 0, iter 1063, loss: 0.556318, acc: 1.000000\n",
      "epoch 0, iter 1064, loss: 0.500942, acc: 1.000000\n",
      "epoch 0, iter 1065, loss: 0.380552, acc: 1.000000\n",
      "epoch 0, iter 1066, loss: 0.305482, acc: 1.000000\n",
      "epoch 0, iter 1067, loss: 0.279843, acc: 1.000000\n",
      "epoch 0, iter 1068, loss: 0.268980, acc: 1.000000\n",
      "epoch 0, iter 1069, loss: 0.216845, acc: 1.000000\n",
      "epoch 0, iter 1070, loss: 0.191965, acc: 1.000000\n",
      "epoch 0, iter 1071, loss: 0.187584, acc: 1.000000\n",
      "epoch 0, iter 1072, loss: 0.189975, acc: 1.000000\n",
      "epoch 0, iter 1073, loss: 0.161665, acc: 1.000000\n",
      "epoch 0, iter 1074, loss: 0.154875, acc: 1.000000\n",
      "epoch 0, iter 1075, loss: 0.154673, acc: 1.000000\n",
      "epoch 0, iter 1076, loss: 0.159909, acc: 1.000000\n",
      "epoch 0, iter 1077, loss: 0.141093, acc: 1.000000\n",
      "epoch 0, iter 1078, loss: 0.141543, acc: 1.000000\n",
      "epoch 0, iter 1079, loss: 0.849624, acc: 0.947368\n",
      "epoch 0, iter 1080, loss: 0.650118, acc: 1.000000\n",
      "epoch 0, iter 1081, loss: 0.484245, acc: 1.000000\n",
      "epoch 0, iter 1082, loss: 0.390595, acc: 1.000000\n",
      "epoch 0, iter 1083, loss: 0.398867, acc: 1.000000\n",
      "epoch 0, iter 1084, loss: 0.328325, acc: 1.000000\n",
      "epoch 0, iter 1085, loss: 0.255491, acc: 1.000000\n",
      "epoch 0, iter 1086, loss: 0.225620, acc: 1.000000\n",
      "epoch 0, iter 1087, loss: 0.233150, acc: 1.000000\n",
      "epoch 0, iter 1088, loss: 0.209006, acc: 1.000000\n",
      "epoch 0, iter 1089, loss: 0.171707, acc: 1.000000\n",
      "epoch 0, iter 1090, loss: 0.164846, acc: 1.000000\n",
      "epoch 0, iter 1091, loss: 0.172355, acc: 1.000000\n",
      "epoch 0, iter 1092, loss: 0.164413, acc: 1.000000\n",
      "epoch 0, iter 1093, loss: 0.141153, acc: 1.000000\n",
      "epoch 0, iter 1094, loss: 0.142296, acc: 1.000000\n",
      "epoch 0, iter 1095, loss: 0.147505, acc: 1.000000\n",
      "epoch 0, iter 1096, loss: 0.145600, acc: 1.000000\n",
      "epoch 0, iter 1097, loss: 0.128807, acc: 1.000000\n",
      "epoch 0, iter 1098, loss: 0.132852, acc: 1.000000\n",
      "epoch 0, iter 1099, loss: 0.135654, acc: 1.000000\n",
      "epoch 0, iter 1100, loss: 0.136229, acc: 1.000000\n",
      "epoch 0, iter 1101, loss: 0.122968, acc: 1.000000\n",
      "epoch 0, iter 1102, loss: 0.128093, acc: 1.000000\n",
      "epoch 0, iter 1103, loss: 0.128982, acc: 1.000000\n",
      "epoch 0, iter 1104, loss: 0.130696, acc: 1.000000\n",
      "epoch 0, iter 1105, loss: 0.119640, acc: 1.000000\n",
      "epoch 0, iter 1106, loss: 0.125126, acc: 1.000000\n",
      "epoch 0, iter 1107, loss: 0.124616, acc: 1.000000\n",
      "epoch 0, iter 1108, loss: 0.126927, acc: 1.000000\n",
      "epoch 0, iter 1109, loss: 0.117371, acc: 1.000000\n",
      "epoch 0, iter 1110, loss: 0.122894, acc: 1.000000\n",
      "epoch 0, iter 1111, loss: 0.121388, acc: 1.000000\n",
      "epoch 0, iter 1112, loss: 0.856444, acc: 0.947368\n",
      "epoch 0, iter 1113, loss: 0.646076, acc: 1.000000\n",
      "epoch 0, iter 1114, loss: 0.493861, acc: 1.000000\n",
      "epoch 0, iter 1115, loss: 0.390665, acc: 1.000000\n",
      "epoch 0, iter 1116, loss: 0.430431, acc: 1.000000\n",
      "epoch 0, iter 1117, loss: 0.334348, acc: 1.000000\n",
      "epoch 0, iter 1118, loss: 0.262542, acc: 1.000000\n",
      "epoch 0, iter 1119, loss: 1.029400, acc: 0.947368\n",
      "epoch 0, iter 1120, loss: 0.791109, acc: 1.000000\n",
      "epoch 0, iter 1121, loss: 0.580717, acc: 1.000000\n",
      "epoch 0, iter 1122, loss: 0.444614, acc: 1.000000\n",
      "epoch 0, iter 1123, loss: 0.410840, acc: 1.000000\n",
      "epoch 0, iter 1124, loss: 0.340924, acc: 1.000000\n",
      "epoch 0, iter 1125, loss: 0.262743, acc: 1.000000\n",
      "epoch 0, iter 1126, loss: 0.220269, acc: 1.000000\n",
      "epoch 0, iter 1127, loss: 0.220252, acc: 1.000000\n",
      "epoch 0, iter 1128, loss: 0.201327, acc: 1.000000\n",
      "epoch 0, iter 1129, loss: 0.164698, acc: 1.000000\n",
      "epoch 0, iter 1130, loss: 0.151317, acc: 1.000000\n",
      "epoch 0, iter 1131, loss: 0.157794, acc: 1.000000\n",
      "epoch 0, iter 1132, loss: 0.153995, acc: 1.000000\n",
      "epoch 0, iter 1133, loss: 0.132014, acc: 1.000000\n",
      "epoch 0, iter 1134, loss: 0.128343, acc: 1.000000\n",
      "epoch 0, iter 1135, loss: 0.134134, acc: 1.000000\n",
      "epoch 0, iter 1136, loss: 0.135156, acc: 1.000000\n",
      "epoch 0, iter 1137, loss: 1.117599, acc: 0.947368\n",
      "epoch 0, iter 1138, loss: 0.824166, acc: 1.000000\n",
      "epoch 0, iter 1139, loss: 0.603579, acc: 1.000000\n",
      "epoch 0, iter 1140, loss: 0.468268, acc: 1.000000\n",
      "epoch 0, iter 1141, loss: 0.395014, acc: 1.000000\n",
      "epoch 0, iter 1142, loss: 0.314707, acc: 1.000000\n",
      "epoch 0, iter 1143, loss: 0.245177, acc: 1.000000\n",
      "epoch 0, iter 1144, loss: 0.212506, acc: 1.000000\n",
      "epoch 0, iter 1145, loss: 0.197768, acc: 1.000000\n",
      "epoch 0, iter 1146, loss: 0.175247, acc: 1.000000\n",
      "epoch 0, iter 1147, loss: 0.147741, acc: 1.000000\n",
      "epoch 0, iter 1148, loss: 0.142116, acc: 1.000000\n",
      "epoch 0, iter 1149, loss: 0.140586, acc: 1.000000\n",
      "epoch 0, iter 1150, loss: 0.911938, acc: 0.947368\n",
      "epoch 0, iter 1151, loss: 1.608806, acc: 0.947368\n",
      "epoch 0, iter 1152, loss: 2.439107, acc: 0.947368\n",
      "epoch 0, iter 1153, loss: 1.733594, acc: 1.000000\n",
      "epoch 0, iter 1154, loss: 1.256711, acc: 1.000000\n",
      "epoch 0, iter 1155, loss: 0.956341, acc: 1.000000\n",
      "epoch 0, iter 1156, loss: 0.715529, acc: 1.000000\n",
      "epoch 0, iter 1157, loss: 0.525748, acc: 1.000000\n",
      "epoch 0, iter 1158, loss: 0.408674, acc: 1.000000\n",
      "epoch 0, iter 1159, loss: 0.340406, acc: 1.000000\n",
      "epoch 0, iter 1160, loss: 0.281958, acc: 1.000000\n",
      "epoch 0, iter 1161, loss: 0.222743, acc: 1.000000\n",
      "epoch 0, iter 1162, loss: 0.195692, acc: 1.000000\n",
      "epoch 0, iter 1163, loss: 0.182478, acc: 1.000000\n",
      "epoch 0, iter 1164, loss: 0.169819, acc: 1.000000\n",
      "epoch 0, iter 1165, loss: 1.125736, acc: 0.947368\n",
      "epoch 0, iter 1166, loss: 0.831443, acc: 1.000000\n",
      "epoch 0, iter 1167, loss: 0.609398, acc: 1.000000\n",
      "epoch 0, iter 1168, loss: 0.473997, acc: 1.000000\n",
      "epoch 0, iter 1169, loss: 0.400489, acc: 1.000000\n",
      "epoch 0, iter 1170, loss: 1.068952, acc: 0.947368\n",
      "epoch 0, iter 1171, loss: 1.817179, acc: 0.947368\n",
      "epoch 0, iter 1172, loss: 1.310251, acc: 1.000000\n",
      "epoch 0, iter 1173, loss: 0.948714, acc: 1.000000\n",
      "epoch 0, iter 1174, loss: 0.787464, acc: 1.000000\n",
      "epoch 0, iter 1175, loss: 0.619479, acc: 1.000000\n",
      "epoch 0, iter 1176, loss: 0.465274, acc: 1.000000\n",
      "epoch 0, iter 1177, loss: 0.353357, acc: 1.000000\n",
      "epoch 0, iter 1178, loss: 0.321390, acc: 1.000000\n",
      "epoch 0, iter 1179, loss: 0.276347, acc: 1.000000\n",
      "epoch 0, iter 1180, loss: 0.225153, acc: 1.000000\n",
      "epoch 0, iter 1181, loss: 0.185059, acc: 1.000000\n",
      "epoch 0, iter 1182, loss: 0.749502, acc: 0.947368\n",
      "epoch 0, iter 1183, loss: 0.580318, acc: 1.000000\n",
      "epoch 0, iter 1184, loss: 0.444343, acc: 1.000000\n",
      "epoch 0, iter 1185, loss: 0.347176, acc: 1.000000\n",
      "epoch 0, iter 1186, loss: 0.408883, acc: 1.000000\n",
      "epoch 0, iter 1187, loss: 0.330444, acc: 1.000000\n",
      "epoch 0, iter 1188, loss: 0.260794, acc: 1.000000\n",
      "epoch 0, iter 1189, loss: 1.118788, acc: 0.947368\n",
      "epoch 0, iter 1190, loss: 0.873565, acc: 1.000000\n",
      "epoch 0, iter 1191, loss: 0.641480, acc: 1.000000\n",
      "epoch 0, iter 1192, loss: 0.484542, acc: 1.000000\n",
      "epoch 0, iter 1193, loss: 0.421040, acc: 1.000000\n",
      "epoch 0, iter 1194, loss: 0.360994, acc: 1.000000\n",
      "epoch 0, iter 1195, loss: 0.279358, acc: 1.000000\n",
      "epoch 0, iter 1196, loss: 0.228726, acc: 1.000000\n",
      "epoch 0, iter 1197, loss: 0.781154, acc: 0.947368\n",
      "epoch 0, iter 1198, loss: 0.616349, acc: 1.000000\n",
      "epoch 0, iter 1199, loss: 0.470515, acc: 1.000000\n",
      "epoch 0, iter 1200, loss: 0.375939, acc: 1.000000\n",
      "epoch 0, iter 1201, loss: 0.573381, acc: 0.947368\n",
      "epoch 0, iter 1202, loss: 0.476208, acc: 1.000000\n",
      "epoch 0, iter 1203, loss: 0.389825, acc: 1.000000\n",
      "epoch 0, iter 1204, loss: 0.330354, acc: 0.947368\n",
      "epoch 0, iter 1205, loss: 0.546217, acc: 1.000000\n",
      "epoch 0, iter 1206, loss: 0.442014, acc: 1.000000\n",
      "epoch 0, iter 1207, loss: 0.341217, acc: 1.000000\n",
      "epoch 0, iter 1208, loss: 0.281637, acc: 1.000000\n",
      "epoch 0, iter 1209, loss: 0.317266, acc: 1.000000\n",
      "epoch 0, iter 1210, loss: 0.270580, acc: 1.000000\n",
      "epoch 0, iter 1211, loss: 1.387572, acc: 0.947368\n",
      "epoch 0, iter 1212, loss: 1.006916, acc: 1.000000\n",
      "epoch 0, iter 1213, loss: 1.675361, acc: 0.947368\n",
      "epoch 0, iter 1214, loss: 1.220021, acc: 1.000000\n",
      "epoch 0, iter 1215, loss: 0.885925, acc: 1.000000\n",
      "epoch 0, iter 1216, loss: 0.660429, acc: 1.000000\n",
      "epoch 0, iter 1217, loss: 0.542456, acc: 1.000000\n",
      "epoch 0, iter 1218, loss: 0.423630, acc: 1.000000\n",
      "epoch 0, iter 1219, loss: 0.326512, acc: 1.000000\n",
      "epoch 0, iter 1220, loss: 0.266156, acc: 1.000000\n",
      "epoch 0, iter 1221, loss: 0.244472, acc: 1.000000\n",
      "epoch 0, iter 1222, loss: 0.938425, acc: 0.947368\n",
      "epoch 0, iter 1223, loss: 0.695361, acc: 1.000000\n",
      "epoch 0, iter 1224, loss: 1.767910, acc: 0.947368\n",
      "epoch 0, iter 1225, loss: 1.287837, acc: 1.000000\n",
      "epoch 0, iter 1226, loss: 0.943064, acc: 1.000000\n",
      "epoch 0, iter 1227, loss: 0.692211, acc: 1.000000\n",
      "epoch 0, iter 1228, loss: 0.526815, acc: 1.000000\n",
      "epoch 0, iter 1229, loss: 0.412982, acc: 1.000000\n",
      "epoch 0, iter 1230, loss: 0.330476, acc: 1.000000\n",
      "epoch 0, iter 1231, loss: 0.263697, acc: 1.000000\n",
      "epoch 0, iter 1232, loss: 0.225064, acc: 1.000000\n",
      "epoch 0, iter 1233, loss: 0.198054, acc: 1.000000\n",
      "epoch 0, iter 1234, loss: 0.179715, acc: 1.000000\n",
      "epoch 0, iter 1235, loss: 0.158310, acc: 1.000000\n",
      "epoch 0, iter 1236, loss: 0.150095, acc: 1.000000\n",
      "epoch 0, iter 1237, loss: 0.143132, acc: 1.000000\n",
      "epoch 0, iter 1238, loss: 0.140860, acc: 1.000000\n",
      "epoch 0, iter 1239, loss: 0.131107, acc: 1.000000\n",
      "epoch 0, iter 1240, loss: 0.130178, acc: 1.000000\n",
      "epoch 0, iter 1241, loss: 0.127470, acc: 1.000000\n",
      "epoch 0, iter 1242, loss: 0.129430, acc: 1.000000\n",
      "epoch 0, iter 1243, loss: 0.122985, acc: 1.000000\n",
      "epoch 0, iter 1244, loss: 0.123807, acc: 1.000000\n",
      "epoch 0, iter 1245, loss: 0.121735, acc: 1.000000\n",
      "epoch 0, iter 1246, loss: 0.124919, acc: 1.000000\n",
      "epoch 0, iter 1247, loss: 0.119626, acc: 1.000000\n",
      "epoch 0, iter 1248, loss: 0.120887, acc: 1.000000\n",
      "epoch 0, iter 1249, loss: 0.118701, acc: 1.000000\n",
      "epoch 0, iter 1250, loss: 0.122290, acc: 1.000000\n",
      "epoch 0, iter 1251, loss: 0.117532, acc: 1.000000\n",
      "epoch 0, iter 1252, loss: 0.118927, acc: 1.000000\n",
      "epoch 0, iter 1253, loss: 0.116529, acc: 1.000000\n",
      "epoch 0, iter 1254, loss: 0.120268, acc: 1.000000\n",
      "epoch 0, iter 1255, loss: 0.115829, acc: 1.000000\n",
      "epoch 0, iter 1256, loss: 0.117292, acc: 1.000000\n",
      "epoch 0, iter 1257, loss: 0.114715, acc: 1.000000\n",
      "epoch 0, iter 1258, loss: 0.118507, acc: 1.000000\n",
      "epoch 0, iter 1259, loss: 0.114290, acc: 1.000000\n",
      "epoch 0, iter 1260, loss: 0.889632, acc: 0.947368\n",
      "epoch 0, iter 1261, loss: 0.666064, acc: 1.000000\n",
      "epoch 0, iter 1262, loss: 1.711738, acc: 0.947368\n",
      "epoch 0, iter 1263, loss: 1.231331, acc: 1.000000\n",
      "epoch 0, iter 1264, loss: 0.897213, acc: 1.000000\n",
      "epoch 0, iter 1265, loss: 0.659877, acc: 1.000000\n",
      "epoch 0, iter 1266, loss: 0.508878, acc: 1.000000\n",
      "epoch 0, iter 1267, loss: 1.204909, acc: 0.947368\n",
      "epoch 0, iter 1268, loss: 1.678773, acc: 0.947368\n",
      "epoch 0, iter 1269, loss: 1.213043, acc: 1.000000\n",
      "epoch 0, iter 1270, loss: 0.892414, acc: 1.000000\n",
      "epoch 0, iter 1271, loss: 0.741644, acc: 1.000000\n",
      "epoch 0, iter 1272, loss: 0.963432, acc: 0.947368\n",
      "epoch 0, iter 1273, loss: 1.804951, acc: 0.947368\n",
      "epoch 0, iter 1274, loss: 1.332345, acc: 1.000000\n",
      "epoch 0, iter 1275, loss: 1.965469, acc: 1.000000\n",
      "epoch 0, iter 1276, loss: 1.657321, acc: 1.000000\n",
      "epoch 0, iter 1277, loss: 2.198679, acc: 0.947368\n",
      "epoch 0, iter 1278, loss: 1.582241, acc: 1.000000\n",
      "epoch 0, iter 1279, loss: 1.903403, acc: 0.947368\n",
      "epoch 0, iter 1280, loss: 1.428261, acc: 1.000000\n",
      "epoch 0, iter 1281, loss: 1.030619, acc: 1.000000\n",
      "epoch 0, iter 1282, loss: 0.755646, acc: 0.947368\n",
      "epoch 0, iter 1283, loss: 1.218075, acc: 1.000000\n",
      "epoch 0, iter 1284, loss: 0.925144, acc: 1.000000\n",
      "epoch 0, iter 1285, loss: 0.700802, acc: 1.000000\n",
      "epoch 0, iter 1286, loss: 0.523738, acc: 1.000000\n",
      "epoch 0, iter 1287, loss: 0.402437, acc: 1.000000\n",
      "epoch 0, iter 1288, loss: 0.338667, acc: 1.000000\n",
      "epoch 0, iter 1289, loss: 0.282016, acc: 1.000000\n",
      "epoch 0, iter 1290, loss: 0.231674, acc: 1.000000\n",
      "epoch 0, iter 1291, loss: 0.197522, acc: 1.000000\n",
      "epoch 0, iter 1292, loss: 0.187625, acc: 1.000000\n",
      "epoch 0, iter 1293, loss: 0.171971, acc: 1.000000\n",
      "epoch 0, iter 1294, loss: 0.155858, acc: 1.000000\n",
      "epoch 0, iter 1295, loss: 0.925374, acc: 0.947368\n",
      "epoch 0, iter 1296, loss: 0.700504, acc: 1.000000\n",
      "epoch 0, iter 1297, loss: 0.518286, acc: 1.000000\n",
      "epoch 0, iter 1298, loss: 0.403823, acc: 1.000000\n",
      "epoch 0, iter 1299, loss: 0.397604, acc: 1.000000\n",
      "epoch 0, iter 1300, loss: 0.324469, acc: 1.000000\n",
      "epoch 0, iter 1301, loss: 0.253595, acc: 1.000000\n",
      "epoch 0, iter 1302, loss: 0.216075, acc: 1.000000\n",
      "epoch 0, iter 1303, loss: 0.222852, acc: 1.000000\n",
      "epoch 0, iter 1304, loss: 0.198865, acc: 1.000000\n",
      "epoch 0, iter 1305, loss: 0.166533, acc: 1.000000\n",
      "epoch 0, iter 1306, loss: 0.154176, acc: 1.000000\n",
      "epoch 0, iter 1307, loss: 0.163531, acc: 1.000000\n",
      "epoch 0, iter 1308, loss: 0.155352, acc: 1.000000\n",
      "epoch 0, iter 1309, loss: 0.137366, acc: 1.000000\n",
      "epoch 0, iter 1310, loss: 0.133292, acc: 1.000000\n",
      "epoch 0, iter 1311, loss: 0.140824, acc: 1.000000\n",
      "epoch 0, iter 1312, loss: 0.138079, acc: 1.000000\n",
      "epoch 0, iter 1313, loss: 0.126392, acc: 1.000000\n",
      "epoch 0, iter 1314, loss: 0.125299, acc: 1.000000\n",
      "epoch 0, iter 1315, loss: 0.802707, acc: 0.947368\n",
      "epoch 0, iter 1316, loss: 0.608318, acc: 1.000000\n",
      "epoch 0, iter 1317, loss: 0.454065, acc: 1.000000\n",
      "epoch 0, iter 1318, loss: 0.360358, acc: 1.000000\n",
      "epoch 0, iter 1319, loss: 0.388089, acc: 1.000000\n",
      "epoch 0, iter 1320, loss: 0.313188, acc: 1.000000\n",
      "epoch 0, iter 1321, loss: 0.244175, acc: 1.000000\n",
      "epoch 0, iter 1322, loss: 0.967814, acc: 0.947368\n",
      "epoch 0, iter 1323, loss: 0.767880, acc: 1.000000\n",
      "epoch 0, iter 1324, loss: 1.791653, acc: 0.947368\n",
      "epoch 0, iter 1325, loss: 1.280921, acc: 1.000000\n",
      "epoch 0, iter 1326, loss: 0.934018, acc: 1.000000\n",
      "epoch 0, iter 1327, loss: 0.711914, acc: 1.000000\n",
      "epoch 0, iter 1328, loss: 0.542879, acc: 1.000000\n",
      "epoch 0, iter 1329, loss: 0.407722, acc: 1.000000\n",
      "epoch 0, iter 1330, loss: 0.322700, acc: 1.000000\n",
      "epoch 0, iter 1331, loss: 0.274753, acc: 1.000000\n",
      "epoch 0, iter 1332, loss: 0.234476, acc: 1.000000\n",
      "epoch 0, iter 1333, loss: 0.192869, acc: 1.000000\n",
      "epoch 0, iter 1334, loss: 0.172276, acc: 1.000000\n",
      "epoch 0, iter 1335, loss: 0.164114, acc: 1.000000\n",
      "epoch 0, iter 1336, loss: 0.155356, acc: 1.000000\n",
      "epoch 0, iter 1337, loss: 0.138321, acc: 1.000000\n",
      "epoch 0, iter 1338, loss: 0.134014, acc: 1.000000\n",
      "epoch 0, iter 1339, loss: 0.133935, acc: 1.000000\n",
      "epoch 0, iter 1340, loss: 0.132982, acc: 1.000000\n",
      "epoch 0, iter 1341, loss: 0.985065, acc: 0.947368\n",
      "epoch 0, iter 1342, loss: 0.734809, acc: 1.000000\n",
      "epoch 0, iter 1343, loss: 0.543799, acc: 1.000000\n",
      "epoch 0, iter 1344, loss: 0.424830, acc: 1.000000\n",
      "epoch 0, iter 1345, loss: 0.393140, acc: 1.000000\n",
      "epoch 0, iter 1346, loss: 0.315451, acc: 1.000000\n",
      "epoch 0, iter 1347, loss: 0.246392, acc: 1.000000\n",
      "epoch 0, iter 1348, loss: 0.211807, acc: 1.000000\n",
      "epoch 0, iter 1349, loss: 0.731824, acc: 0.947368\n",
      "epoch 0, iter 1350, loss: 0.565995, acc: 1.000000\n",
      "epoch 0, iter 1351, loss: 0.434610, acc: 1.000000\n",
      "epoch 0, iter 1352, loss: 0.352457, acc: 1.000000\n",
      "epoch 0, iter 1353, loss: 0.534847, acc: 0.947368\n",
      "epoch 0, iter 1354, loss: 0.438076, acc: 1.000000\n",
      "epoch 0, iter 1355, loss: 0.359835, acc: 1.000000\n",
      "epoch 0, iter 1356, loss: 0.303530, acc: 0.947368\n",
      "epoch 0, iter 1357, loss: 0.537744, acc: 1.000000\n",
      "epoch 0, iter 1358, loss: 0.427490, acc: 1.000000\n",
      "epoch 0, iter 1359, loss: 0.328218, acc: 1.000000\n",
      "epoch 0, iter 1360, loss: 0.273359, acc: 1.000000\n",
      "epoch 0, iter 1361, loss: 0.310553, acc: 1.000000\n",
      "epoch 0, iter 1362, loss: 0.258717, acc: 1.000000\n",
      "epoch 0, iter 1363, loss: 0.205347, acc: 1.000000\n",
      "epoch 0, iter 1364, loss: 0.181716, acc: 1.000000\n",
      "epoch 0, iter 1365, loss: 0.199693, acc: 1.000000\n",
      "epoch 0, iter 1366, loss: 0.178612, acc: 1.000000\n",
      "epoch 0, iter 1367, loss: 0.149435, acc: 1.000000\n",
      "epoch 0, iter 1368, loss: 0.140568, acc: 1.000000\n",
      "epoch 0, iter 1369, loss: 0.723665, acc: 0.947368\n",
      "epoch 0, iter 1370, loss: 1.342461, acc: 0.947368\n",
      "epoch 0, iter 1371, loss: 0.978362, acc: 1.000000\n",
      "epoch 0, iter 1372, loss: 0.723373, acc: 1.000000\n",
      "epoch 0, iter 1373, loss: 0.667756, acc: 1.000000\n",
      "epoch 0, iter 1374, loss: 0.578116, acc: 1.000000\n",
      "epoch 0, iter 1375, loss: 1.621830, acc: 0.947368\n",
      "epoch 0, iter 1376, loss: 1.164049, acc: 1.000000\n",
      "epoch 0, iter 1377, loss: 0.847580, acc: 1.000000\n",
      "epoch 0, iter 1378, loss: 0.667577, acc: 1.000000\n",
      "epoch 0, iter 1379, loss: 0.513535, acc: 1.000000\n",
      "epoch 0, iter 1380, loss: 0.387872, acc: 1.000000\n",
      "epoch 0, iter 1381, loss: 0.303909, acc: 1.000000\n",
      "epoch 0, iter 1382, loss: 0.834348, acc: 0.947368\n",
      "epoch 0, iter 1383, loss: 0.637394, acc: 1.000000\n",
      "epoch 0, iter 1384, loss: 0.479913, acc: 1.000000\n",
      "epoch 0, iter 1385, loss: 0.375886, acc: 1.000000\n",
      "epoch 0, iter 1386, loss: 0.420972, acc: 1.000000\n",
      "epoch 0, iter 1387, loss: 0.337802, acc: 1.000000\n",
      "epoch 0, iter 1388, loss: 0.263904, acc: 1.000000\n",
      "epoch 0, iter 1389, loss: 0.220410, acc: 1.000000\n",
      "epoch 0, iter 1390, loss: 0.241598, acc: 1.000000\n",
      "epoch 0, iter 1391, loss: 0.207651, acc: 1.000000\n",
      "epoch 0, iter 1392, loss: 0.172035, acc: 1.000000\n",
      "epoch 0, iter 1393, loss: 0.154793, acc: 1.000000\n",
      "epoch 0, iter 1394, loss: 0.171697, acc: 1.000000\n",
      "epoch 0, iter 1395, loss: 0.156311, acc: 1.000000\n",
      "epoch 0, iter 1396, loss: 0.136979, acc: 1.000000\n",
      "epoch 0, iter 1397, loss: 0.129631, acc: 1.000000\n",
      "epoch 0, iter 1398, loss: 0.142914, acc: 1.000000\n",
      "epoch 0, iter 1399, loss: 0.134726, acc: 1.000000\n",
      "epoch 0, iter 1400, loss: 0.123045, acc: 1.000000\n",
      "epoch 0, iter 1401, loss: 0.913283, acc: 0.947368\n",
      "epoch 0, iter 1402, loss: 0.691865, acc: 1.000000\n",
      "epoch 0, iter 1403, loss: 0.513108, acc: 1.000000\n",
      "epoch 0, iter 1404, loss: 0.394248, acc: 1.000000\n",
      "epoch 0, iter 1405, loss: 0.685050, acc: 0.947368\n",
      "epoch 0, iter 1406, loss: 0.542177, acc: 1.000000\n",
      "epoch 0, iter 1407, loss: 1.930880, acc: 0.947368\n",
      "epoch 0, iter 1408, loss: 1.387660, acc: 1.000000\n",
      "epoch 0, iter 1409, loss: 1.068475, acc: 1.000000\n",
      "epoch 0, iter 1410, loss: 0.792435, acc: 1.000000\n",
      "epoch 0, iter 1411, loss: 0.591162, acc: 1.000000\n",
      "epoch 0, iter 1412, loss: 0.447803, acc: 1.000000\n",
      "epoch 0, iter 1413, loss: 0.375973, acc: 1.000000\n",
      "epoch 0, iter 1414, loss: 0.303829, acc: 1.000000\n",
      "epoch 0, iter 1415, loss: 1.055753, acc: 0.947368\n",
      "epoch 0, iter 1416, loss: 0.781944, acc: 1.000000\n",
      "epoch 0, iter 1417, loss: 0.575431, acc: 1.000000\n",
      "epoch 0, iter 1418, loss: 0.449580, acc: 1.000000\n",
      "epoch 0, iter 1419, loss: 0.423524, acc: 1.000000\n",
      "epoch 0, iter 1420, loss: 0.333956, acc: 1.000000\n",
      "epoch 0, iter 1421, loss: 0.262999, acc: 1.000000\n",
      "epoch 0, iter 1422, loss: 0.224353, acc: 1.000000\n",
      "epoch 0, iter 1423, loss: 0.225428, acc: 1.000000\n",
      "epoch 0, iter 1424, loss: 0.192767, acc: 1.000000\n",
      "epoch 0, iter 1425, loss: 0.165511, acc: 1.000000\n",
      "epoch 0, iter 1426, loss: 0.153572, acc: 1.000000\n",
      "epoch 0, iter 1427, loss: 0.159904, acc: 1.000000\n",
      "epoch 0, iter 1428, loss: 0.145726, acc: 1.000000\n",
      "epoch 0, iter 1429, loss: 0.956743, acc: 0.947368\n",
      "epoch 0, iter 1430, loss: 0.713144, acc: 1.000000\n",
      "epoch 0, iter 1431, loss: 0.537785, acc: 1.000000\n",
      "epoch 0, iter 1432, loss: 0.413550, acc: 1.000000\n",
      "epoch 0, iter 1433, loss: 0.398464, acc: 1.000000\n",
      "epoch 0, iter 1434, loss: 0.316567, acc: 1.000000\n",
      "epoch 0, iter 1435, loss: 1.195347, acc: 0.947368\n",
      "epoch 0, iter 1436, loss: 0.875903, acc: 1.000000\n",
      "epoch 0, iter 1437, loss: 1.514494, acc: 0.947368\n",
      "epoch 0, iter 1438, loss: 1.107683, acc: 1.000000\n",
      "epoch 0, iter 1439, loss: 1.561843, acc: 0.947368\n",
      "epoch 0, iter 1440, loss: 1.127476, acc: 1.000000\n",
      "epoch 0, iter 1441, loss: 0.833449, acc: 1.000000\n",
      "epoch 0, iter 1442, loss: 0.625630, acc: 0.947368\n",
      "epoch 0, iter 1443, loss: 0.826008, acc: 1.000000\n",
      "epoch 0, iter 1444, loss: 0.615177, acc: 1.000000\n",
      "epoch 0, iter 1445, loss: 0.496774, acc: 1.000000\n",
      "epoch 0, iter 1446, loss: 0.389703, acc: 1.000000\n",
      "epoch 0, iter 1447, loss: 0.329830, acc: 1.000000\n",
      "epoch 0, iter 1448, loss: 0.266155, acc: 1.000000\n",
      "epoch 0, iter 1449, loss: 0.239678, acc: 1.000000\n",
      "epoch 0, iter 1450, loss: 0.205713, acc: 1.000000\n",
      "epoch 0, iter 1451, loss: 0.193533, acc: 1.000000\n",
      "epoch 0, iter 1452, loss: 0.169192, acc: 1.000000\n",
      "epoch 0, iter 1453, loss: 0.164992, acc: 1.000000\n",
      "epoch 0, iter 1454, loss: 0.151659, acc: 1.000000\n",
      "epoch 0, iter 1455, loss: 0.149574, acc: 1.000000\n",
      "epoch 0, iter 1456, loss: 0.955056, acc: 0.947368\n",
      "epoch 0, iter 1457, loss: 0.718730, acc: 1.000000\n",
      "epoch 0, iter 1458, loss: 0.536311, acc: 1.000000\n",
      "epoch 0, iter 1459, loss: 0.418388, acc: 1.000000\n",
      "epoch 0, iter 1460, loss: 0.759708, acc: 0.947368\n",
      "epoch 0, iter 1461, loss: 0.591933, acc: 1.000000\n",
      "epoch 0, iter 1462, loss: 0.479266, acc: 1.000000\n",
      "epoch 0, iter 1463, loss: 0.385070, acc: 0.947368\n",
      "epoch 0, iter 1464, loss: 0.509540, acc: 1.000000\n",
      "epoch 0, iter 1465, loss: 0.407190, acc: 1.000000\n",
      "epoch 0, iter 1466, loss: 1.552685, acc: 0.947368\n",
      "epoch 0, iter 1467, loss: 1.139352, acc: 1.000000\n",
      "epoch 0, iter 1468, loss: 0.835210, acc: 1.000000\n",
      "epoch 0, iter 1469, loss: 0.629261, acc: 1.000000\n",
      "epoch 0, iter 1470, loss: 0.500246, acc: 1.000000\n",
      "epoch 0, iter 1471, loss: 1.093998, acc: 0.947368\n",
      "epoch 0, iter 1472, loss: 1.617266, acc: 0.947368\n",
      "epoch 0, iter 1473, loss: 1.167694, acc: 1.000000\n",
      "epoch 0, iter 1474, loss: 0.863434, acc: 1.000000\n",
      "epoch 0, iter 1475, loss: 0.734526, acc: 1.000000\n",
      "epoch 0, iter 1476, loss: 0.624114, acc: 1.000000\n",
      "epoch 0, iter 1477, loss: 0.468313, acc: 1.000000\n",
      "epoch 0, iter 1478, loss: 0.361812, acc: 1.000000\n",
      "epoch 0, iter 1479, loss: 0.776980, acc: 0.947368\n",
      "epoch 0, iter 1480, loss: 1.148989, acc: 0.947368\n",
      "epoch 0, iter 1481, loss: 0.842933, acc: 1.000000\n",
      "epoch 0, iter 1482, loss: 0.644362, acc: 1.000000\n",
      "epoch 0, iter 1483, loss: 0.666696, acc: 1.000000\n",
      "epoch 0, iter 1484, loss: 0.773650, acc: 0.947368\n",
      "epoch 0, iter 1485, loss: 0.580752, acc: 1.000000\n",
      "epoch 0, iter 1486, loss: 0.464203, acc: 1.000000\n",
      "epoch 0, iter 1487, loss: 0.422848, acc: 0.947368\n",
      "epoch 0, iter 1488, loss: 0.674321, acc: 1.000000\n",
      "epoch 0, iter 1489, loss: 0.505833, acc: 1.000000\n",
      "epoch 0, iter 1490, loss: 0.386954, acc: 1.000000\n",
      "epoch 0, iter 1491, loss: 0.340034, acc: 1.000000\n",
      "epoch 0, iter 1492, loss: 0.348160, acc: 1.000000\n",
      "epoch 0, iter 1493, loss: 0.274217, acc: 1.000000\n",
      "epoch 0, iter 1494, loss: 0.219246, acc: 1.000000\n",
      "epoch 0, iter 1495, loss: 0.793077, acc: 0.947368\n",
      "epoch 0, iter 1496, loss: 1.134326, acc: 0.947368\n",
      "epoch 0, iter 1497, loss: 0.831746, acc: 1.000000\n",
      "epoch 0, iter 1498, loss: 0.628889, acc: 1.000000\n",
      "epoch 0, iter 1499, loss: 0.605258, acc: 1.000000\n",
      "epoch 0, iter 1500, loss: 0.729652, acc: 0.947368\n",
      "epoch 0, iter 1501, loss: 1.662040, acc: 0.947368\n",
      "epoch 0, iter 1502, loss: 1.217866, acc: 1.000000\n",
      "epoch 0, iter 1503, loss: 0.904168, acc: 0.947368\n",
      "epoch 0, iter 1504, loss: 0.961634, acc: 1.000000\n",
      "epoch 0, iter 1505, loss: 1.337614, acc: 0.947368\n",
      "epoch 0, iter 1506, loss: 0.983405, acc: 1.000000\n",
      "epoch 0, iter 1507, loss: 0.730091, acc: 1.000000\n",
      "epoch 0, iter 1508, loss: 0.652235, acc: 1.000000\n",
      "epoch 0, iter 1509, loss: 0.619095, acc: 1.000000\n",
      "epoch 0, iter 1510, loss: 0.465263, acc: 1.000000\n",
      "epoch 0, iter 1511, loss: 0.356083, acc: 1.000000\n",
      "epoch 0, iter 1512, loss: 0.329909, acc: 1.000000\n",
      "epoch 0, iter 1513, loss: 0.310309, acc: 1.000000\n",
      "epoch 0, iter 1514, loss: 0.245290, acc: 1.000000\n",
      "epoch 0, iter 1515, loss: 0.201014, acc: 1.000000\n",
      "epoch 0, iter 1516, loss: 0.199710, acc: 1.000000\n",
      "epoch 0, iter 1517, loss: 0.198603, acc: 1.000000\n",
      "epoch 0, iter 1518, loss: 0.166988, acc: 1.000000\n",
      "epoch 0, iter 1519, loss: 1.013920, acc: 0.947368\n",
      "epoch 0, iter 1520, loss: 0.760358, acc: 1.000000\n",
      "epoch 0, iter 1521, loss: 1.602577, acc: 0.947368\n",
      "epoch 0, iter 1522, loss: 1.155847, acc: 1.000000\n",
      "epoch 0, iter 1523, loss: 0.851869, acc: 1.000000\n",
      "epoch 0, iter 1524, loss: 0.641561, acc: 1.000000\n",
      "epoch 0, iter 1525, loss: 0.516334, acc: 1.000000\n",
      "epoch 0, iter 1526, loss: 0.393069, acc: 1.000000\n",
      "epoch 0, iter 1527, loss: 0.315391, acc: 1.000000\n",
      "epoch 0, iter 1528, loss: 0.980424, acc: 0.947368\n",
      "epoch 0, iter 1529, loss: 0.751606, acc: 1.000000\n",
      "epoch 0, iter 1530, loss: 0.557188, acc: 1.000000\n",
      "epoch 0, iter 1531, loss: 0.434177, acc: 1.000000\n",
      "epoch 0, iter 1532, loss: 0.701247, acc: 0.947368\n",
      "epoch 0, iter 1533, loss: 0.556253, acc: 1.000000\n",
      "epoch 0, iter 1534, loss: 0.433978, acc: 1.000000\n",
      "epoch 0, iter 1535, loss: 0.352602, acc: 0.947368\n",
      "epoch 0, iter 1536, loss: 0.511444, acc: 1.000000\n",
      "epoch 0, iter 1537, loss: 0.409715, acc: 1.000000\n",
      "epoch 0, iter 1538, loss: 0.314798, acc: 1.000000\n",
      "epoch 0, iter 1539, loss: 0.263206, acc: 1.000000\n",
      "epoch 0, iter 1540, loss: 0.296639, acc: 1.000000\n",
      "epoch 0, iter 1541, loss: 0.943737, acc: 0.947368\n",
      "epoch 0, iter 1542, loss: 0.695001, acc: 1.000000\n",
      "epoch 0, iter 1543, loss: 0.518209, acc: 1.000000\n",
      "epoch 0, iter 1544, loss: 0.442703, acc: 1.000000\n",
      "epoch 0, iter 1545, loss: 0.436679, acc: 1.000000\n",
      "epoch 0, iter 1546, loss: 0.334595, acc: 1.000000\n",
      "epoch 0, iter 1547, loss: 0.262433, acc: 1.000000\n",
      "epoch 0, iter 1548, loss: 0.803404, acc: 0.947368\n",
      "epoch 0, iter 1549, loss: 0.651870, acc: 1.000000\n",
      "epoch 0, iter 1550, loss: 0.488230, acc: 1.000000\n",
      "epoch 0, iter 1551, loss: 0.377003, acc: 1.000000\n",
      "epoch 0, iter 1552, loss: 0.423663, acc: 1.000000\n",
      "epoch 0, iter 1553, loss: 0.360366, acc: 1.000000\n",
      "epoch 0, iter 1554, loss: 1.466591, acc: 0.947368\n",
      "epoch 0, iter 1555, loss: 1.058734, acc: 1.000000\n",
      "epoch 0, iter 1556, loss: 0.775145, acc: 1.000000\n",
      "epoch 0, iter 1557, loss: 0.591032, acc: 1.000000\n",
      "epoch 0, iter 1558, loss: 0.456596, acc: 1.000000\n",
      "epoch 0, iter 1559, loss: 0.352460, acc: 1.000000\n",
      "epoch 0, iter 1560, loss: 0.281060, acc: 1.000000\n",
      "epoch 0, iter 1561, loss: 0.240897, acc: 1.000000\n",
      "epoch 0, iter 1562, loss: 0.208270, acc: 1.000000\n",
      "epoch 0, iter 1563, loss: 0.179127, acc: 1.000000\n",
      "epoch 0, iter 1564, loss: 0.159879, acc: 1.000000\n",
      "epoch 0, iter 1565, loss: 0.844187, acc: 0.947368\n",
      "epoch 0, iter 1566, loss: 0.635502, acc: 1.000000\n",
      "epoch 0, iter 1567, loss: 0.472754, acc: 1.000000\n",
      "epoch 0, iter 1568, loss: 0.370557, acc: 1.000000\n",
      "epoch 0, iter 1569, loss: 0.384713, acc: 1.000000\n",
      "epoch 0, iter 1570, loss: 0.308546, acc: 1.000000\n",
      "epoch 0, iter 1571, loss: 0.241324, acc: 1.000000\n",
      "epoch 0, iter 1572, loss: 0.205326, acc: 1.000000\n",
      "epoch 0, iter 1573, loss: 0.220438, acc: 1.000000\n",
      "epoch 0, iter 1574, loss: 0.191144, acc: 1.000000\n",
      "epoch 0, iter 1575, loss: 0.159968, acc: 1.000000\n",
      "epoch 0, iter 1576, loss: 0.147003, acc: 1.000000\n",
      "epoch 0, iter 1577, loss: 0.161871, acc: 1.000000\n",
      "epoch 0, iter 1578, loss: 0.148802, acc: 1.000000\n",
      "epoch 0, iter 1579, loss: 0.131736, acc: 1.000000\n",
      "epoch 0, iter 1580, loss: 0.126449, acc: 1.000000\n",
      "epoch 0, iter 1581, loss: 0.138326, acc: 1.000000\n",
      "epoch 0, iter 1582, loss: 0.912287, acc: 0.947368\n",
      "epoch 0, iter 1583, loss: 0.676012, acc: 1.000000\n",
      "epoch 0, iter 1584, loss: 0.502438, acc: 1.000000\n",
      "epoch 0, iter 1585, loss: 0.399968, acc: 1.000000\n",
      "epoch 0, iter 1586, loss: 0.376880, acc: 1.000000\n",
      "epoch 0, iter 1587, loss: 0.297912, acc: 1.000000\n",
      "epoch 0, iter 1588, loss: 0.234312, acc: 1.000000\n",
      "epoch 0, iter 1589, loss: 0.206720, acc: 1.000000\n",
      "epoch 0, iter 1590, loss: 0.209796, acc: 1.000000\n",
      "epoch 0, iter 1591, loss: 0.179705, acc: 1.000000\n",
      "epoch 0, iter 1592, loss: 0.151564, acc: 1.000000\n",
      "epoch 0, iter 1593, loss: 0.145677, acc: 1.000000\n",
      "epoch 0, iter 1594, loss: 0.153773, acc: 1.000000\n",
      "epoch 0, iter 1595, loss: 0.139923, acc: 1.000000\n",
      "epoch 0, iter 1596, loss: 0.124550, acc: 1.000000\n",
      "epoch 0, iter 1597, loss: 0.124691, acc: 1.000000\n",
      "epoch 0, iter 1598, loss: 0.132007, acc: 1.000000\n",
      "epoch 0, iter 1599, loss: 0.124376, acc: 1.000000\n",
      "epoch 0, iter 1600, loss: 0.114605, acc: 1.000000\n",
      "epoch 0, iter 1601, loss: 0.116229, acc: 1.000000\n",
      "epoch 0, iter 1602, loss: 0.121842, acc: 1.000000\n",
      "epoch 0, iter 1603, loss: 0.117039, acc: 1.000000\n",
      "epoch 0, iter 1604, loss: 0.110272, acc: 1.000000\n",
      "epoch 0, iter 1605, loss: 0.112051, acc: 1.000000\n",
      "epoch 0, iter 1606, loss: 0.116167, acc: 1.000000\n",
      "epoch 0, iter 1607, loss: 0.912250, acc: 0.947368\n",
      "epoch 0, iter 1608, loss: 0.673795, acc: 1.000000\n",
      "epoch 0, iter 1609, loss: 0.494574, acc: 1.000000\n",
      "epoch 0, iter 1610, loss: 1.113828, acc: 0.947368\n",
      "epoch 0, iter 1611, loss: 0.891289, acc: 1.000000\n",
      "epoch 0, iter 1612, loss: 0.654729, acc: 1.000000\n",
      "epoch 0, iter 1613, loss: 0.486562, acc: 1.000000\n",
      "epoch 0, iter 1614, loss: 0.445472, acc: 1.000000\n",
      "epoch 0, iter 1615, loss: 0.381230, acc: 1.000000\n",
      "epoch 0, iter 1616, loss: 0.292677, acc: 1.000000\n",
      "epoch 0, iter 1617, loss: 0.230806, acc: 1.000000\n",
      "epoch 0, iter 1618, loss: 0.229911, acc: 1.000000\n",
      "epoch 0, iter 1619, loss: 0.214566, acc: 1.000000\n",
      "epoch 0, iter 1620, loss: 0.175116, acc: 1.000000\n",
      "epoch 0, iter 1621, loss: 0.148606, acc: 1.000000\n",
      "epoch 0, iter 1622, loss: 0.156958, acc: 1.000000\n",
      "epoch 0, iter 1623, loss: 0.155254, acc: 1.000000\n",
      "epoch 0, iter 1624, loss: 0.134016, acc: 1.000000\n",
      "epoch 0, iter 1625, loss: 0.120537, acc: 1.000000\n",
      "epoch 0, iter 1626, loss: 0.778660, acc: 0.947368\n",
      "epoch 0, iter 1627, loss: 0.596887, acc: 1.000000\n",
      "epoch 0, iter 1628, loss: 0.446692, acc: 1.000000\n",
      "epoch 0, iter 1629, loss: 0.345927, acc: 1.000000\n",
      "epoch 0, iter 1630, loss: 0.368039, acc: 1.000000\n",
      "epoch 0, iter 1631, loss: 0.300315, acc: 1.000000\n",
      "epoch 0, iter 1632, loss: 0.234080, acc: 1.000000\n",
      "epoch 0, iter 1633, loss: 1.056401, acc: 0.947368\n",
      "epoch 0, iter 1634, loss: 0.817974, acc: 1.000000\n",
      "epoch 0, iter 1635, loss: 0.598218, acc: 1.000000\n",
      "epoch 0, iter 1636, loss: 0.444425, acc: 1.000000\n",
      "epoch 0, iter 1637, loss: 0.399211, acc: 1.000000\n",
      "epoch 0, iter 1638, loss: 0.337968, acc: 1.000000\n",
      "epoch 0, iter 1639, loss: 0.260334, acc: 1.000000\n",
      "epoch 0, iter 1640, loss: 1.207616, acc: 0.947368\n",
      "epoch 0, iter 1641, loss: 0.913711, acc: 1.000000\n",
      "epoch 0, iter 1642, loss: 1.721559, acc: 0.947368\n",
      "epoch 0, iter 1643, loss: 1.230946, acc: 1.000000\n",
      "epoch 0, iter 1644, loss: 0.888318, acc: 1.000000\n",
      "epoch 0, iter 1645, loss: 0.671125, acc: 1.000000\n",
      "epoch 0, iter 1646, loss: 0.529053, acc: 1.000000\n",
      "epoch 0, iter 1647, loss: 0.396500, acc: 1.000000\n",
      "epoch 0, iter 1648, loss: 0.304912, acc: 1.000000\n",
      "epoch 0, iter 1649, loss: 0.256927, acc: 1.000000\n",
      "epoch 0, iter 1650, loss: 0.227895, acc: 1.000000\n",
      "epoch 0, iter 1651, loss: 0.186334, acc: 1.000000\n",
      "epoch 0, iter 1652, loss: 0.158611, acc: 1.000000\n",
      "epoch 0, iter 1653, loss: 0.150701, acc: 1.000000\n",
      "epoch 0, iter 1654, loss: 0.148130, acc: 1.000000\n",
      "epoch 0, iter 1655, loss: 1.009544, acc: 0.947368\n",
      "epoch 0, iter 1656, loss: 0.739781, acc: 1.000000\n",
      "epoch 0, iter 1657, loss: 0.539884, acc: 1.000000\n",
      "epoch 0, iter 1658, loss: 1.104067, acc: 0.947368\n",
      "epoch 0, iter 1659, loss: 0.863577, acc: 1.000000\n",
      "epoch 0, iter 1660, loss: 0.634538, acc: 1.000000\n",
      "epoch 0, iter 1661, loss: 1.551512, acc: 0.947368\n",
      "epoch 0, iter 1662, loss: 1.477124, acc: 0.947368\n",
      "epoch 0, iter 1663, loss: 1.998909, acc: 0.947368\n",
      "epoch 0, iter 1664, loss: 2.987376, acc: 0.947368\n",
      "epoch 0, iter 1665, loss: 3.099327, acc: 0.947368\n",
      "epoch 0, iter 1666, loss: 2.513016, acc: 0.947368\n",
      "epoch 0, iter 1667, loss: 1.796758, acc: 1.000000\n",
      "epoch 0, iter 1668, loss: 2.679361, acc: 0.947368\n",
      "epoch 0, iter 1669, loss: 1.958426, acc: 1.000000\n",
      "epoch 0, iter 1670, loss: 1.709174, acc: 0.947368\n",
      "epoch 0, iter 1671, loss: 1.231681, acc: 1.000000\n",
      "epoch 0, iter 1672, loss: 1.915258, acc: 0.947368\n",
      "epoch 0, iter 1673, loss: 1.401975, acc: 1.000000\n",
      "epoch 0, iter 1674, loss: 1.150517, acc: 1.000000\n",
      "epoch 0, iter 1675, loss: 0.844113, acc: 0.947368\n",
      "epoch 0, iter 1676, loss: 0.901360, acc: 1.000000\n",
      "epoch 0, iter 1677, loss: 0.682766, acc: 1.000000\n",
      "epoch 0, iter 1678, loss: 0.572625, acc: 1.000000\n",
      "epoch 0, iter 1679, loss: 0.439055, acc: 1.000000\n",
      "epoch 0, iter 1680, loss: 0.365039, acc: 1.000000\n",
      "epoch 0, iter 1681, loss: 0.958827, acc: 0.947368\n",
      "epoch 0, iter 1682, loss: 0.739944, acc: 1.000000\n",
      "epoch 0, iter 1683, loss: 0.547852, acc: 1.000000\n",
      "epoch 0, iter 1684, loss: 0.442813, acc: 1.000000\n",
      "epoch 0, iter 1685, loss: 0.443440, acc: 1.000000\n",
      "epoch 0, iter 1686, loss: 0.357112, acc: 1.000000\n",
      "epoch 0, iter 1687, loss: 0.277059, acc: 1.000000\n",
      "epoch 0, iter 1688, loss: 0.244494, acc: 1.000000\n",
      "epoch 0, iter 1689, loss: 0.249012, acc: 1.000000\n",
      "epoch 0, iter 1690, loss: 0.217235, acc: 1.000000\n",
      "epoch 0, iter 1691, loss: 0.180142, acc: 1.000000\n",
      "epoch 0, iter 1692, loss: 0.171927, acc: 1.000000\n",
      "epoch 0, iter 1693, loss: 0.179336, acc: 1.000000\n",
      "epoch 0, iter 1694, loss: 0.166180, acc: 1.000000\n",
      "epoch 0, iter 1695, loss: 0.145697, acc: 1.000000\n",
      "epoch 0, iter 1696, loss: 0.144793, acc: 1.000000\n",
      "epoch 0, iter 1697, loss: 0.151580, acc: 1.000000\n",
      "epoch 0, iter 1698, loss: 0.145320, acc: 1.000000\n",
      "epoch 0, iter 1699, loss: 0.132224, acc: 1.000000\n",
      "epoch 0, iter 1700, loss: 0.843373, acc: 0.947368\n",
      "epoch 0, iter 1701, loss: 0.645363, acc: 1.000000\n",
      "epoch 0, iter 1702, loss: 1.601330, acc: 0.947368\n",
      "epoch 0, iter 1703, loss: 1.153619, acc: 1.000000\n",
      "epoch 0, iter 1704, loss: 0.856344, acc: 1.000000\n",
      "epoch 0, iter 1705, loss: 0.640271, acc: 1.000000\n",
      "epoch 0, iter 1706, loss: 0.538342, acc: 1.000000\n",
      "epoch 0, iter 1707, loss: 0.410107, acc: 1.000000\n",
      "epoch 0, iter 1708, loss: 0.335871, acc: 1.000000\n",
      "epoch 0, iter 1709, loss: 0.274607, acc: 1.000000\n",
      "epoch 0, iter 1710, loss: 0.240130, acc: 1.000000\n",
      "epoch 0, iter 1711, loss: 0.201567, acc: 1.000000\n",
      "epoch 0, iter 1712, loss: 0.187026, acc: 1.000000\n",
      "epoch 0, iter 1713, loss: 0.168764, acc: 1.000000\n",
      "epoch 0, iter 1714, loss: 0.161391, acc: 1.000000\n",
      "epoch 0, iter 1715, loss: 0.146497, acc: 1.000000\n",
      "epoch 0, iter 1716, loss: 0.146115, acc: 1.000000\n",
      "epoch 0, iter 1717, loss: 0.138831, acc: 1.000000\n",
      "epoch 0, iter 1718, loss: 0.138030, acc: 1.000000\n",
      "epoch 0, iter 1719, loss: 0.130069, acc: 1.000000\n",
      "epoch 0, iter 1720, loss: 0.132716, acc: 1.000000\n",
      "epoch 0, iter 1721, loss: 0.128416, acc: 1.000000\n",
      "epoch 0, iter 1722, loss: 0.129177, acc: 1.000000\n",
      "epoch 0, iter 1723, loss: 0.910165, acc: 0.947368\n",
      "epoch 0, iter 1724, loss: 0.681319, acc: 1.000000\n",
      "epoch 0, iter 1725, loss: 1.691280, acc: 0.947368\n",
      "epoch 0, iter 1726, loss: 1.222477, acc: 1.000000\n",
      "epoch 0, iter 1727, loss: 0.891361, acc: 1.000000\n",
      "epoch 0, iter 1728, loss: 0.664244, acc: 1.000000\n",
      "epoch 0, iter 1729, loss: 0.507592, acc: 1.000000\n",
      "epoch 0, iter 1730, loss: 0.393037, acc: 1.000000\n",
      "epoch 0, iter 1731, loss: 1.062721, acc: 0.947368\n",
      "epoch 0, iter 1732, loss: 0.784568, acc: 1.000000\n",
      "epoch 0, iter 1733, loss: 0.572958, acc: 1.000000\n",
      "epoch 0, iter 1734, loss: 0.440993, acc: 1.000000\n",
      "epoch 0, iter 1735, loss: 0.445034, acc: 1.000000\n",
      "epoch 0, iter 1736, loss: 0.350921, acc: 1.000000\n",
      "epoch 0, iter 1737, loss: 0.271680, acc: 1.000000\n",
      "epoch 0, iter 1738, loss: 0.227753, acc: 1.000000\n",
      "epoch 0, iter 1739, loss: 0.229615, acc: 1.000000\n",
      "epoch 0, iter 1740, loss: 0.198923, acc: 1.000000\n",
      "epoch 0, iter 1741, loss: 0.167069, acc: 1.000000\n",
      "epoch 0, iter 1742, loss: 0.898223, acc: 0.947368\n",
      "epoch 0, iter 1743, loss: 0.691687, acc: 1.000000\n",
      "epoch 0, iter 1744, loss: 0.515727, acc: 1.000000\n",
      "epoch 0, iter 1745, loss: 0.391967, acc: 1.000000\n",
      "epoch 0, iter 1746, loss: 0.385930, acc: 1.000000\n",
      "epoch 0, iter 1747, loss: 0.922266, acc: 0.947368\n",
      "epoch 0, iter 1748, loss: 0.677947, acc: 1.000000\n",
      "epoch 0, iter 1749, loss: 1.752411, acc: 0.947368\n",
      "epoch 0, iter 1750, loss: 1.297392, acc: 1.000000\n",
      "epoch 0, iter 1751, loss: 0.971765, acc: 1.000000\n",
      "epoch 0, iter 1752, loss: 0.708461, acc: 1.000000\n",
      "epoch 0, iter 1753, loss: 0.538333, acc: 1.000000\n",
      "epoch 0, iter 1754, loss: 0.993524, acc: 0.947368\n",
      "epoch 0, iter 1755, loss: 0.753892, acc: 1.000000\n",
      "epoch 0, iter 1756, loss: 0.559957, acc: 1.000000\n",
      "epoch 0, iter 1757, loss: 0.435545, acc: 1.000000\n",
      "epoch 0, iter 1758, loss: 0.478721, acc: 1.000000\n",
      "epoch 0, iter 1759, loss: 0.383164, acc: 1.000000\n",
      "epoch 0, iter 1760, loss: 0.294725, acc: 1.000000\n",
      "epoch 0, iter 1761, loss: 0.247811, acc: 1.000000\n",
      "epoch 0, iter 1762, loss: 0.260505, acc: 1.000000\n",
      "epoch 0, iter 1763, loss: 0.225230, acc: 1.000000\n",
      "epoch 0, iter 1764, loss: 0.183718, acc: 1.000000\n",
      "epoch 0, iter 1765, loss: 0.168564, acc: 1.000000\n",
      "epoch 0, iter 1766, loss: 0.181124, acc: 1.000000\n",
      "epoch 0, iter 1767, loss: 0.166680, acc: 1.000000\n",
      "epoch 0, iter 1768, loss: 0.143776, acc: 1.000000\n",
      "epoch 0, iter 1769, loss: 0.139411, acc: 1.000000\n",
      "epoch 0, iter 1770, loss: 0.149601, acc: 1.000000\n",
      "epoch 0, iter 1771, loss: 0.142718, acc: 1.000000\n",
      "epoch 0, iter 1772, loss: 0.128208, acc: 1.000000\n",
      "epoch 0, iter 1773, loss: 0.127486, acc: 1.000000\n",
      "epoch 0, iter 1774, loss: 0.135059, acc: 1.000000\n",
      "epoch 0, iter 1775, loss: 0.860490, acc: 0.947368\n",
      "epoch 0, iter 1776, loss: 1.510498, acc: 0.947368\n",
      "epoch 0, iter 1777, loss: 1.086304, acc: 1.000000\n",
      "epoch 0, iter 1778, loss: 0.788733, acc: 1.000000\n",
      "epoch 0, iter 1779, loss: 0.682882, acc: 1.000000\n",
      "epoch 0, iter 1780, loss: 0.990245, acc: 0.947368\n",
      "epoch 0, iter 1781, loss: 0.730959, acc: 1.000000\n",
      "epoch 0, iter 1782, loss: 0.547366, acc: 1.000000\n",
      "epoch 0, iter 1783, loss: 0.462848, acc: 1.000000\n",
      "epoch 0, iter 1784, loss: 0.524445, acc: 1.000000\n",
      "epoch 0, iter 1785, loss: 0.399962, acc: 1.000000\n",
      "epoch 0, iter 1786, loss: 0.305227, acc: 1.000000\n",
      "epoch 0, iter 1787, loss: 0.817576, acc: 0.947368\n",
      "epoch 0, iter 1788, loss: 0.673090, acc: 1.000000\n",
      "epoch 0, iter 1789, loss: 0.497794, acc: 1.000000\n",
      "epoch 0, iter 1790, loss: 0.375531, acc: 1.000000\n",
      "epoch 0, iter 1791, loss: 0.460502, acc: 1.000000\n",
      "epoch 0, iter 1792, loss: 0.393837, acc: 1.000000\n",
      "epoch 0, iter 1793, loss: 0.301080, acc: 1.000000\n",
      "epoch 0, iter 1794, loss: 1.191531, acc: 0.947368\n",
      "epoch 0, iter 1795, loss: 0.930199, acc: 1.000000\n",
      "epoch 0, iter 1796, loss: 0.682055, acc: 1.000000\n",
      "epoch 0, iter 1797, loss: 1.511272, acc: 0.947368\n",
      "epoch 0, iter 1798, loss: 1.123856, acc: 1.000000\n",
      "epoch 0, iter 1799, loss: 0.816177, acc: 1.000000\n",
      "epoch 0, iter 1800, loss: 0.602705, acc: 1.000000\n",
      "epoch 0, iter 1801, loss: 0.484940, acc: 1.000000\n",
      "epoch 0, iter 1802, loss: 0.392215, acc: 1.000000\n",
      "epoch 0, iter 1803, loss: 0.304887, acc: 1.000000\n",
      "epoch 0, iter 1804, loss: 0.244885, acc: 1.000000\n",
      "epoch 0, iter 1805, loss: 0.223465, acc: 1.000000\n",
      "epoch 0, iter 1806, loss: 0.202857, acc: 1.000000\n",
      "epoch 0, iter 1807, loss: 0.173359, acc: 1.000000\n",
      "epoch 0, iter 1808, loss: 0.153295, acc: 1.000000\n",
      "epoch 0, iter 1809, loss: 0.153444, acc: 1.000000\n",
      "epoch 0, iter 1810, loss: 0.149980, acc: 1.000000\n",
      "epoch 0, iter 1811, loss: 0.137149, acc: 1.000000\n",
      "epoch 0, iter 1812, loss: 0.128376, acc: 1.000000\n",
      "epoch 0, iter 1813, loss: 0.132372, acc: 1.000000\n",
      "epoch 0, iter 1814, loss: 0.132648, acc: 1.000000\n",
      "epoch 0, iter 1815, loss: 0.125515, acc: 1.000000\n",
      "epoch 0, iter 1816, loss: 0.120504, acc: 1.000000\n",
      "epoch 0, iter 1817, loss: 0.124392, acc: 1.000000\n",
      "epoch 0, iter 1818, loss: 0.842829, acc: 0.947368\n",
      "epoch 0, iter 1819, loss: 1.374666, acc: 0.947368\n",
      "epoch 0, iter 1820, loss: 0.993370, acc: 1.000000\n",
      "epoch 0, iter 1821, loss: 0.724761, acc: 1.000000\n",
      "epoch 0, iter 1822, loss: 0.632141, acc: 1.000000\n",
      "epoch 0, iter 1823, loss: 0.556008, acc: 1.000000\n",
      "epoch 0, iter 1824, loss: 0.416217, acc: 1.000000\n",
      "epoch 0, iter 1825, loss: 0.317654, acc: 1.000000\n",
      "epoch 0, iter 1826, loss: 0.297389, acc: 1.000000\n",
      "epoch 0, iter 1827, loss: 0.279855, acc: 1.000000\n",
      "epoch 0, iter 1828, loss: 1.197995, acc: 0.947368\n",
      "epoch 0, iter 1829, loss: 0.867949, acc: 1.000000\n",
      "epoch 0, iter 1830, loss: 0.634917, acc: 1.000000\n",
      "epoch 0, iter 1831, loss: 0.499156, acc: 1.000000\n",
      "epoch 0, iter 1832, loss: 0.413342, acc: 1.000000\n",
      "epoch 0, iter 1833, loss: 0.318908, acc: 1.000000\n",
      "epoch 0, iter 1834, loss: 0.251392, acc: 1.000000\n",
      "epoch 0, iter 1835, loss: 0.223764, acc: 1.000000\n",
      "epoch 0, iter 1836, loss: 0.209260, acc: 1.000000\n",
      "epoch 0, iter 1837, loss: 0.176570, acc: 1.000000\n",
      "epoch 0, iter 1838, loss: 0.152874, acc: 1.000000\n",
      "epoch 0, iter 1839, loss: 0.150422, acc: 1.000000\n",
      "epoch 0, iter 1840, loss: 0.151507, acc: 1.000000\n",
      "epoch 0, iter 1841, loss: 0.968144, acc: 0.947368\n",
      "epoch 0, iter 1842, loss: 0.712051, acc: 1.000000\n",
      "epoch 0, iter 1843, loss: 0.522219, acc: 1.000000\n",
      "epoch 0, iter 1844, loss: 0.411667, acc: 1.000000\n",
      "epoch 0, iter 1845, loss: 0.772496, acc: 0.947368\n",
      "epoch 0, iter 1846, loss: 0.588088, acc: 1.000000\n",
      "epoch 0, iter 1847, loss: 0.441811, acc: 1.000000\n",
      "epoch 0, iter 1848, loss: 0.358489, acc: 1.000000\n",
      "epoch 0, iter 1849, loss: 0.506601, acc: 0.947368\n",
      "epoch 0, iter 1850, loss: 0.408768, acc: 1.000000\n",
      "epoch 0, iter 1851, loss: 0.321062, acc: 1.000000\n",
      "epoch 0, iter 1852, loss: 0.273523, acc: 0.947368\n",
      "epoch 0, iter 1853, loss: 0.553303, acc: 1.000000\n",
      "epoch 0, iter 1854, loss: 0.430168, acc: 1.000000\n",
      "epoch 0, iter 1855, loss: 0.325005, acc: 1.000000\n",
      "epoch 0, iter 1856, loss: 0.270837, acc: 1.000000\n",
      "epoch 0, iter 1857, loss: 0.318894, acc: 1.000000\n",
      "epoch 0, iter 1858, loss: 0.260490, acc: 1.000000\n",
      "epoch 0, iter 1859, loss: 0.204922, acc: 1.000000\n",
      "epoch 0, iter 1860, loss: 0.181477, acc: 1.000000\n",
      "epoch 0, iter 1861, loss: 0.209929, acc: 1.000000\n",
      "epoch 0, iter 1862, loss: 0.182698, acc: 1.000000\n",
      "epoch 0, iter 1863, loss: 0.151354, acc: 1.000000\n",
      "epoch 0, iter 1864, loss: 0.141497, acc: 1.000000\n",
      "epoch 0, iter 1865, loss: 0.161746, acc: 1.000000\n",
      "epoch 0, iter 1866, loss: 0.921200, acc: 0.947368\n",
      "epoch 0, iter 1867, loss: 1.586776, acc: 0.947368\n",
      "epoch 0, iter 1868, loss: 1.139269, acc: 1.000000\n",
      "epoch 0, iter 1869, loss: 1.846433, acc: 0.947368\n",
      "epoch 0, iter 1870, loss: 1.391397, acc: 1.000000\n",
      "epoch 0, iter 1871, loss: 1.006309, acc: 1.000000\n",
      "epoch 0, iter 1872, loss: 0.729813, acc: 1.000000\n",
      "epoch 0, iter 1873, loss: 0.572773, acc: 1.000000\n",
      "epoch 0, iter 1874, loss: 0.469564, acc: 1.000000\n",
      "epoch 0, iter 1875, loss: 0.360886, acc: 1.000000\n",
      "epoch 0, iter 1876, loss: 0.278176, acc: 1.000000\n",
      "epoch 0, iter 1877, loss: 0.247323, acc: 1.000000\n",
      "epoch 0, iter 1878, loss: 0.227967, acc: 1.000000\n",
      "epoch 0, iter 1879, loss: 0.191914, acc: 1.000000\n",
      "epoch 0, iter 1880, loss: 1.088330, acc: 0.947368\n",
      "epoch 0, iter 1881, loss: 1.426735, acc: 0.947368\n",
      "epoch 0, iter 1882, loss: 1.033832, acc: 1.000000\n",
      "epoch 0, iter 1883, loss: 0.749867, acc: 1.000000\n",
      "epoch 0, iter 1884, loss: 0.602022, acc: 1.000000\n",
      "epoch 0, iter 1885, loss: 0.557687, acc: 1.000000\n",
      "epoch 0, iter 1886, loss: 0.419982, acc: 1.000000\n",
      "epoch 0, iter 1887, loss: 0.318640, acc: 1.000000\n",
      "epoch 0, iter 1888, loss: 0.279274, acc: 1.000000\n",
      "epoch 0, iter 1889, loss: 0.690108, acc: 0.947368\n",
      "epoch 0, iter 1890, loss: 1.454635, acc: 0.947368\n",
      "epoch 0, iter 1891, loss: 1.044280, acc: 1.000000\n",
      "epoch 0, iter 1892, loss: 0.761874, acc: 1.000000\n",
      "epoch 0, iter 1893, loss: 0.727730, acc: 1.000000\n",
      "epoch 0, iter 1894, loss: 0.588556, acc: 1.000000\n",
      "epoch 0, iter 1895, loss: 0.436152, acc: 1.000000\n",
      "epoch 0, iter 1896, loss: 0.334232, acc: 1.000000\n",
      "epoch 0, iter 1897, loss: 0.338057, acc: 1.000000\n",
      "epoch 0, iter 1898, loss: 0.297160, acc: 1.000000\n",
      "epoch 0, iter 1899, loss: 0.232478, acc: 1.000000\n",
      "epoch 0, iter 1900, loss: 0.191383, acc: 1.000000\n",
      "epoch 0, iter 1901, loss: 0.207839, acc: 1.000000\n",
      "epoch 0, iter 1902, loss: 0.196551, acc: 1.000000\n",
      "epoch 0, iter 1903, loss: 0.163390, acc: 1.000000\n",
      "epoch 0, iter 1904, loss: 0.143411, acc: 1.000000\n",
      "epoch 0, iter 1905, loss: 0.159566, acc: 1.000000\n",
      "epoch 0, iter 1906, loss: 0.803236, acc: 0.947368\n",
      "epoch 0, iter 1907, loss: 0.595701, acc: 1.000000\n",
      "epoch 0, iter 1908, loss: 0.443808, acc: 1.000000\n",
      "epoch 0, iter 1909, loss: 0.362872, acc: 1.000000\n",
      "epoch 0, iter 1910, loss: 0.389896, acc: 1.000000\n",
      "epoch 0, iter 1911, loss: 0.304664, acc: 1.000000\n",
      "epoch 0, iter 1912, loss: 0.237417, acc: 1.000000\n",
      "epoch 0, iter 1913, loss: 0.212840, acc: 1.000000\n",
      "epoch 0, iter 1914, loss: 0.229500, acc: 1.000000\n",
      "epoch 0, iter 1915, loss: 0.191826, acc: 1.000000\n",
      "epoch 0, iter 1916, loss: 0.158659, acc: 1.000000\n",
      "epoch 0, iter 1917, loss: 0.153907, acc: 1.000000\n",
      "epoch 0, iter 1918, loss: 0.168153, acc: 1.000000\n",
      "epoch 0, iter 1919, loss: 0.148781, acc: 1.000000\n",
      "epoch 0, iter 1920, loss: 0.129586, acc: 1.000000\n",
      "epoch 0, iter 1921, loss: 0.130828, acc: 1.000000\n",
      "epoch 0, iter 1922, loss: 0.141795, acc: 1.000000\n",
      "epoch 0, iter 1923, loss: 0.130367, acc: 1.000000\n",
      "epoch 0, iter 1924, loss: 0.117843, acc: 1.000000\n",
      "epoch 0, iter 1925, loss: 0.120520, acc: 1.000000\n",
      "epoch 0, iter 1926, loss: 0.128544, acc: 1.000000\n",
      "epoch 0, iter 1927, loss: 0.121136, acc: 1.000000\n",
      "epoch 0, iter 1928, loss: 0.112375, acc: 1.000000\n",
      "epoch 0, iter 1929, loss: 0.115037, acc: 1.000000\n",
      "epoch 0, iter 1930, loss: 0.120792, acc: 1.000000\n",
      "epoch 0, iter 1931, loss: 0.115730, acc: 1.000000\n",
      "epoch 0, iter 1932, loss: 0.963675, acc: 0.947368\n",
      "epoch 0, iter 1933, loss: 0.715272, acc: 1.000000\n",
      "epoch 0, iter 1934, loss: 0.525819, acc: 1.000000\n",
      "epoch 0, iter 1935, loss: 0.401340, acc: 1.000000\n",
      "epoch 0, iter 1936, loss: 0.359961, acc: 1.000000\n",
      "epoch 0, iter 1937, loss: 0.289094, acc: 1.000000\n",
      "epoch 0, iter 1938, loss: 0.226377, acc: 1.000000\n",
      "epoch 0, iter 1939, loss: 0.190720, acc: 1.000000\n",
      "epoch 0, iter 1940, loss: 0.192210, acc: 1.000000\n",
      "epoch 0, iter 1941, loss: 0.169619, acc: 1.000000\n",
      "epoch 0, iter 1942, loss: 0.143383, acc: 1.000000\n",
      "epoch 0, iter 1943, loss: 0.132140, acc: 1.000000\n",
      "epoch 0, iter 1944, loss: 0.748411, acc: 0.947368\n",
      "epoch 0, iter 1945, loss: 0.566023, acc: 1.000000\n",
      "epoch 0, iter 1946, loss: 0.424520, acc: 1.000000\n",
      "epoch 0, iter 1947, loss: 1.137947, acc: 0.947368\n",
      "epoch 0, iter 1948, loss: 0.933558, acc: 1.000000\n",
      "epoch 0, iter 1949, loss: 0.679119, acc: 1.000000\n",
      "epoch 0, iter 1950, loss: 0.500303, acc: 1.000000\n",
      "epoch 0, iter 1951, loss: 0.442668, acc: 1.000000\n",
      "epoch 0, iter 1952, loss: 0.393225, acc: 1.000000\n",
      "epoch 0, iter 1953, loss: 0.299118, acc: 1.000000\n",
      "epoch 0, iter 1954, loss: 1.270670, acc: 0.947368\n",
      "epoch 0, iter 1955, loss: 0.961117, acc: 1.000000\n",
      "epoch 0, iter 1956, loss: 1.632141, acc: 0.947368\n",
      "epoch 0, iter 1957, loss: 1.167365, acc: 1.000000\n",
      "epoch 0, iter 1958, loss: 1.795289, acc: 0.947368\n",
      "epoch 0, iter 1959, loss: 1.844140, acc: 0.947368\n",
      "epoch 0, iter 1960, loss: 1.339104, acc: 1.000000\n",
      "epoch 0, iter 1961, loss: 2.321201, acc: 0.947368\n",
      "epoch 0, iter 1962, loss: 1.720008, acc: 1.000000\n",
      "epoch 0, iter 1963, loss: 1.264551, acc: 1.000000\n",
      "epoch 0, iter 1964, loss: 0.926736, acc: 1.000000\n",
      "epoch 0, iter 1965, loss: 0.683679, acc: 1.000000\n",
      "epoch 0, iter 1966, loss: 0.532368, acc: 1.000000\n",
      "epoch 0, iter 1967, loss: 0.422101, acc: 1.000000\n",
      "epoch 0, iter 1968, loss: 0.335533, acc: 1.000000\n",
      "epoch 0, iter 1969, loss: 0.269028, acc: 1.000000\n",
      "epoch 0, iter 1970, loss: 0.235075, acc: 1.000000\n",
      "epoch 0, iter 1971, loss: 0.208287, acc: 1.000000\n",
      "epoch 0, iter 1972, loss: 0.184858, acc: 1.000000\n",
      "epoch 0, iter 1973, loss: 0.955468, acc: 0.947368\n",
      "epoch 0, iter 1974, loss: 0.716823, acc: 1.000000\n",
      "epoch 0, iter 1975, loss: 1.582281, acc: 0.947368\n",
      "epoch 0, iter 1976, loss: 1.867943, acc: 0.947368\n",
      "epoch 0, iter 1977, loss: 1.365122, acc: 1.000000\n",
      "epoch 0, iter 1978, loss: 0.983727, acc: 1.000000\n",
      "epoch 0, iter 1979, loss: 0.758512, acc: 1.000000\n",
      "epoch 0, iter 1980, loss: 0.722354, acc: 1.000000\n",
      "epoch 0, iter 1981, loss: 0.546595, acc: 1.000000\n",
      "epoch 0, iter 1982, loss: 0.410938, acc: 1.000000\n",
      "epoch 0, iter 1983, loss: 0.337687, acc: 1.000000\n",
      "epoch 0, iter 1984, loss: 0.299308, acc: 1.000000\n",
      "epoch 0, iter 1985, loss: 0.248532, acc: 1.000000\n",
      "epoch 0, iter 1986, loss: 0.203838, acc: 1.000000\n",
      "epoch 0, iter 1987, loss: 0.187706, acc: 1.000000\n",
      "epoch 0, iter 1988, loss: 0.184230, acc: 1.000000\n",
      "epoch 0, iter 1989, loss: 0.166945, acc: 1.000000\n",
      "epoch 0, iter 1990, loss: 0.147866, acc: 1.000000\n",
      "epoch 0, iter 1991, loss: 0.145314, acc: 1.000000\n",
      "epoch 0, iter 1992, loss: 0.148756, acc: 1.000000\n",
      "epoch 0, iter 1993, loss: 0.141224, acc: 1.000000\n",
      "epoch 0, iter 1994, loss: 0.130625, acc: 1.000000\n",
      "epoch 0, iter 1995, loss: 0.131035, acc: 1.000000\n",
      "epoch 0, iter 1996, loss: 0.135035, acc: 1.000000\n",
      "epoch 0, iter 1997, loss: 0.130800, acc: 1.000000\n",
      "epoch 0, iter 1998, loss: 0.908231, acc: 0.947368\n",
      "epoch 0, iter 1999, loss: 0.679339, acc: 1.000000\n",
      "epoch 0, acc: 0.986395\n"
     ]
    }
   ],
   "source": [
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/num_problems)*num_timesteps # loss at iteration 0\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "for e in xrange(epochs):\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    total_acc = 0.0\n",
    "    for i in xrange(num_train):\n",
    "        inputs = X_train[i,:,:].reshape((num_timesteps, num_problems * 2))\n",
    "        targets = y_train[i,:].reshape((num_timesteps,))\n",
    "        correctness_for_student = correctness[i,:].reshape((num_problems))\n",
    "\n",
    "        # forward num_timesteps characters through the net and fetch gradient\n",
    "        loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, correctness_for_student, hprev)\n",
    "        smooth_loss = smooth_loss * 0.7 + loss * 0.3\n",
    "        losses.append(smooth_loss)\n",
    "\n",
    "        # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                      [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                      [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "        ps = forward_pass(inputs)\n",
    "        acc = accuracy(ps, targets, correctness_for_student) \n",
    "        accuracies.append(acc)\n",
    "        print ('epoch %d, iter %d, loss: %f, acc: %f' % (e, i, smooth_loss, acc)) # print progress\n",
    "        total_acc += acc\n",
    "    total_acc /= num_train\n",
    "    print ('epoch %d, acc: %f' % (e, total_acc)) # print progress\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAEPCAYAAAAJYmAlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npfIvGAtc6EmboX8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b887890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "smooth_accs = []\n",
    "\n",
    "smooth_window = 100\n",
    "for i in xrange(len(accuracies)-smooth_window):\n",
    "    smooth_accs.append(np.mean(accuracies[i:i+smooth_window]))\n",
    "\n",
    "for i in xrange(len(accuracies)-smooth_window, len(accuracies)):\n",
    "    smooth_accs.append(np.mean(accuracies[i:len(accuracies)]))\n",
    "                    \n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(xrange(len(smooth_accs)/2), smooth_accs[:len(smooth_accs)/2], 'b-')\n",
    "ax1.set_ylabel('accuracies', color='b')\n",
    "ax1.set_xlabel('iterations')\n",
    "for tl in ax1.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(xrange(len(losses)/2), losses[:len(losses)/2], 'r-')\n",
    "ax2.set_ylabel('losses', color='r')\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure_filename = \"loss_plots/test.png\"\n",
    "fig.savefig(figure_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
