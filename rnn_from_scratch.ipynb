{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImplements an RNN on a synthetic data set, following the architecture \\ndescribed in \"Deep Knowledge Tracing\" by Chris Piech et al.\\nThe RNN implementation is based on min-char-rnn.py by Andrej Karpathy (@karpathy).\\nBSD License\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implements an RNN on a synthetic data set, following the architecture \n",
    "described in \"Deep Knowledge Tracing\" by Chris Piech et al.\n",
    "The RNN implementation is based on min-char-rnn.py by Andrej Karpathy (@karpathy).\n",
    "BSD License\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "synthetic_data_set = \"syntheticDetailed/naive_c5_q50_s4000_v0.csv\"\n",
    "code_org_data_set = \"data/hoc_1-9_binary_input.csv\"\n",
    "\n",
    "DATA_SET = code_org_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Vectorization done!\n",
      "X_train\n",
      "(2000, 8, 18)\n",
      "[[[ True False False False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False False False False False False  True False\n",
      "   False False False False False False]\n",
      "  [False False  True False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False  True False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False  True False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False False False False False False False False\n",
      "   False False  True False False False]\n",
      "  [False False False False False False  True False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False False False  True False False False False\n",
      "   False False False False False False]]\n",
      "\n",
      " [[ True False False False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False  True False False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False  True False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False  True False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False  True False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False  True False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False False  True False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False False False  True False False False False\n",
      "   False False False False False False]]\n",
      "\n",
      " [[ True False False False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False  True False False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False  True False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False  True False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False  True False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False False False False False False False False\n",
      "   False False  True False False False]\n",
      "  [False False False False False False False False False False False False\n",
      "   False False False  True False False]\n",
      "  [False False False False False False False False False False False False\n",
      "   False False False False  True False]]\n",
      "\n",
      " [[ True False False False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False  True False False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False  True False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False  True False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False  True False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False  True False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False False  True False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False False False  True False False False False\n",
      "   False False False False False False]]\n",
      "\n",
      " [[ True False False False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False  True False False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False  True False False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False  True False False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False  True False False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False  True False False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False False  True False False False False False\n",
      "   False False False False False False]\n",
      "  [False False False False False False False False False False False False\n",
      "   False False False False  True False]]]\n",
      "y_train\n",
      "(2000, 8)\n",
      "[[1 2 3 4 5 6 7 8]\n",
      " [1 2 3 4 5 6 7 8]\n",
      " [1 2 3 4 5 6 7 8]\n",
      " [1 2 3 4 5 6 7 8]\n",
      " [1 2 3 4 5 6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "# Read in the data set\n",
    "# This function can be moved to utils.py\n",
    "data_array = np.array(list(csv.reader(open(DATA_SET,\"rb\"),delimiter=','))).astype('int')\n",
    "num_samples = data_array.shape[0]\n",
    "num_problems = data_array.shape[1]\n",
    "\n",
    "# time steps is number of problems - 1 because we cannot predict on the last problem.\n",
    "num_timesteps = num_problems - 1 \n",
    "# Split data into train and test (half and half)\n",
    "num_samples = 4000  # Note: for code.org sample, enforcing limit on num_students as a trial\n",
    "train = data_array[0:num_samples/2,:]\n",
    "test = data_array[num_samples/2:num_samples,:]\n",
    "\n",
    "num_train = train.shape[0]\n",
    "num_test = test.shape[0]\n",
    "\n",
    "\n",
    "print('Vectorization...')\n",
    "X_train = np.zeros((num_train, num_timesteps, num_problems * 2), dtype=np.bool)\n",
    "y_train = np.zeros((num_train, num_timesteps), dtype=np.int)\n",
    "\n",
    "# Create 3-dimensional input tensor with one-hot encodings for each sample\n",
    "# the dimension of each vector for a student i and time t is 2 * num_problems\n",
    "# where the first half corresponds to the correctly answered problems and the\n",
    "# second half to the incorrectly answered ones.\n",
    "for i in xrange(num_train):\n",
    "    \n",
    "    # for the first time step. Done separately so we can populate the output \n",
    "    # tensor at the same time, which is shifted back by 1.\n",
    "\n",
    "    for t in xrange(0,num_timesteps):\n",
    "        p = t # since timestep t corresponds to problem p where t=p\n",
    "        if train[i,p] == 1:\n",
    "            X_train[i, t, p] = 1 \n",
    "        else:\n",
    "            X_train[i, t, num_problems + p] = 1\n",
    "        # this is a special case for the synthetic data set, where the next problem \n",
    "        # is just the current problem index + 1\n",
    "        y_train[i,t] = p + 1\n",
    "correctness = train\n",
    "\n",
    "print (\"Vectorization done!\")\n",
    "\n",
    "print (\"X_train\")\n",
    "print (X_train.shape)\n",
    "print (X_train[:5])\n",
    "\n",
    "print (\"y_train\")\n",
    "print (y_train.shape)\n",
    "print (y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 200 # size of hidden layer of neurons\n",
    "learning_rate = 1e-1\n",
    "epochs = 10\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, num_problems * 2)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(num_problems, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((num_problems, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, correctness, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps, ps_denom = {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = inputs[t,:].reshape((num_problems * 2, 1))\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps_denom[t] = np.sum(np.exp(ys[t]))\n",
    "        ps[t] = np.exp(ys[t]) / ps_denom[t] # probabilities for next chars\n",
    "\n",
    "        # softmax (cross-entropy loss)\n",
    "        if correctness[targets[t]] == 1:\n",
    "            loss += -np.log(ps[t][targets[t],0]) \n",
    "        else:\n",
    "            loss += -np.log(1-ps[t][targets[t],0]) \n",
    "        # backward pass: compute gradients going backwards\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "        dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        if correctness[targets[t]] == 1:\n",
    "            dy[targets[t]] -= 1 # backprop into y\n",
    "        else:\n",
    "            for p in xrange(num_problems):\n",
    "                if p != targets[t]:\n",
    "                    dy[p] -= np.exp(ys[t][p]) / (ps_denom[t] - np.exp(ys[t][targets[t]]))\n",
    "\n",
    "\n",
    "\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(ps, targets, correctness):\n",
    "    \"\"\"\n",
    "    Computes the accuracy using the predictions at each time step.\n",
    "    For each t, if probability of next problem is > 0.5 for correct, or <= 0.5 \n",
    "    for incorrect, then count this as correct prediction.\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    for t in xrange(num_timesteps):\n",
    "        predicted_prob = ps[t][targets[t],0] \n",
    "        if (predicted_prob >= 0.5 and correctness[targets[t]] == 1) or (predicted_prob < 0.5 and correctness[targets[t]] == 0):\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / float(num_timesteps)\n",
    "    return accuracy\n",
    "\n",
    "def forward_pass(inputs):\n",
    "    xs, hs, ys, ps, ps_denom = {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = inputs[t,:].reshape((num_problems * 2, 1))\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps_denom[t] = np.sum(np.exp(ys[t]))\n",
    "        ps[t] = np.exp(ys[t]) / ps_denom[t] # probabilities for next chars\n",
    "    return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, iter 0, loss: 15.706005, acc: 0.375000\n",
      "epoch 0, iter 1, loss: 16.007323, acc: 0.000000\n",
      "epoch 0, iter 2, loss: 16.957829, acc: 0.875000\n",
      "epoch 0, iter 3, loss: 28.401902, acc: 0.250000\n",
      "epoch 0, iter 4, loss: 48.779271, acc: 0.375000\n",
      "epoch 0, iter 5, loss: 50.658509, acc: 0.500000\n",
      "epoch 0, iter 6, loss: 41.632960, acc: 0.750000\n",
      "epoch 0, iter 7, loss: 50.545449, acc: 0.375000\n",
      "epoch 0, iter 8, loss: 48.272435, acc: 0.500000\n",
      "epoch 0, iter 9, loss: 50.678742, acc: 0.375000\n",
      "epoch 0, iter 10, loss: 45.698031, acc: 0.375000\n",
      "epoch 0, iter 11, loss: 50.331421, acc: 0.250000\n",
      "epoch 0, iter 12, loss: 55.136425, acc: 0.250000\n",
      "epoch 0, iter 13, loss: 53.503641, acc: 0.250000\n",
      "epoch 0, iter 14, loss: 38.870041, acc: 0.750000\n",
      "epoch 0, iter 15, loss: 34.029627, acc: 0.625000\n",
      "epoch 0, iter 16, loss: 32.670992, acc: 0.250000\n",
      "epoch 0, iter 17, loss: 23.418370, acc: 0.875000\n",
      "epoch 0, iter 18, loss: 24.646263, acc: 0.250000\n",
      "epoch 0, iter 19, loss: 26.017179, acc: 0.375000\n",
      "epoch 0, iter 20, loss: 25.456763, acc: 0.375000\n",
      "epoch 0, iter 21, loss: 22.745586, acc: 0.375000\n",
      "epoch 0, iter 22, loss: 24.731153, acc: 0.250000\n",
      "epoch 0, iter 23, loss: 24.539620, acc: 0.375000\n",
      "epoch 0, iter 24, loss: 24.160514, acc: 0.125000\n",
      "epoch 0, iter 25, loss: 24.523151, acc: 0.250000\n",
      "epoch 0, iter 26, loss: 23.459560, acc: 0.250000\n",
      "epoch 0, iter 27, loss: 22.846523, acc: 0.250000\n",
      "epoch 0, iter 28, loss: 21.535770, acc: 0.125000\n",
      "epoch 0, iter 29, loss: 20.619323, acc: 0.125000\n",
      "epoch 0, iter 30, loss: 18.766717, acc: 0.250000\n",
      "epoch 0, iter 31, loss: 16.698152, acc: 0.375000\n",
      "epoch 0, iter 32, loss: 17.996021, acc: 0.125000\n",
      "epoch 0, iter 33, loss: 17.154665, acc: 0.000000\n",
      "epoch 0, iter 34, loss: 17.160522, acc: 0.125000\n",
      "epoch 0, iter 35, loss: 16.409157, acc: 0.125000\n",
      "epoch 0, iter 36, loss: 16.475608, acc: 0.125000\n",
      "epoch 0, iter 37, loss: 15.425263, acc: 0.375000\n",
      "epoch 0, iter 38, loss: 14.658363, acc: 0.250000\n",
      "epoch 0, iter 39, loss: 12.438998, acc: 0.500000\n",
      "epoch 0, iter 40, loss: 12.568911, acc: 0.250000\n",
      "epoch 0, iter 41, loss: 12.456579, acc: 0.375000\n",
      "epoch 0, iter 42, loss: 13.908555, acc: 0.125000\n",
      "epoch 0, iter 43, loss: 14.257696, acc: 0.375000\n",
      "epoch 0, iter 44, loss: 14.193433, acc: 0.250000\n",
      "epoch 0, iter 45, loss: 14.693354, acc: 0.375000\n",
      "epoch 0, iter 46, loss: 15.587453, acc: 0.000000\n",
      "epoch 0, iter 47, loss: 15.572740, acc: 0.750000\n",
      "epoch 0, iter 48, loss: 16.404683, acc: 0.250000\n",
      "epoch 0, iter 49, loss: 16.943103, acc: 0.250000\n",
      "epoch 0, iter 50, loss: 15.971043, acc: 0.125000\n",
      "epoch 0, iter 51, loss: 16.510570, acc: 0.125000\n",
      "epoch 0, iter 52, loss: 14.082067, acc: 0.500000\n",
      "epoch 0, iter 53, loss: 15.237615, acc: 0.125000\n",
      "epoch 0, iter 54, loss: 14.743158, acc: 0.000000\n",
      "epoch 0, iter 55, loss: 13.359271, acc: 0.375000\n",
      "epoch 0, iter 56, loss: 13.257238, acc: 0.250000\n",
      "epoch 0, iter 57, loss: 14.627069, acc: 0.125000\n",
      "epoch 0, iter 58, loss: 14.375984, acc: 0.000000\n",
      "epoch 0, iter 59, loss: 14.025968, acc: 0.000000\n",
      "epoch 0, iter 60, loss: 13.589310, acc: 0.250000\n",
      "epoch 0, iter 61, loss: 14.446632, acc: 0.125000\n",
      "epoch 0, iter 62, loss: 14.155617, acc: 0.000000\n",
      "epoch 0, iter 63, loss: 13.942565, acc: 0.125000\n",
      "epoch 0, iter 64, loss: 11.217170, acc: 1.000000\n",
      "epoch 0, iter 65, loss: 15.039346, acc: 0.000000\n",
      "epoch 0, iter 66, loss: 16.121796, acc: 0.125000\n",
      "epoch 0, iter 67, loss: 15.163132, acc: 0.250000\n",
      "epoch 0, iter 68, loss: 14.939051, acc: 0.250000\n",
      "epoch 0, iter 69, loss: 14.720722, acc: 0.000000\n",
      "epoch 0, iter 70, loss: 14.396202, acc: 0.125000\n",
      "epoch 0, iter 71, loss: 13.737164, acc: 0.125000\n",
      "epoch 0, iter 72, loss: 11.935783, acc: 0.375000\n",
      "epoch 0, iter 73, loss: 11.447634, acc: 0.250000\n",
      "epoch 0, iter 74, loss: 12.400812, acc: 0.125000\n",
      "epoch 0, iter 75, loss: 12.347605, acc: 0.250000\n",
      "epoch 0, iter 76, loss: 11.258519, acc: 0.500000\n",
      "epoch 0, iter 77, loss: 10.776803, acc: 0.500000\n",
      "epoch 0, iter 78, loss: 11.129934, acc: 0.250000\n",
      "epoch 0, iter 79, loss: 12.221676, acc: 0.125000\n",
      "epoch 0, iter 80, loss: 12.329251, acc: 0.250000\n",
      "epoch 0, iter 81, loss: 14.466103, acc: 0.250000\n",
      "epoch 0, iter 82, loss: 14.618069, acc: 0.000000\n",
      "epoch 0, iter 83, loss: 13.748009, acc: 0.750000\n",
      "epoch 0, iter 84, loss: 15.337038, acc: 0.000000\n",
      "epoch 0, iter 85, loss: 15.699628, acc: 0.250000\n",
      "epoch 0, iter 86, loss: 15.303997, acc: 0.125000\n",
      "epoch 0, iter 87, loss: 15.261112, acc: 0.250000\n",
      "epoch 0, iter 88, loss: 11.983670, acc: 0.750000\n",
      "epoch 0, iter 89, loss: 13.862280, acc: 0.125000\n",
      "epoch 0, iter 90, loss: 13.649704, acc: 0.000000\n",
      "epoch 0, iter 91, loss: 13.351503, acc: 0.250000\n",
      "epoch 0, iter 92, loss: 11.214464, acc: 0.625000\n",
      "epoch 0, iter 93, loss: 11.369504, acc: 0.125000\n",
      "epoch 0, iter 94, loss: 12.529383, acc: 0.000000\n",
      "epoch 0, iter 95, loss: 11.879724, acc: 0.125000\n",
      "epoch 0, iter 96, loss: 12.075077, acc: 0.000000\n",
      "epoch 0, iter 97, loss: 11.886137, acc: 0.000000\n",
      "epoch 0, iter 98, loss: 10.526659, acc: 0.500000\n",
      "epoch 0, iter 99, loss: 11.027620, acc: 0.250000\n",
      "epoch 0, iter 100, loss: 11.871659, acc: 0.000000\n",
      "epoch 0, iter 101, loss: 11.738428, acc: 0.250000\n",
      "epoch 0, iter 102, loss: 12.877949, acc: 0.250000\n",
      "epoch 0, iter 103, loss: 13.259853, acc: 0.125000\n",
      "epoch 0, iter 104, loss: 12.918351, acc: 0.125000\n",
      "epoch 0, iter 105, loss: 13.530564, acc: 0.000000\n",
      "epoch 0, iter 106, loss: 12.101450, acc: 0.125000\n",
      "epoch 0, iter 107, loss: 11.675267, acc: 0.125000\n",
      "epoch 0, iter 108, loss: 10.530884, acc: 0.125000\n",
      "epoch 0, iter 109, loss: 11.202359, acc: 0.000000\n",
      "epoch 0, iter 110, loss: 10.442046, acc: 0.375000\n",
      "epoch 0, iter 111, loss: 11.280892, acc: 0.250000\n",
      "epoch 0, iter 112, loss: 12.563848, acc: 0.125000\n",
      "epoch 0, iter 113, loss: 13.329226, acc: 0.125000\n",
      "epoch 0, iter 114, loss: 13.012384, acc: 0.125000\n",
      "epoch 0, iter 115, loss: 13.007587, acc: 0.000000\n",
      "epoch 0, iter 116, loss: 12.466879, acc: 0.125000\n",
      "epoch 0, iter 117, loss: 11.220137, acc: 0.250000\n",
      "epoch 0, iter 118, loss: 10.305608, acc: 0.125000\n",
      "epoch 0, iter 119, loss: 11.209364, acc: 0.125000\n",
      "epoch 0, iter 120, loss: 10.658488, acc: 0.375000\n",
      "epoch 0, iter 121, loss: 9.597814, acc: 0.500000\n",
      "epoch 0, iter 122, loss: 7.896866, acc: 0.875000\n",
      "epoch 0, iter 123, loss: 12.556063, acc: 0.000000\n",
      "epoch 0, iter 124, loss: 11.220361, acc: 0.750000\n",
      "epoch 0, iter 125, loss: 12.475002, acc: 0.250000\n",
      "epoch 0, iter 126, loss: 12.621440, acc: 0.125000\n",
      "epoch 0, iter 127, loss: 12.932764, acc: 0.125000\n",
      "epoch 0, iter 128, loss: 11.090334, acc: 0.500000\n",
      "epoch 0, iter 129, loss: 11.012861, acc: 0.375000\n",
      "epoch 0, iter 130, loss: 12.218741, acc: 0.250000\n",
      "epoch 0, iter 131, loss: 12.509659, acc: 0.250000\n",
      "epoch 0, iter 132, loss: 11.732444, acc: 0.375000\n",
      "epoch 0, iter 133, loss: 10.014234, acc: 0.375000\n",
      "epoch 0, iter 134, loss: 8.103703, acc: 1.000000\n",
      "epoch 0, iter 135, loss: 10.789582, acc: 0.250000\n",
      "epoch 0, iter 136, loss: 8.902769, acc: 0.750000\n",
      "epoch 0, iter 137, loss: 10.758459, acc: 0.125000\n",
      "epoch 0, iter 138, loss: 10.588502, acc: 0.250000\n",
      "epoch 0, iter 139, loss: 10.482027, acc: 0.125000\n",
      "epoch 0, iter 140, loss: 11.483013, acc: 0.125000\n",
      "epoch 0, iter 141, loss: 11.143747, acc: 0.250000\n",
      "epoch 0, iter 142, loss: 11.453138, acc: 0.125000\n",
      "epoch 0, iter 143, loss: 10.714009, acc: 0.125000\n",
      "epoch 0, iter 144, loss: 11.271922, acc: 0.125000\n",
      "epoch 0, iter 145, loss: 11.369960, acc: 0.250000\n",
      "epoch 0, iter 146, loss: 10.973109, acc: 0.250000\n",
      "epoch 0, iter 147, loss: 10.371778, acc: 0.375000\n",
      "epoch 0, iter 148, loss: 9.984127, acc: 0.250000\n",
      "epoch 0, iter 149, loss: 10.271440, acc: 0.250000\n",
      "epoch 0, iter 150, loss: 11.088387, acc: 0.250000\n",
      "epoch 0, iter 151, loss: 11.921298, acc: 0.125000\n",
      "epoch 0, iter 152, loss: 11.858594, acc: 0.500000\n",
      "epoch 0, iter 153, loss: 12.953583, acc: 0.125000\n",
      "epoch 0, iter 154, loss: 12.350856, acc: 0.625000\n",
      "epoch 0, iter 155, loss: 11.779241, acc: 0.250000\n",
      "epoch 0, iter 156, loss: 10.918937, acc: 0.375000\n",
      "epoch 0, iter 157, loss: 10.725975, acc: 0.625000\n",
      "epoch 0, iter 158, loss: 12.963221, acc: 0.250000\n",
      "epoch 0, iter 159, loss: 12.190672, acc: 0.125000\n",
      "epoch 0, iter 160, loss: 12.270383, acc: 0.125000\n",
      "epoch 0, iter 161, loss: 10.595628, acc: 0.625000\n",
      "epoch 0, iter 162, loss: 10.283489, acc: 0.250000\n",
      "epoch 0, iter 163, loss: 10.500773, acc: 0.250000\n",
      "epoch 0, iter 164, loss: 9.650226, acc: 0.125000\n",
      "epoch 0, iter 165, loss: 9.966674, acc: 0.250000\n",
      "epoch 0, iter 166, loss: 9.369975, acc: 0.250000\n",
      "epoch 0, iter 167, loss: 9.484378, acc: 0.375000\n",
      "epoch 0, iter 168, loss: 8.998636, acc: 0.500000\n",
      "epoch 0, iter 169, loss: 10.785287, acc: 0.250000\n",
      "epoch 0, iter 170, loss: 10.225516, acc: 0.125000\n",
      "epoch 0, iter 171, loss: 9.486780, acc: 0.250000\n",
      "epoch 0, iter 172, loss: 8.621478, acc: 0.500000\n",
      "epoch 0, iter 173, loss: 7.800592, acc: 0.500000\n",
      "epoch 0, iter 174, loss: 7.430727, acc: 0.500000\n",
      "epoch 0, iter 175, loss: 10.223552, acc: 0.250000\n",
      "epoch 0, iter 176, loss: 9.331136, acc: 0.500000\n",
      "epoch 0, iter 177, loss: 9.588487, acc: 0.375000\n",
      "epoch 0, iter 178, loss: 9.266856, acc: 0.250000\n",
      "epoch 0, iter 179, loss: 10.048789, acc: 0.250000\n",
      "epoch 0, iter 180, loss: 10.501815, acc: 0.500000\n",
      "epoch 0, iter 181, loss: 10.634611, acc: 0.375000\n",
      "epoch 0, iter 182, loss: 10.369186, acc: 0.250000\n",
      "epoch 0, iter 183, loss: 9.852097, acc: 0.500000\n",
      "epoch 0, iter 184, loss: 7.740907, acc: 0.875000\n",
      "epoch 0, iter 185, loss: 8.851003, acc: 0.375000\n",
      "epoch 0, iter 186, loss: 9.031061, acc: 0.250000\n",
      "epoch 0, iter 187, loss: 8.890911, acc: 0.625000\n",
      "epoch 0, iter 188, loss: 7.047396, acc: 1.000000\n",
      "epoch 0, iter 189, loss: 10.047712, acc: 0.250000\n",
      "epoch 0, iter 190, loss: 10.330257, acc: 0.250000\n",
      "epoch 0, iter 191, loss: 9.750742, acc: 0.500000\n",
      "epoch 0, iter 192, loss: 10.969847, acc: 0.625000\n",
      "epoch 0, iter 193, loss: 11.473287, acc: 0.500000\n",
      "epoch 0, iter 194, loss: 10.469557, acc: 0.500000\n",
      "epoch 0, iter 195, loss: 9.882372, acc: 0.375000\n",
      "epoch 0, iter 196, loss: 10.116547, acc: 0.000000\n",
      "epoch 0, iter 197, loss: 9.244703, acc: 0.500000\n",
      "epoch 0, iter 198, loss: 7.981135, acc: 0.500000\n",
      "epoch 0, iter 199, loss: 9.255450, acc: 0.250000\n",
      "epoch 0, iter 200, loss: 8.358043, acc: 0.500000\n",
      "epoch 0, iter 201, loss: 9.246292, acc: 0.250000\n",
      "epoch 0, iter 202, loss: 9.202112, acc: 0.000000\n",
      "epoch 0, iter 203, loss: 7.096155, acc: 1.000000\n",
      "epoch 0, iter 204, loss: 7.359236, acc: 0.250000\n",
      "epoch 0, iter 205, loss: 6.924038, acc: 0.500000\n",
      "epoch 0, iter 206, loss: 8.403259, acc: 0.250000\n",
      "epoch 0, iter 207, loss: 6.937116, acc: 1.000000\n",
      "epoch 0, iter 208, loss: 5.853435, acc: 1.000000\n",
      "epoch 0, iter 209, loss: 8.548350, acc: 0.250000\n",
      "epoch 0, iter 210, loss: 8.807666, acc: 0.625000\n",
      "epoch 0, iter 211, loss: 9.773400, acc: 0.125000\n",
      "epoch 0, iter 212, loss: 7.520976, acc: 1.000000\n",
      "epoch 0, iter 213, loss: 7.649099, acc: 0.250000\n",
      "epoch 0, iter 214, loss: 7.147701, acc: 0.500000\n",
      "epoch 0, iter 215, loss: 8.524111, acc: 0.250000\n",
      "epoch 0, iter 216, loss: 9.077106, acc: 0.125000\n",
      "epoch 0, iter 217, loss: 9.518835, acc: 0.375000\n",
      "epoch 0, iter 218, loss: 10.071712, acc: 0.125000\n",
      "epoch 0, iter 219, loss: 10.101728, acc: 0.125000\n",
      "epoch 0, iter 220, loss: 9.761291, acc: 0.250000\n",
      "epoch 0, iter 221, loss: 10.864591, acc: 0.000000\n",
      "epoch 0, iter 222, loss: 10.365538, acc: 0.250000\n",
      "epoch 0, iter 223, loss: 9.586006, acc: 0.250000\n",
      "epoch 0, iter 224, loss: 9.478471, acc: 0.375000\n",
      "epoch 0, iter 225, loss: 7.248883, acc: 1.000000\n",
      "epoch 0, iter 226, loss: 6.458955, acc: 1.000000\n",
      "epoch 0, iter 227, loss: 6.769491, acc: 0.500000\n",
      "epoch 0, iter 228, loss: 5.355603, acc: 1.000000\n",
      "epoch 0, iter 229, loss: 6.025347, acc: 0.250000\n",
      "epoch 0, iter 230, loss: 5.904919, acc: 0.625000\n",
      "epoch 0, iter 231, loss: 5.515947, acc: 0.500000\n",
      "epoch 0, iter 232, loss: 6.263401, acc: 0.375000\n",
      "epoch 0, iter 233, loss: 7.701832, acc: 0.125000\n",
      "epoch 0, iter 234, loss: 7.473095, acc: 0.375000\n",
      "epoch 0, iter 235, loss: 8.156982, acc: 0.125000\n",
      "epoch 0, iter 236, loss: 8.351281, acc: 0.125000\n",
      "epoch 0, iter 237, loss: 8.321692, acc: 0.250000\n",
      "epoch 0, iter 238, loss: 7.984904, acc: 0.500000\n",
      "epoch 0, iter 239, loss: 8.857284, acc: 0.375000\n",
      "epoch 0, iter 240, loss: 10.090673, acc: 0.250000\n",
      "epoch 0, iter 241, loss: 11.706677, acc: 0.125000\n",
      "epoch 0, iter 242, loss: 11.390201, acc: 0.125000\n",
      "epoch 0, iter 243, loss: 10.281878, acc: 0.250000\n",
      "epoch 0, iter 244, loss: 9.237023, acc: 0.375000\n",
      "epoch 0, iter 245, loss: 8.468085, acc: 0.375000\n",
      "epoch 0, iter 246, loss: 7.891375, acc: 0.375000\n",
      "epoch 0, iter 247, loss: 8.609643, acc: 0.500000\n",
      "epoch 0, iter 248, loss: 8.580167, acc: 0.375000\n",
      "epoch 0, iter 249, loss: 8.536296, acc: 0.250000\n",
      "epoch 0, iter 250, loss: 8.651656, acc: 0.375000\n",
      "epoch 0, iter 251, loss: 8.612069, acc: 0.375000\n",
      "epoch 0, iter 252, loss: 8.226821, acc: 0.625000\n",
      "epoch 0, iter 253, loss: 9.192345, acc: 0.250000\n",
      "epoch 0, iter 254, loss: 8.291130, acc: 0.625000\n",
      "epoch 0, iter 255, loss: 8.092872, acc: 0.375000\n",
      "epoch 0, iter 256, loss: 7.600386, acc: 0.375000\n",
      "epoch 0, iter 257, loss: 7.235780, acc: 0.375000\n",
      "epoch 0, iter 258, loss: 6.970451, acc: 0.375000\n",
      "epoch 0, iter 259, loss: 7.973675, acc: 0.250000\n",
      "epoch 0, iter 260, loss: 7.884722, acc: 0.250000\n",
      "epoch 0, iter 261, loss: 7.453310, acc: 0.375000\n",
      "epoch 0, iter 262, loss: 6.778057, acc: 0.500000\n",
      "epoch 0, iter 263, loss: 8.079548, acc: 0.250000\n",
      "epoch 0, iter 264, loss: 7.358141, acc: 0.500000\n",
      "epoch 0, iter 265, loss: 7.404639, acc: 0.375000\n",
      "epoch 0, iter 266, loss: 8.090457, acc: 0.125000\n",
      "epoch 0, iter 267, loss: 7.956984, acc: 0.375000\n",
      "epoch 0, iter 268, loss: 7.195006, acc: 0.750000\n",
      "epoch 0, iter 269, loss: 8.928114, acc: 0.375000\n",
      "epoch 0, iter 270, loss: 8.953548, acc: 0.125000\n",
      "epoch 0, iter 271, loss: 8.789984, acc: 0.250000\n",
      "epoch 0, iter 272, loss: 7.830688, acc: 0.625000\n",
      "epoch 0, iter 273, loss: 9.384953, acc: 0.375000\n",
      "epoch 0, iter 274, loss: 9.181236, acc: 0.250000\n",
      "epoch 0, iter 275, loss: 8.474725, acc: 0.375000\n",
      "epoch 0, iter 276, loss: 8.424259, acc: 0.375000\n",
      "epoch 0, iter 277, loss: 9.281221, acc: 0.125000\n",
      "epoch 0, iter 278, loss: 9.275952, acc: 0.500000\n",
      "epoch 0, iter 279, loss: 9.828928, acc: 0.375000\n",
      "epoch 0, iter 280, loss: 10.273330, acc: 0.125000\n",
      "epoch 0, iter 281, loss: 10.134227, acc: 0.125000\n",
      "epoch 0, iter 282, loss: 10.910593, acc: 0.250000\n",
      "epoch 0, iter 283, loss: 11.305798, acc: 0.125000\n",
      "epoch 0, iter 284, loss: 10.031661, acc: 0.500000\n",
      "epoch 0, iter 285, loss: 8.188032, acc: 0.750000\n",
      "epoch 0, iter 286, loss: 9.353458, acc: 0.375000\n",
      "epoch 0, iter 287, loss: 8.765695, acc: 0.375000\n",
      "epoch 0, iter 288, loss: 8.877594, acc: 0.250000\n",
      "epoch 0, iter 289, loss: 8.047720, acc: 0.500000\n",
      "epoch 0, iter 290, loss: 7.838155, acc: 0.375000\n",
      "epoch 0, iter 291, loss: 8.369380, acc: 0.250000\n",
      "epoch 0, iter 292, loss: 7.268807, acc: 0.875000\n",
      "epoch 0, iter 293, loss: 8.731958, acc: 0.250000\n",
      "epoch 0, iter 294, loss: 8.408223, acc: 0.625000\n",
      "epoch 0, iter 295, loss: 9.377384, acc: 0.250000\n",
      "epoch 0, iter 296, loss: 9.069501, acc: 0.250000\n",
      "epoch 0, iter 297, loss: 8.249374, acc: 0.750000\n",
      "epoch 0, iter 298, loss: 8.365162, acc: 0.500000\n",
      "epoch 0, iter 299, loss: 9.131325, acc: 0.500000\n",
      "epoch 0, iter 300, loss: 9.184432, acc: 0.250000\n",
      "epoch 0, iter 301, loss: 8.884387, acc: 0.250000\n",
      "epoch 0, iter 302, loss: 8.097748, acc: 0.500000\n",
      "epoch 0, iter 303, loss: 8.758179, acc: 0.250000\n",
      "epoch 0, iter 304, loss: 8.183638, acc: 0.500000\n",
      "epoch 0, iter 305, loss: 8.423970, acc: 0.250000\n",
      "epoch 0, iter 306, loss: 8.362076, acc: 0.250000\n",
      "epoch 0, iter 307, loss: 8.298773, acc: 0.250000\n",
      "epoch 0, iter 308, loss: 7.946976, acc: 0.375000\n",
      "epoch 0, iter 309, loss: 8.291970, acc: 0.250000\n",
      "epoch 0, iter 310, loss: 7.553754, acc: 0.500000\n",
      "epoch 0, iter 311, loss: 6.581555, acc: 1.000000\n",
      "epoch 0, iter 312, loss: 8.121674, acc: 0.625000\n",
      "epoch 0, iter 313, loss: 8.564172, acc: 0.375000\n",
      "epoch 0, iter 314, loss: 7.132433, acc: 0.750000\n",
      "epoch 0, iter 315, loss: 7.540665, acc: 0.375000\n",
      "epoch 0, iter 316, loss: 7.962155, acc: 0.250000\n",
      "epoch 0, iter 317, loss: 7.965514, acc: 0.250000\n",
      "epoch 0, iter 318, loss: 7.424668, acc: 0.500000\n",
      "epoch 0, iter 319, loss: 6.276705, acc: 1.000000\n",
      "epoch 0, iter 320, loss: 7.541327, acc: 0.375000\n",
      "epoch 0, iter 321, loss: 6.924769, acc: 0.625000\n",
      "epoch 0, iter 322, loss: 6.088021, acc: 0.500000\n",
      "epoch 0, iter 323, loss: 5.457179, acc: 0.500000\n",
      "epoch 0, iter 324, loss: 4.869359, acc: 0.750000\n",
      "epoch 0, iter 325, loss: 7.515566, acc: 0.250000\n",
      "epoch 0, iter 326, loss: 8.041897, acc: 0.250000\n",
      "epoch 0, iter 327, loss: 7.777767, acc: 0.375000\n",
      "epoch 0, iter 328, loss: 8.143083, acc: 0.250000\n",
      "epoch 0, iter 329, loss: 7.686687, acc: 0.375000\n",
      "epoch 0, iter 330, loss: 6.897220, acc: 0.500000\n",
      "epoch 0, iter 331, loss: 7.892994, acc: 0.250000\n",
      "epoch 0, iter 332, loss: 7.492947, acc: 0.375000\n",
      "epoch 0, iter 333, loss: 7.036309, acc: 0.375000\n",
      "epoch 0, iter 334, loss: 6.177399, acc: 0.750000\n",
      "epoch 0, iter 335, loss: 7.684029, acc: 0.250000\n",
      "epoch 0, iter 336, loss: 7.962518, acc: 0.250000\n",
      "epoch 0, iter 337, loss: 7.337852, acc: 0.625000\n",
      "epoch 0, iter 338, loss: 8.649977, acc: 0.375000\n",
      "epoch 0, iter 339, loss: 8.846098, acc: 0.500000\n",
      "epoch 0, iter 340, loss: 7.994540, acc: 0.375000\n",
      "epoch 0, iter 341, loss: 7.351740, acc: 0.500000\n",
      "epoch 0, iter 342, loss: 8.133259, acc: 0.250000\n",
      "epoch 0, iter 343, loss: 8.190860, acc: 0.250000\n",
      "epoch 0, iter 344, loss: 6.326583, acc: 1.000000\n",
      "epoch 0, iter 345, loss: 6.585818, acc: 0.375000\n",
      "epoch 0, iter 346, loss: 5.357414, acc: 1.000000\n",
      "epoch 0, iter 347, loss: 6.916527, acc: 0.500000\n",
      "epoch 0, iter 348, loss: 7.756842, acc: 0.250000\n",
      "epoch 0, iter 349, loss: 7.807293, acc: 0.250000\n",
      "epoch 0, iter 350, loss: 7.795445, acc: 0.250000\n",
      "epoch 0, iter 351, loss: 7.491790, acc: 0.375000\n",
      "epoch 0, iter 352, loss: 6.367324, acc: 1.000000\n",
      "epoch 0, iter 353, loss: 7.012078, acc: 0.500000\n",
      "epoch 0, iter 354, loss: 6.788175, acc: 0.375000\n",
      "epoch 0, iter 355, loss: 7.696810, acc: 0.375000\n",
      "epoch 0, iter 356, loss: 6.229406, acc: 1.000000\n",
      "epoch 0, iter 357, loss: 6.553739, acc: 0.625000\n",
      "epoch 0, iter 358, loss: 7.890268, acc: 0.375000\n",
      "epoch 0, iter 359, loss: 6.561278, acc: 1.000000\n",
      "epoch 0, iter 360, loss: 8.526517, acc: 0.375000\n",
      "epoch 0, iter 361, loss: 9.038394, acc: 0.250000\n",
      "epoch 0, iter 362, loss: 9.445273, acc: 0.375000\n",
      "epoch 0, iter 363, loss: 9.785127, acc: 0.125000\n",
      "epoch 0, iter 364, loss: 9.473987, acc: 0.250000\n",
      "epoch 0, iter 365, loss: 9.844252, acc: 0.250000\n",
      "epoch 0, iter 366, loss: 9.047201, acc: 0.500000\n",
      "epoch 0, iter 367, loss: 8.872259, acc: 0.250000\n",
      "epoch 0, iter 368, loss: 8.697727, acc: 0.500000\n",
      "epoch 0, iter 369, loss: 9.221807, acc: 0.250000\n",
      "epoch 0, iter 370, loss: 8.962537, acc: 0.250000\n",
      "epoch 0, iter 371, loss: 8.626186, acc: 0.250000\n",
      "epoch 0, iter 372, loss: 8.628919, acc: 0.250000\n",
      "epoch 0, iter 373, loss: 7.999394, acc: 0.375000\n",
      "epoch 0, iter 374, loss: 7.435907, acc: 0.500000\n",
      "epoch 0, iter 375, loss: 6.546100, acc: 0.625000\n",
      "epoch 0, iter 376, loss: 6.624942, acc: 0.500000\n",
      "epoch 0, iter 377, loss: 6.026513, acc: 0.625000\n",
      "epoch 0, iter 378, loss: 7.568747, acc: 0.375000\n",
      "epoch 0, iter 379, loss: 7.191804, acc: 0.375000\n",
      "epoch 0, iter 380, loss: 7.588330, acc: 0.250000\n",
      "epoch 0, iter 381, loss: 7.264668, acc: 0.375000\n",
      "epoch 0, iter 382, loss: 7.363939, acc: 0.625000\n",
      "epoch 0, iter 383, loss: 7.751827, acc: 0.375000\n",
      "epoch 0, iter 384, loss: 7.234476, acc: 0.375000\n",
      "epoch 0, iter 385, loss: 7.641299, acc: 0.250000\n",
      "epoch 0, iter 386, loss: 7.270933, acc: 0.375000\n",
      "epoch 0, iter 387, loss: 7.617354, acc: 0.250000\n",
      "epoch 0, iter 388, loss: 6.513365, acc: 1.000000\n",
      "epoch 0, iter 389, loss: 6.963783, acc: 0.500000\n",
      "epoch 0, iter 390, loss: 7.492938, acc: 0.250000\n",
      "epoch 0, iter 391, loss: 6.402390, acc: 1.000000\n",
      "epoch 0, iter 392, loss: 5.985946, acc: 0.625000\n",
      "epoch 0, iter 393, loss: 8.634080, acc: 0.250000\n",
      "epoch 0, iter 394, loss: 9.791197, acc: 0.125000\n",
      "epoch 0, iter 395, loss: 10.145593, acc: 0.125000\n",
      "epoch 0, iter 396, loss: 9.552387, acc: 0.375000\n",
      "epoch 0, iter 397, loss: 8.520888, acc: 0.500000\n",
      "epoch 0, iter 398, loss: 8.028275, acc: 0.625000\n",
      "epoch 0, iter 399, loss: 8.826748, acc: 0.375000\n",
      "epoch 0, iter 400, loss: 9.169857, acc: 0.500000\n",
      "epoch 0, iter 401, loss: 8.933179, acc: 0.625000\n",
      "epoch 0, iter 402, loss: 7.292212, acc: 0.750000\n",
      "epoch 0, iter 403, loss: 8.451676, acc: 0.250000\n",
      "epoch 0, iter 404, loss: 8.327778, acc: 0.250000\n",
      "epoch 0, iter 405, loss: 8.116420, acc: 0.250000\n",
      "epoch 0, iter 406, loss: 7.411955, acc: 0.500000\n",
      "epoch 0, iter 407, loss: 7.158661, acc: 0.625000\n",
      "epoch 0, iter 408, loss: 7.109776, acc: 0.375000\n",
      "epoch 0, iter 409, loss: 7.586746, acc: 0.250000\n",
      "epoch 0, iter 410, loss: 7.120409, acc: 0.375000\n",
      "epoch 0, iter 411, loss: 7.467016, acc: 0.250000\n",
      "epoch 0, iter 412, loss: 7.513731, acc: 0.250000\n",
      "epoch 0, iter 413, loss: 7.487286, acc: 0.250000\n",
      "epoch 0, iter 414, loss: 7.414095, acc: 0.250000\n",
      "epoch 0, iter 415, loss: 7.444239, acc: 0.250000\n",
      "epoch 0, iter 416, loss: 8.029952, acc: 0.250000\n",
      "epoch 0, iter 417, loss: 8.045380, acc: 0.625000\n",
      "epoch 0, iter 418, loss: 8.735120, acc: 0.250000\n",
      "epoch 0, iter 419, loss: 8.569142, acc: 0.250000\n",
      "epoch 0, iter 420, loss: 7.784575, acc: 0.500000\n",
      "epoch 0, iter 421, loss: 7.471484, acc: 0.375000\n",
      "epoch 0, iter 422, loss: 7.758384, acc: 0.250000\n",
      "epoch 0, iter 423, loss: 7.673727, acc: 0.250000\n",
      "epoch 0, iter 424, loss: 7.414002, acc: 0.375000\n",
      "epoch 0, iter 425, loss: 6.371321, acc: 1.000000\n",
      "epoch 0, iter 426, loss: 8.072671, acc: 0.375000\n",
      "epoch 0, iter 427, loss: 7.440172, acc: 0.875000\n",
      "epoch 0, iter 428, loss: 7.582422, acc: 0.375000\n",
      "epoch 0, iter 429, loss: 7.916643, acc: 0.625000\n",
      "epoch 0, iter 430, loss: 6.594268, acc: 1.000000\n",
      "epoch 0, iter 431, loss: 6.732220, acc: 0.875000\n",
      "epoch 0, iter 432, loss: 7.656089, acc: 0.375000\n",
      "epoch 0, iter 433, loss: 7.226161, acc: 0.500000\n",
      "epoch 0, iter 434, loss: 6.019280, acc: 1.000000\n",
      "epoch 0, iter 435, loss: 5.113553, acc: 1.000000\n",
      "epoch 0, iter 436, loss: 8.182663, acc: 0.375000\n",
      "epoch 0, iter 437, loss: 7.216712, acc: 0.875000\n",
      "epoch 0, iter 438, loss: 7.111803, acc: 0.375000\n",
      "epoch 0, iter 439, loss: 7.469028, acc: 0.250000\n",
      "epoch 0, iter 440, loss: 7.088655, acc: 0.375000\n",
      "epoch 0, iter 441, loss: 7.103785, acc: 0.625000\n",
      "epoch 0, iter 442, loss: 8.315991, acc: 0.375000\n",
      "epoch 0, iter 443, loss: 7.651058, acc: 0.875000\n",
      "epoch 0, iter 444, loss: 7.420005, acc: 0.625000\n",
      "epoch 0, iter 445, loss: 6.434940, acc: 0.500000\n",
      "epoch 0, iter 446, loss: 6.452992, acc: 0.500000\n",
      "epoch 0, iter 447, loss: 6.184861, acc: 0.500000\n",
      "epoch 0, iter 448, loss: 5.977965, acc: 0.500000\n",
      "epoch 0, iter 449, loss: 5.826780, acc: 0.500000\n",
      "epoch 0, iter 450, loss: 5.468522, acc: 0.625000\n",
      "epoch 0, iter 451, loss: 5.752008, acc: 0.500000\n",
      "epoch 0, iter 452, loss: 5.672612, acc: 0.500000\n",
      "epoch 0, iter 453, loss: 5.368840, acc: 0.625000\n",
      "epoch 0, iter 454, loss: 5.687668, acc: 0.625000\n",
      "epoch 0, iter 455, loss: 7.467409, acc: 0.375000\n",
      "epoch 0, iter 456, loss: 6.393916, acc: 0.875000\n",
      "epoch 0, iter 457, loss: 6.789033, acc: 0.375000\n",
      "epoch 0, iter 458, loss: 6.541979, acc: 0.375000\n",
      "epoch 0, iter 459, loss: 6.229096, acc: 0.500000\n",
      "epoch 0, iter 460, loss: 5.994630, acc: 0.500000\n",
      "epoch 0, iter 461, loss: 5.815225, acc: 0.500000\n",
      "epoch 0, iter 462, loss: 6.303649, acc: 0.500000\n",
      "epoch 0, iter 463, loss: 7.693413, acc: 0.250000\n",
      "epoch 0, iter 464, loss: 7.235492, acc: 0.500000\n",
      "epoch 0, iter 465, loss: 7.516865, acc: 0.250000\n",
      "epoch 0, iter 466, loss: 7.441823, acc: 0.375000\n",
      "epoch 0, iter 467, loss: 7.121300, acc: 0.625000\n",
      "epoch 0, iter 468, loss: 6.550291, acc: 0.500000\n",
      "epoch 0, iter 469, loss: 6.211657, acc: 0.500000\n",
      "epoch 0, iter 470, loss: 5.808161, acc: 0.500000\n",
      "epoch 0, iter 471, loss: 5.740266, acc: 1.000000\n",
      "epoch 0, iter 472, loss: 5.419144, acc: 0.875000\n",
      "epoch 0, iter 473, loss: 6.901032, acc: 0.375000\n",
      "epoch 0, iter 474, loss: 6.634523, acc: 0.375000\n",
      "epoch 0, iter 475, loss: 6.034892, acc: 0.500000\n",
      "epoch 0, iter 476, loss: 4.846844, acc: 1.000000\n",
      "epoch 0, iter 477, loss: 6.086246, acc: 0.250000\n",
      "epoch 0, iter 478, loss: 6.384041, acc: 0.250000\n",
      "epoch 0, iter 479, loss: 6.414642, acc: 0.500000\n",
      "epoch 0, iter 480, loss: 5.979350, acc: 0.625000\n",
      "epoch 0, iter 481, loss: 5.409194, acc: 0.875000\n",
      "epoch 0, iter 482, loss: 4.612249, acc: 1.000000\n",
      "epoch 0, iter 483, loss: 4.425342, acc: 0.875000\n",
      "epoch 0, iter 484, loss: 6.519765, acc: 0.375000\n",
      "epoch 0, iter 485, loss: 5.040585, acc: 1.000000\n",
      "epoch 0, iter 486, loss: 5.424984, acc: 0.625000\n",
      "epoch 0, iter 487, loss: 5.088917, acc: 0.750000\n",
      "epoch 0, iter 488, loss: 5.126433, acc: 1.000000\n",
      "epoch 0, iter 489, loss: 6.511959, acc: 0.625000\n",
      "epoch 0, iter 490, loss: 7.136580, acc: 0.375000\n",
      "epoch 0, iter 491, loss: 6.615351, acc: 0.500000\n",
      "epoch 0, iter 492, loss: 6.845312, acc: 0.250000\n",
      "epoch 0, iter 493, loss: 6.556677, acc: 0.750000\n",
      "epoch 0, iter 494, loss: 6.762737, acc: 0.375000\n",
      "epoch 0, iter 495, loss: 6.381375, acc: 0.625000\n",
      "epoch 0, iter 496, loss: 6.539208, acc: 0.375000\n",
      "epoch 0, iter 497, loss: 6.419891, acc: 0.375000\n",
      "epoch 0, iter 498, loss: 6.133341, acc: 0.625000\n",
      "epoch 0, iter 499, loss: 5.651088, acc: 0.625000\n",
      "epoch 0, iter 500, loss: 5.257929, acc: 1.000000\n",
      "epoch 0, iter 501, loss: 6.208488, acc: 0.500000\n",
      "epoch 0, iter 502, loss: 6.965189, acc: 0.500000\n",
      "epoch 0, iter 503, loss: 6.804309, acc: 0.625000\n",
      "epoch 0, iter 504, loss: 6.801918, acc: 0.375000\n",
      "epoch 0, iter 505, loss: 6.370815, acc: 0.500000\n",
      "epoch 0, iter 506, loss: 6.013924, acc: 0.500000\n",
      "epoch 0, iter 507, loss: 6.441250, acc: 0.250000\n",
      "epoch 0, iter 508, loss: 6.397351, acc: 0.625000\n",
      "epoch 0, iter 509, loss: 6.765412, acc: 0.625000\n",
      "epoch 0, iter 510, loss: 6.938273, acc: 0.375000\n",
      "epoch 0, iter 511, loss: 6.770023, acc: 0.375000\n",
      "epoch 0, iter 512, loss: 6.392812, acc: 0.500000\n",
      "epoch 0, iter 513, loss: 5.773813, acc: 0.750000\n",
      "epoch 0, iter 514, loss: 5.310797, acc: 0.625000\n",
      "epoch 0, iter 515, loss: 4.931150, acc: 0.750000\n",
      "epoch 0, iter 516, loss: 4.722462, acc: 0.625000\n",
      "epoch 0, iter 517, loss: 4.248627, acc: 0.875000\n",
      "epoch 0, iter 518, loss: 5.681159, acc: 0.500000\n",
      "epoch 0, iter 519, loss: 5.455530, acc: 0.875000\n",
      "epoch 0, iter 520, loss: 5.085351, acc: 0.875000\n",
      "epoch 0, iter 521, loss: 4.468638, acc: 0.875000\n",
      "epoch 0, iter 522, loss: 4.591172, acc: 1.000000\n",
      "epoch 0, iter 523, loss: 4.397708, acc: 0.875000\n",
      "epoch 0, iter 524, loss: 6.117978, acc: 0.500000\n",
      "epoch 0, iter 525, loss: 6.549915, acc: 0.375000\n",
      "epoch 0, iter 526, loss: 6.519762, acc: 0.625000\n",
      "epoch 0, iter 527, loss: 5.715561, acc: 1.000000\n",
      "epoch 0, iter 528, loss: 6.317499, acc: 0.500000\n",
      "epoch 0, iter 529, loss: 5.688707, acc: 1.000000\n",
      "epoch 0, iter 530, loss: 6.170577, acc: 0.500000\n",
      "epoch 0, iter 531, loss: 6.051725, acc: 0.500000\n",
      "epoch 0, iter 532, loss: 5.836286, acc: 0.625000\n",
      "epoch 0, iter 533, loss: 6.111482, acc: 0.500000\n",
      "epoch 0, iter 534, loss: 6.288914, acc: 0.375000\n",
      "epoch 0, iter 535, loss: 6.172103, acc: 0.375000\n",
      "epoch 0, iter 536, loss: 6.032381, acc: 0.625000\n",
      "epoch 0, iter 537, loss: 5.128898, acc: 1.000000\n",
      "epoch 0, iter 538, loss: 5.332105, acc: 0.750000\n",
      "epoch 0, iter 539, loss: 4.947678, acc: 0.750000\n",
      "epoch 0, iter 540, loss: 5.481408, acc: 0.625000\n",
      "epoch 0, iter 541, loss: 5.480137, acc: 0.500000\n",
      "epoch 0, iter 542, loss: 5.376771, acc: 0.500000\n",
      "epoch 0, iter 543, loss: 5.295697, acc: 0.500000\n",
      "epoch 0, iter 544, loss: 5.748436, acc: 0.625000\n",
      "epoch 0, iter 545, loss: 5.980866, acc: 0.375000\n",
      "epoch 0, iter 546, loss: 5.845782, acc: 0.625000\n",
      "epoch 0, iter 547, loss: 5.704854, acc: 0.500000\n",
      "epoch 0, iter 548, loss: 5.512608, acc: 0.500000\n",
      "epoch 0, iter 549, loss: 5.253545, acc: 0.750000\n",
      "epoch 0, iter 550, loss: 4.756462, acc: 0.750000\n",
      "epoch 0, iter 551, loss: 5.034039, acc: 0.750000\n",
      "epoch 0, iter 552, loss: 4.210614, acc: 1.000000\n",
      "epoch 0, iter 553, loss: 4.150075, acc: 0.750000\n",
      "epoch 0, iter 554, loss: 3.957981, acc: 0.875000\n",
      "epoch 0, iter 555, loss: 3.389113, acc: 1.000000\n",
      "epoch 0, iter 556, loss: 3.391084, acc: 0.750000\n",
      "epoch 0, iter 557, loss: 3.287608, acc: 0.875000\n",
      "epoch 0, iter 558, loss: 4.417830, acc: 0.625000\n",
      "epoch 0, iter 559, loss: 5.678924, acc: 0.500000\n",
      "epoch 0, iter 560, loss: 6.398564, acc: 0.375000\n",
      "epoch 0, iter 561, loss: 6.244980, acc: 0.500000\n",
      "epoch 0, iter 562, loss: 6.613610, acc: 0.375000\n",
      "epoch 0, iter 563, loss: 6.819024, acc: 0.375000\n",
      "epoch 0, iter 564, loss: 6.818181, acc: 0.375000\n",
      "epoch 0, iter 565, loss: 6.101643, acc: 1.000000\n",
      "epoch 0, iter 566, loss: 6.204491, acc: 0.625000\n",
      "epoch 0, iter 567, loss: 5.545689, acc: 0.750000\n",
      "epoch 0, iter 568, loss: 5.557786, acc: 0.500000\n",
      "epoch 0, iter 569, loss: 5.389606, acc: 0.500000\n",
      "epoch 0, iter 570, loss: 5.051458, acc: 1.000000\n",
      "epoch 0, iter 571, loss: 5.203189, acc: 0.750000\n",
      "epoch 0, iter 572, loss: 5.448387, acc: 0.625000\n",
      "epoch 0, iter 573, loss: 5.486812, acc: 0.500000\n",
      "epoch 0, iter 574, loss: 5.295029, acc: 0.500000\n",
      "epoch 0, iter 575, loss: 4.976801, acc: 0.750000\n",
      "epoch 0, iter 576, loss: 4.544024, acc: 1.000000\n",
      "epoch 0, iter 577, loss: 4.714062, acc: 0.750000\n",
      "epoch 0, iter 578, loss: 5.177344, acc: 0.625000\n",
      "epoch 0, iter 579, loss: 4.838259, acc: 0.750000\n",
      "epoch 0, iter 580, loss: 4.375206, acc: 0.750000\n",
      "epoch 0, iter 581, loss: 4.766930, acc: 0.625000\n",
      "epoch 0, iter 582, loss: 4.738078, acc: 0.500000\n",
      "epoch 0, iter 583, loss: 3.968320, acc: 1.000000\n",
      "epoch 0, iter 584, loss: 4.471455, acc: 0.750000\n",
      "epoch 0, iter 585, loss: 5.294812, acc: 0.750000\n",
      "epoch 0, iter 586, loss: 5.226994, acc: 0.875000\n",
      "epoch 0, iter 587, loss: 6.149932, acc: 0.500000\n",
      "epoch 0, iter 588, loss: 5.193122, acc: 1.000000\n",
      "epoch 0, iter 589, loss: 4.548850, acc: 1.000000\n",
      "epoch 0, iter 590, loss: 5.159508, acc: 0.500000\n",
      "epoch 0, iter 591, loss: 5.183549, acc: 0.750000\n",
      "epoch 0, iter 592, loss: 6.206175, acc: 0.500000\n",
      "epoch 0, iter 593, loss: 8.919750, acc: 0.625000\n",
      "epoch 0, iter 594, loss: 8.333664, acc: 0.500000\n",
      "epoch 0, iter 595, loss: 7.058311, acc: 0.750000\n",
      "epoch 0, iter 596, loss: 5.913287, acc: 0.750000\n",
      "epoch 0, iter 597, loss: 5.856560, acc: 0.500000\n",
      "epoch 0, iter 598, loss: 5.672053, acc: 0.875000\n",
      "epoch 0, iter 599, loss: 5.444097, acc: 0.750000\n",
      "epoch 0, iter 600, loss: 5.450407, acc: 0.625000\n",
      "epoch 0, iter 601, loss: 5.997163, acc: 0.625000\n",
      "epoch 0, iter 602, loss: 5.726721, acc: 0.750000\n",
      "epoch 0, iter 603, loss: 5.631155, acc: 0.625000\n",
      "epoch 0, iter 604, loss: 5.150647, acc: 1.000000\n",
      "epoch 0, iter 605, loss: 5.556539, acc: 0.625000\n",
      "epoch 0, iter 606, loss: 5.239058, acc: 0.625000\n",
      "epoch 0, iter 607, loss: 4.796695, acc: 0.750000\n",
      "epoch 0, iter 608, loss: 4.289784, acc: 0.750000\n",
      "epoch 0, iter 609, loss: 4.655984, acc: 0.625000\n",
      "epoch 0, iter 610, loss: 4.321126, acc: 0.750000\n",
      "epoch 0, iter 611, loss: 4.537484, acc: 0.625000\n",
      "epoch 0, iter 612, loss: 4.127688, acc: 1.000000\n",
      "epoch 0, iter 613, loss: 4.081231, acc: 0.750000\n",
      "epoch 0, iter 614, loss: 3.546359, acc: 1.000000\n",
      "epoch 0, iter 615, loss: 2.921026, acc: 1.000000\n",
      "epoch 0, iter 616, loss: 4.107805, acc: 0.625000\n",
      "epoch 0, iter 617, loss: 4.261130, acc: 0.625000\n",
      "epoch 0, iter 618, loss: 4.129915, acc: 0.750000\n",
      "epoch 0, iter 619, loss: 4.401279, acc: 0.500000\n",
      "epoch 0, iter 620, loss: 4.334756, acc: 0.625000\n",
      "epoch 0, iter 621, loss: 4.258376, acc: 0.750000\n",
      "epoch 0, iter 622, loss: 4.240517, acc: 0.625000\n",
      "epoch 0, iter 623, loss: 4.176801, acc: 0.750000\n",
      "epoch 0, iter 624, loss: 4.171914, acc: 0.625000\n",
      "epoch 0, iter 625, loss: 4.189030, acc: 0.750000\n",
      "epoch 0, iter 626, loss: 4.471177, acc: 0.625000\n",
      "epoch 0, iter 627, loss: 4.388934, acc: 0.875000\n",
      "epoch 0, iter 628, loss: 4.326627, acc: 0.750000\n",
      "epoch 0, iter 629, loss: 5.538486, acc: 0.875000\n",
      "epoch 0, iter 630, loss: 7.119447, acc: 0.500000\n",
      "epoch 0, iter 631, loss: 7.119797, acc: 0.750000\n",
      "epoch 0, iter 632, loss: 7.326105, acc: 0.375000\n",
      "epoch 0, iter 633, loss: 6.983199, acc: 0.375000\n",
      "epoch 0, iter 634, loss: 6.886856, acc: 0.625000\n",
      "epoch 0, iter 635, loss: 7.294142, acc: 0.375000\n",
      "epoch 0, iter 636, loss: 6.958643, acc: 0.375000\n",
      "epoch 0, iter 637, loss: 6.135288, acc: 1.000000\n",
      "epoch 0, iter 638, loss: 5.736837, acc: 0.750000\n",
      "epoch 0, iter 639, loss: 5.173503, acc: 0.750000\n",
      "epoch 0, iter 640, loss: 5.534962, acc: 0.625000\n",
      "epoch 0, iter 641, loss: 5.101168, acc: 0.750000\n",
      "epoch 0, iter 642, loss: 4.553970, acc: 1.000000\n",
      "epoch 0, iter 643, loss: 6.383392, acc: 0.500000\n",
      "epoch 0, iter 644, loss: 8.041647, acc: 0.375000\n",
      "epoch 0, iter 645, loss: 8.539399, acc: 0.375000\n",
      "epoch 0, iter 646, loss: 7.848439, acc: 0.875000\n",
      "epoch 0, iter 647, loss: 7.213608, acc: 0.625000\n",
      "epoch 0, iter 648, loss: 6.278509, acc: 0.625000\n",
      "epoch 0, iter 649, loss: 5.573139, acc: 0.625000\n",
      "epoch 0, iter 650, loss: 5.056158, acc: 0.750000\n",
      "epoch 0, iter 651, loss: 4.953471, acc: 0.625000\n",
      "epoch 0, iter 652, loss: 4.646300, acc: 0.625000\n",
      "epoch 0, iter 653, loss: 4.373495, acc: 0.750000\n",
      "epoch 0, iter 654, loss: 3.729833, acc: 1.000000\n",
      "epoch 0, iter 655, loss: 3.955037, acc: 1.000000\n",
      "epoch 0, iter 656, loss: 4.041914, acc: 0.750000\n",
      "epoch 0, iter 657, loss: 4.515766, acc: 0.625000\n",
      "epoch 0, iter 658, loss: 4.123793, acc: 0.750000\n",
      "epoch 0, iter 659, loss: 4.277929, acc: 0.625000\n",
      "epoch 0, iter 660, loss: 3.912714, acc: 1.000000\n",
      "epoch 0, iter 661, loss: 3.855131, acc: 0.750000\n",
      "epoch 0, iter 662, loss: 3.330070, acc: 1.000000\n",
      "epoch 0, iter 663, loss: 4.116034, acc: 0.625000\n",
      "epoch 0, iter 664, loss: 3.834162, acc: 0.750000\n",
      "epoch 0, iter 665, loss: 4.061551, acc: 0.625000\n",
      "epoch 0, iter 666, loss: 3.989363, acc: 0.625000\n",
      "epoch 0, iter 667, loss: 3.900187, acc: 0.750000\n",
      "epoch 0, iter 668, loss: 4.091076, acc: 0.625000\n",
      "epoch 0, iter 669, loss: 4.000618, acc: 0.625000\n",
      "epoch 0, iter 670, loss: 3.905671, acc: 0.750000\n",
      "epoch 0, iter 671, loss: 3.830353, acc: 0.750000\n",
      "epoch 0, iter 672, loss: 3.923663, acc: 1.000000\n",
      "epoch 0, iter 673, loss: 4.100614, acc: 0.875000\n",
      "epoch 0, iter 674, loss: 4.288385, acc: 0.625000\n",
      "epoch 0, iter 675, loss: 4.120386, acc: 0.750000\n",
      "epoch 0, iter 676, loss: 3.988415, acc: 0.750000\n",
      "epoch 0, iter 677, loss: 4.596713, acc: 0.625000\n",
      "epoch 0, iter 678, loss: 4.722671, acc: 0.625000\n",
      "epoch 0, iter 679, loss: 4.443232, acc: 0.750000\n",
      "epoch 0, iter 680, loss: 4.525684, acc: 0.625000\n",
      "epoch 0, iter 681, loss: 4.315427, acc: 0.625000\n",
      "epoch 0, iter 682, loss: 4.114136, acc: 0.750000\n",
      "epoch 0, iter 683, loss: 4.135790, acc: 0.875000\n",
      "epoch 0, iter 684, loss: 5.313248, acc: 0.875000\n",
      "epoch 0, iter 685, loss: 5.341226, acc: 0.750000\n",
      "epoch 0, iter 686, loss: 4.853680, acc: 0.750000\n",
      "epoch 0, iter 687, loss: 4.466175, acc: 0.750000\n",
      "epoch 0, iter 688, loss: 4.210611, acc: 1.000000\n",
      "epoch 0, iter 689, loss: 4.609380, acc: 0.625000\n",
      "epoch 0, iter 690, loss: 4.270145, acc: 0.750000\n",
      "epoch 0, iter 691, loss: 3.785232, acc: 0.750000\n",
      "epoch 0, iter 692, loss: 3.310659, acc: 1.000000\n",
      "epoch 0, iter 693, loss: 4.050351, acc: 0.625000\n",
      "epoch 0, iter 694, loss: 3.736284, acc: 0.750000\n",
      "epoch 0, iter 695, loss: 3.919254, acc: 0.625000\n",
      "epoch 0, iter 696, loss: 3.818908, acc: 0.750000\n",
      "epoch 0, iter 697, loss: 3.741636, acc: 0.750000\n",
      "epoch 0, iter 698, loss: 3.899840, acc: 0.625000\n",
      "epoch 0, iter 699, loss: 3.790726, acc: 0.750000\n",
      "epoch 0, iter 700, loss: 3.858556, acc: 0.875000\n",
      "epoch 0, iter 701, loss: 3.992807, acc: 0.875000\n",
      "epoch 0, iter 702, loss: 3.889820, acc: 1.000000\n",
      "epoch 0, iter 703, loss: 4.010515, acc: 0.750000\n",
      "epoch 0, iter 704, loss: 3.882790, acc: 0.875000\n",
      "epoch 0, iter 705, loss: 3.494568, acc: 1.000000\n",
      "epoch 0, iter 706, loss: 4.343724, acc: 0.875000\n",
      "epoch 0, iter 707, loss: 4.174708, acc: 0.750000\n",
      "epoch 0, iter 708, loss: 4.424141, acc: 0.750000\n",
      "epoch 0, iter 709, loss: 3.886192, acc: 1.000000\n",
      "epoch 0, iter 710, loss: 3.828849, acc: 0.750000\n",
      "epoch 0, iter 711, loss: 3.424011, acc: 0.750000\n",
      "epoch 0, iter 712, loss: 3.119812, acc: 0.750000\n",
      "epoch 0, iter 713, loss: 3.713727, acc: 0.625000\n",
      "epoch 0, iter 714, loss: 3.747215, acc: 0.750000\n",
      "epoch 0, iter 715, loss: 3.266538, acc: 1.000000\n",
      "epoch 0, iter 716, loss: 3.541313, acc: 0.875000\n",
      "epoch 0, iter 717, loss: 3.529778, acc: 0.875000\n",
      "epoch 0, iter 718, loss: 3.524049, acc: 0.875000\n",
      "epoch 0, iter 719, loss: 3.891042, acc: 0.875000\n",
      "epoch 0, iter 720, loss: 3.970419, acc: 0.750000\n",
      "epoch 0, iter 721, loss: 4.038906, acc: 0.625000\n",
      "epoch 0, iter 722, loss: 4.001283, acc: 0.875000\n",
      "epoch 0, iter 723, loss: 4.347229, acc: 0.875000\n",
      "epoch 0, iter 724, loss: 4.092597, acc: 0.875000\n",
      "epoch 0, iter 725, loss: 3.916090, acc: 0.750000\n",
      "epoch 0, iter 726, loss: 3.388544, acc: 1.000000\n",
      "epoch 0, iter 727, loss: 3.395053, acc: 0.750000\n",
      "epoch 0, iter 728, loss: 2.934828, acc: 1.000000\n",
      "epoch 0, iter 729, loss: 3.788628, acc: 0.625000\n",
      "epoch 0, iter 730, loss: 3.323218, acc: 1.000000\n",
      "epoch 0, iter 731, loss: 3.634810, acc: 0.750000\n",
      "epoch 0, iter 732, loss: 3.389167, acc: 1.000000\n",
      "epoch 0, iter 733, loss: 3.491300, acc: 0.750000\n",
      "epoch 0, iter 734, loss: 3.017054, acc: 1.000000\n",
      "epoch 0, iter 735, loss: 3.101866, acc: 0.750000\n",
      "epoch 0, iter 736, loss: 2.702734, acc: 1.000000\n",
      "epoch 0, iter 737, loss: 2.188218, acc: 1.000000\n",
      "epoch 0, iter 738, loss: 2.644562, acc: 0.750000\n",
      "epoch 0, iter 739, loss: 2.985917, acc: 0.875000\n",
      "epoch 0, iter 740, loss: 3.798796, acc: 0.625000\n",
      "epoch 0, iter 741, loss: 4.672705, acc: 0.875000\n",
      "epoch 0, iter 742, loss: 4.103084, acc: 0.750000\n",
      "epoch 0, iter 743, loss: 3.565703, acc: 0.875000\n",
      "epoch 0, iter 744, loss: 3.179933, acc: 0.750000\n",
      "epoch 0, iter 745, loss: 3.704973, acc: 0.750000\n",
      "epoch 0, iter 746, loss: 3.724808, acc: 0.625000\n",
      "epoch 0, iter 747, loss: 3.577533, acc: 0.750000\n",
      "epoch 0, iter 748, loss: 3.244732, acc: 0.750000\n",
      "epoch 0, iter 749, loss: 3.588869, acc: 0.625000\n",
      "epoch 0, iter 750, loss: 3.538227, acc: 0.750000\n",
      "epoch 0, iter 751, loss: 3.443022, acc: 0.750000\n",
      "epoch 0, iter 752, loss: 3.357758, acc: 0.750000\n",
      "epoch 0, iter 753, loss: 3.258674, acc: 0.875000\n",
      "epoch 0, iter 754, loss: 3.299297, acc: 1.000000\n",
      "epoch 0, iter 755, loss: 4.334708, acc: 1.000000\n",
      "epoch 0, iter 756, loss: 6.462911, acc: 0.875000\n",
      "epoch 0, iter 757, loss: 6.149032, acc: 1.000000\n",
      "epoch 0, iter 758, loss: 6.330177, acc: 0.750000\n",
      "epoch 0, iter 759, loss: 5.873321, acc: 0.625000\n",
      "epoch 0, iter 760, loss: 4.988084, acc: 0.750000\n",
      "epoch 0, iter 761, loss: 4.701083, acc: 0.625000\n",
      "epoch 0, iter 762, loss: 4.193865, acc: 0.750000\n",
      "epoch 0, iter 763, loss: 4.076847, acc: 1.000000\n",
      "epoch 0, iter 764, loss: 3.921046, acc: 0.750000\n",
      "epoch 0, iter 765, loss: 3.319682, acc: 1.000000\n",
      "epoch 0, iter 766, loss: 3.287715, acc: 0.750000\n",
      "epoch 0, iter 767, loss: 3.729879, acc: 0.625000\n",
      "epoch 0, iter 768, loss: 3.664243, acc: 0.750000\n",
      "epoch 0, iter 769, loss: 3.519098, acc: 0.750000\n",
      "epoch 0, iter 770, loss: 4.008581, acc: 0.875000\n",
      "epoch 0, iter 771, loss: 4.114289, acc: 0.750000\n",
      "epoch 0, iter 772, loss: 4.784186, acc: 0.625000\n",
      "epoch 0, iter 773, loss: 4.175214, acc: 1.000000\n",
      "epoch 0, iter 774, loss: 4.405047, acc: 0.750000\n",
      "epoch 0, iter 775, loss: 4.050342, acc: 0.750000\n",
      "epoch 0, iter 776, loss: 4.395191, acc: 0.625000\n",
      "epoch 0, iter 777, loss: 4.818034, acc: 0.750000\n",
      "epoch 0, iter 778, loss: 4.818724, acc: 1.000000\n",
      "epoch 0, iter 779, loss: 4.156583, acc: 1.000000\n",
      "epoch 0, iter 780, loss: 4.569343, acc: 0.875000\n",
      "epoch 0, iter 781, loss: 5.794057, acc: 0.750000\n",
      "epoch 0, iter 782, loss: 7.174780, acc: 0.750000\n",
      "epoch 0, iter 783, loss: 6.649408, acc: 0.750000\n",
      "epoch 0, iter 784, loss: 6.169964, acc: 0.625000\n",
      "epoch 0, iter 785, loss: 5.322080, acc: 0.750000\n",
      "epoch 0, iter 786, loss: 4.654171, acc: 0.750000\n",
      "epoch 0, iter 787, loss: 4.300427, acc: 0.750000\n",
      "epoch 0, iter 788, loss: 4.171760, acc: 0.750000\n",
      "epoch 0, iter 789, loss: 3.883827, acc: 0.750000\n",
      "epoch 0, iter 790, loss: 3.858204, acc: 0.625000\n",
      "epoch 0, iter 791, loss: 3.480179, acc: 1.000000\n",
      "epoch 0, iter 792, loss: 3.809175, acc: 0.625000\n",
      "epoch 0, iter 793, loss: 3.346005, acc: 1.000000\n",
      "epoch 0, iter 794, loss: 4.387892, acc: 0.625000\n",
      "epoch 0, iter 795, loss: 4.795643, acc: 0.625000\n",
      "epoch 0, iter 796, loss: 4.615300, acc: 0.875000\n",
      "epoch 0, iter 797, loss: 4.185620, acc: 1.000000\n",
      "epoch 0, iter 798, loss: 4.724025, acc: 0.875000\n",
      "epoch 0, iter 799, loss: 4.062012, acc: 1.000000\n",
      "epoch 0, iter 800, loss: 3.678809, acc: 1.000000\n",
      "epoch 0, iter 801, loss: 4.216198, acc: 0.875000\n",
      "epoch 0, iter 802, loss: 3.996831, acc: 0.750000\n",
      "epoch 0, iter 803, loss: 3.932017, acc: 0.625000\n",
      "epoch 0, iter 804, loss: 3.684456, acc: 0.750000\n",
      "epoch 0, iter 805, loss: 3.481965, acc: 0.750000\n",
      "epoch 0, iter 806, loss: 3.464423, acc: 0.750000\n",
      "epoch 0, iter 807, loss: 3.146651, acc: 0.750000\n",
      "epoch 0, iter 808, loss: 2.886099, acc: 0.750000\n",
      "epoch 0, iter 809, loss: 2.601160, acc: 1.000000\n",
      "epoch 0, iter 810, loss: 3.138458, acc: 0.750000\n",
      "epoch 0, iter 811, loss: 5.137545, acc: 0.625000\n",
      "epoch 0, iter 812, loss: 4.981007, acc: 0.875000\n",
      "epoch 0, iter 813, loss: 4.563920, acc: 1.000000\n",
      "epoch 0, iter 814, loss: 4.518525, acc: 0.875000\n",
      "epoch 0, iter 815, loss: 4.339963, acc: 0.750000\n",
      "epoch 0, iter 816, loss: 3.946459, acc: 0.750000\n",
      "epoch 0, iter 817, loss: 3.638440, acc: 0.750000\n",
      "epoch 0, iter 818, loss: 4.483718, acc: 0.750000\n",
      "epoch 0, iter 819, loss: 4.284261, acc: 0.875000\n",
      "epoch 0, iter 820, loss: 3.797077, acc: 1.000000\n",
      "epoch 0, iter 821, loss: 3.657548, acc: 0.875000\n",
      "epoch 0, iter 822, loss: 3.715965, acc: 0.875000\n",
      "epoch 0, iter 823, loss: 3.411450, acc: 0.875000\n",
      "epoch 0, iter 824, loss: 3.050863, acc: 1.000000"
     ]
    }
   ],
   "source": [
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/num_problems)*num_timesteps # loss at iteration 0\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "for e in xrange(epochs):\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    total_acc = 0.0\n",
    "    for i in xrange(num_train):\n",
    "        inputs = X_train[i,:,:].reshape((num_timesteps, num_problems * 2))\n",
    "        targets = y_train[i,:].reshape((num_timesteps,))\n",
    "        correctness_for_student = correctness[i,:].reshape((num_problems))\n",
    "\n",
    "        # forward num_timesteps characters through the net and fetch gradient\n",
    "        loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, correctness_for_student, hprev)\n",
    "        smooth_loss = smooth_loss * 0.7 + loss * 0.3\n",
    "        losses.append(smooth_loss)\n",
    "\n",
    "        # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                      [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                      [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "        ps = forward_pass(inputs)\n",
    "        acc = accuracy(ps, targets, correctness_for_student) \n",
    "        accuracies.append(acc)\n",
    "        print ('epoch %d, iter %d, loss: %f, acc: %f' % (e, i, smooth_loss, acc)) # print progress\n",
    "        total_acc += acc\n",
    "    total_acc /= num_train\n",
    "    print ('epoch %d, acc: %f' % (e, total_acc)) # print progress\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAEPCAYAAAAJYmAlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109fc8c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "smooth_accs = []\n",
    "\n",
    "smooth_window = 100\n",
    "for i in xrange(len(accuracies)-smooth_window):\n",
    "    smooth_accs.append(np.mean(accuracies[i:i+smooth_window]))\n",
    "\n",
    "for i in xrange(len(accuracies)-smooth_window, len(accuracies)):\n",
    "    smooth_accs.append(np.mean(accuracies[i:len(accuracies)]))\n",
    "                    \n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(xrange(len(smooth_accs)/2), smooth_accs[:len(smooth_accs)/2], 'b-')\n",
    "ax1.set_ylabel('accuracies', color='b')\n",
    "ax1.set_xlabel('iterations')\n",
    "for tl in ax1.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(xrange(len(losses)/2), losses[:len(losses)/2], 'r-')\n",
    "ax2.set_ylabel('losses', color='r')\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure_filename = \"loss_plots/test.png\"\n",
    "fig.savefig(figure_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
