{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\"\"\"\n",
    "Implements an RNN on a synthetic data set, following the architecture \n",
    "described in \"Deep Knowledge Tracing\" by Chris Piech et al.\n",
    "The RNN implementation is based on min-char-rnn.py by Andrej Karpathy (@karpathy).\n",
    "BSD License\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "# allows plots to show inline in ipython notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# our own modules\n",
    "import utils\n",
    "\n",
    "synthetic_data_set = \"syntheticDetailed/naive_c5_q50_s4000_v0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Vectorization done!\n"
     ]
    }
   ],
   "source": [
    "# Read in the data set\n",
    "# This function can be moved to utils.py\n",
    "data_array = np.array(list(csv.reader(open(synthetic_data_set,\"rb\"),delimiter=','))).astype('int')\n",
    "data_array = data_array[:500]\n",
    "num_samples = data_array.shape[0]\n",
    "num_problems = data_array.shape[1]\n",
    "\n",
    "# time steps is number of problems - 1 because we cannot predict on the last problem.\n",
    "num_timesteps = num_problems - 1 \n",
    "# Split data into train and test (half and half)\n",
    "train = data_array[0:num_samples/2,:]\n",
    "test = data_array[num_samples/2:num_samples,:]\n",
    "\n",
    "num_train = train.shape[0]\n",
    "num_test = test.shape[0]\n",
    "\n",
    "print('Vectorization...')\n",
    "X_train, y_train, corr_train = utils.vectorize_syn_data(train, num_timesteps)\n",
    "X_test, y_test, corr_test = utils.vectorize_syn_data(test, num_timesteps)\n",
    "print (\"Vectorization done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 200 # size of hidden layer of neurons\n",
    "learning_rate = 1e-1\n",
    "epochs = 40\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, num_problems * 2)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(num_problems, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((num_problems, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, correctness, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps, ps_denom = {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = inputs[t,:].reshape((num_problems * 2, 1))\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps_denom[t] = np.sum(np.exp(ys[t]))\n",
    "        ps[t] = np.exp(ys[t]) / ps_denom[t] # probabilities for next chars\n",
    "\n",
    "        # softmax (cross-entropy loss)\n",
    "        if correctness[targets[t]] == 1:\n",
    "            loss += -np.log(ps[t][targets[t],0]) \n",
    "        else:\n",
    "            loss += -np.log(1-ps[t][targets[t],0]) \n",
    "        # backward pass: compute gradients going backwards\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "        dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        if correctness[targets[t]] == 1:\n",
    "            dy[targets[t]] -= 1 # backprop into y\n",
    "        else:\n",
    "            for p in xrange(num_problems):\n",
    "                if p != targets[t]:\n",
    "                    dy[p] -= np.exp(ys[t][p]) / (ps_denom[t] - np.exp(ys[t][targets[t]]))\n",
    "\n",
    "\n",
    "\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(ps, targets, correctness):\n",
    "    \"\"\"\n",
    "    Computes the accuracy using the predictions at each time step.\n",
    "    For each t, if probability of next problem is > 0.5 for correct, or <= 0.5 \n",
    "    for incorrect, then count this as correct prediction.\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    for t in xrange(num_timesteps):\n",
    "        predicted_prob = ps[t][targets[t],0] \n",
    "        if (predicted_prob >= 0.5 and correctness[targets[t]] == 1) or (predicted_prob < 0.5 and correctness[targets[t]] == 0):\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / float(num_timesteps)\n",
    "    return accuracy\n",
    "\n",
    "def forward_pass(inputs):\n",
    "    xs, hs, ys, ps, ps_denom = {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = inputs[t,:].reshape((num_problems * 2, 1))\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps_denom[t] = np.sum(np.exp(ys[t]))\n",
    "        ps[t] = np.exp(ys[t]) / ps_denom[t] # probabilities for next chars\n",
    "    return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_x_y_corr_for_sample(X, y, corr):\n",
    "    num_timesteps = X.shape[1]\n",
    "    num_problems = corr.shape[1]\n",
    "    inputs = X[i,:,:].reshape((num_timesteps, num_problems * 2))\n",
    "    targets = y[i,:].reshape((num_timesteps,))\n",
    "    correctness = corr[i,:].reshape((num_problems))\n",
    "    return inputs, targets, correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, iter 20, loss: 275.819068, train acc: 0.102041\n",
      "epoch 0, iter 40, loss: 146.245137, train acc: 0.448980\n",
      "epoch 0, iter 60, loss: 144.715648, train acc: 0.346939\n",
      "epoch 0, iter 80, loss: 154.769467, train acc: 0.061224\n",
      "epoch 0, iter 100, loss: 123.713382, train acc: 0.612245\n",
      "epoch 0, iter 120, loss: 118.403724, train acc: 0.387755\n",
      "epoch 0, iter 140, loss: 114.642354, train acc: 0.469388\n",
      "epoch 0, iter 160, loss: 119.412068, train acc: 0.530612\n",
      "epoch 0, iter 180, loss: 147.113278, train acc: 0.204082\n",
      "epoch 0, iter 200, loss: 115.731052, train acc: 0.551020\n",
      "epoch 0, iter 220, loss: 115.791378, train acc: 0.653061\n",
      "epoch 0, iter 240, loss: 126.706344, train acc: 0.265306\n",
      "epoch 0, train acc: 0.380571, test acc: 0.397714\n",
      "epoch 1, iter 20, loss: 145.386998, train acc: 0.102041\n",
      "epoch 1, iter 40, loss: 112.609038, train acc: 0.448980\n",
      "epoch 1, iter 60, loss: 127.033438, train acc: 0.346939\n",
      "epoch 1, iter 80, loss: 139.232045, train acc: 0.061224\n",
      "epoch 1, iter 100, loss: 111.896846, train acc: 0.612245\n",
      "epoch 1, iter 120, loss: 106.224709, train acc: 0.387755\n",
      "epoch 1, iter 140, loss: 101.134486, train acc: 0.469388\n",
      "epoch 1, iter 160, loss: 106.444052, train acc: 0.530612\n",
      "epoch 1, iter 180, loss: 129.529466, train acc: 0.204082\n",
      "epoch 1, iter 200, loss: 101.214508, train acc: 0.551020\n",
      "epoch 1, iter 220, loss: 97.879062, train acc: 0.653061\n",
      "epoch 1, iter 240, loss: 107.035307, train acc: 0.265306\n",
      "epoch 1, train acc: 0.379673, test acc: 0.397714\n",
      "epoch 2, iter 20, loss: 121.472321, train acc: 0.102041\n",
      "epoch 2, iter 40, loss: 93.251799, train acc: 0.448980\n",
      "epoch 2, iter 60, loss: 100.758138, train acc: 0.346939\n",
      "epoch 2, iter 80, loss: 104.871498, train acc: 0.061224\n",
      "epoch 2, iter 100, loss: 79.624871, train acc: 0.612245\n",
      "epoch 2, iter 120, loss: 73.688963, train acc: 0.387755\n",
      "epoch 2, iter 140, loss: 64.419536, train acc: 0.469388\n",
      "epoch 2, iter 160, loss: 69.619235, train acc: 0.530612\n",
      "epoch 2, iter 180, loss: 76.482897, train acc: 0.224490\n",
      "epoch 2, iter 200, loss: 57.725964, train acc: 0.571429\n",
      "epoch 2, iter 220, loss: 55.497447, train acc: 0.612245\n",
      "epoch 2, iter 240, loss: 52.420136, train acc: 0.530612\n",
      "epoch 2, train acc: 0.402857, test acc: 0.442204\n",
      "epoch 3, iter 20, loss: 53.120689, train acc: 0.428571\n",
      "epoch 3, iter 40, loss: 42.819139, train acc: 0.591837\n",
      "epoch 3, iter 60, loss: 45.710191, train acc: 0.591837\n",
      "epoch 3, iter 80, loss: 41.697557, train acc: 0.530612\n",
      "epoch 3, iter 100, loss: 38.198537, train acc: 0.673469\n",
      "epoch 3, iter 120, loss: 37.520269, train acc: 0.653061\n",
      "epoch 3, iter 140, loss: 36.946485, train acc: 0.693878\n",
      "epoch 3, iter 160, loss: 38.892666, train acc: 0.571429\n",
      "epoch 3, iter 180, loss: 36.914154, train acc: 0.632653\n",
      "epoch 3, iter 200, loss: 37.093805, train acc: 0.734694\n",
      "epoch 3, iter 220, loss: 38.458499, train acc: 0.734694\n",
      "epoch 3, iter 240, loss: 34.262336, train acc: 0.734694\n",
      "epoch 3, train acc: 0.640816, test acc: 0.474939\n",
      "epoch 4, iter 20, loss: 45.822180, train acc: 0.734694\n",
      "epoch 4, iter 40, loss: 36.321398, train acc: 0.673469\n",
      "epoch 4, iter 60, loss: 35.517929, train acc: 0.714286\n",
      "epoch 4, iter 80, loss: 32.378514, train acc: 0.795918\n",
      "epoch 4, iter 100, loss: 38.910725, train acc: 0.795918\n",
      "epoch 4, iter 120, loss: 34.554083, train acc: 0.653061\n",
      "epoch 4, iter 140, loss: 34.624649, train acc: 0.653061\n",
      "epoch 4, iter 160, loss: 37.651252, train acc: 0.571429\n",
      "epoch 4, iter 180, loss: 33.706881, train acc: 0.673469\n",
      "epoch 4, iter 200, loss: 37.048800, train acc: 0.755102\n",
      "epoch 4, iter 220, loss: 36.952508, train acc: 0.673469\n",
      "epoch 4, iter 240, loss: 33.731339, train acc: 0.714286\n",
      "epoch 4, train acc: 0.708490, test acc: 0.574939\n",
      "epoch 5, iter 20, loss: 33.248673, train acc: 0.897959\n",
      "epoch 5, iter 40, loss: 34.627871, train acc: 0.836735\n",
      "epoch 5, iter 60, loss: 33.413177, train acc: 0.714286\n",
      "epoch 5, iter 80, loss: 32.350298, train acc: 0.877551\n",
      "epoch 5, iter 100, loss: 36.038952, train acc: 0.795918\n",
      "epoch 5, iter 120, loss: 33.775357, train acc: 0.673469\n",
      "epoch 5, iter 140, loss: 33.712833, train acc: 0.632653\n",
      "epoch 5, iter 160, loss: 37.376181, train acc: 0.653061\n",
      "epoch 5, iter 180, loss: 32.539790, train acc: 0.734694\n",
      "epoch 5, iter 200, loss: 34.705238, train acc: 0.714286\n",
      "epoch 5, iter 220, loss: 36.150559, train acc: 0.653061\n",
      "epoch 5, iter 240, loss: 32.441041, train acc: 0.775510\n",
      "epoch 5, train acc: 0.708816, test acc: 0.561633\n",
      "epoch 6, iter 20, loss: 30.940111, train acc: 0.857143\n",
      "epoch 6, iter 40, loss: 33.210697, train acc: 0.816327\n",
      "epoch 6, iter 60, loss: 32.565886, train acc: 0.693878\n",
      "epoch 6, iter 80, loss: 31.197966, train acc: 0.877551\n",
      "epoch 6, iter 100, loss: 35.024925, train acc: 0.714286\n",
      "epoch 6, iter 120, loss: 32.981308, train acc: 0.632653\n",
      "epoch 6, iter 140, loss: 33.645828, train acc: 0.673469\n",
      "epoch 6, iter 160, loss: 35.220406, train acc: 0.612245\n",
      "epoch 6, iter 180, loss: 32.510821, train acc: 0.734694\n",
      "epoch 6, iter 200, loss: 33.423928, train acc: 0.693878\n",
      "epoch 6, iter 220, loss: 35.080046, train acc: 0.632653\n",
      "epoch 6, iter 240, loss: 32.178246, train acc: 0.734694\n",
      "epoch 6, train acc: 0.704163, test acc: 0.578694\n",
      "epoch 7, iter 20, loss: 29.643506, train acc: 0.857143\n",
      "epoch 7, iter 40, loss: 32.239216, train acc: 0.755102\n",
      "epoch 7, iter 60, loss: 32.071967, train acc: 0.693878\n",
      "epoch 7, iter 80, loss: 30.140107, train acc: 0.897959\n",
      "epoch 7, iter 100, loss: 34.522437, train acc: 0.653061\n",
      "epoch 7, iter 120, loss: 32.350810, train acc: 0.693878\n",
      "epoch 7, iter 140, loss: 31.812504, train acc: 0.734694\n",
      "epoch 7, iter 160, loss: 34.332927, train acc: 0.612245\n",
      "epoch 7, iter 180, loss: 31.658825, train acc: 0.775510\n",
      "epoch 7, iter 200, loss: 33.002020, train acc: 0.693878\n",
      "epoch 7, iter 220, loss: 34.064233, train acc: 0.551020\n",
      "epoch 7, iter 240, loss: 32.161791, train acc: 0.734694\n",
      "epoch 7, train acc: 0.700245, test acc: 0.572000\n",
      "epoch 8, iter 20, loss: 28.970359, train acc: 0.836735\n",
      "epoch 8, iter 40, loss: 31.283410, train acc: 0.734694\n",
      "epoch 8, iter 60, loss: 31.259503, train acc: 0.734694\n",
      "epoch 8, iter 80, loss: 30.065704, train acc: 0.877551\n",
      "epoch 8, iter 100, loss: 34.096861, train acc: 0.612245\n",
      "epoch 8, iter 120, loss: 31.993410, train acc: 0.612245\n",
      "epoch 8, iter 140, loss: 30.395736, train acc: 0.693878\n",
      "epoch 8, iter 160, loss: 34.371088, train acc: 0.653061\n",
      "epoch 8, iter 180, loss: 31.660880, train acc: 0.734694\n",
      "epoch 8, iter 200, loss: 32.041891, train acc: 0.653061\n",
      "epoch 8, iter 220, loss: 33.522809, train acc: 0.612245\n",
      "epoch 8, iter 240, loss: 32.100983, train acc: 0.734694\n",
      "epoch 8, train acc: 0.694694, test acc: 0.578041\n",
      "epoch 9, iter 20, loss: 28.693121, train acc: 0.816327\n",
      "epoch 9, iter 40, loss: 30.576527, train acc: 0.714286\n",
      "epoch 9, iter 60, loss: 31.046165, train acc: 0.693878\n",
      "epoch 9, iter 80, loss: 29.290731, train acc: 0.877551\n",
      "epoch 9, iter 100, loss: 33.560042, train acc: 0.612245\n",
      "epoch 9, iter 120, loss: 31.457834, train acc: 0.632653\n",
      "epoch 9, iter 140, loss: 30.809853, train acc: 0.673469\n",
      "epoch 9, iter 160, loss: 34.063312, train acc: 0.612245\n",
      "epoch 9, iter 180, loss: 30.659478, train acc: 0.734694\n",
      "epoch 9, iter 200, loss: 31.786239, train acc: 0.693878\n",
      "epoch 9, iter 220, loss: 33.378033, train acc: 0.653061\n",
      "epoch 9, iter 240, loss: 31.548125, train acc: 0.734694\n",
      "epoch 9, train acc: 0.693143, test acc: 0.583592\n",
      "epoch 10, iter 20, loss: 27.961165, train acc: 0.836735\n",
      "epoch 10, iter 40, loss: 29.814372, train acc: 0.734694\n",
      "epoch 10, iter 60, loss: 30.757681, train acc: 0.714286\n",
      "epoch 10, iter 80, loss: 28.976900, train acc: 0.877551\n",
      "epoch 10, iter 100, loss: 32.900125, train acc: 0.591837\n",
      "epoch 10, iter 120, loss: 30.805110, train acc: 0.612245\n",
      "epoch 10, iter 140, loss: 29.867288, train acc: 0.653061\n",
      "epoch 10, iter 160, loss: 33.127050, train acc: 0.591837\n",
      "epoch 10, iter 180, loss: 29.860675, train acc: 0.775510\n",
      "epoch 10, iter 200, loss: 31.172186, train acc: 0.673469\n",
      "epoch 10, iter 220, loss: 32.531907, train acc: 0.653061\n",
      "epoch 10, iter 240, loss: 31.114071, train acc: 0.714286\n",
      "epoch 10, train acc: 0.688816, test acc: 0.567184\n",
      "epoch 11, iter 20, loss: 27.168052, train acc: 0.857143\n",
      "epoch 11, iter 40, loss: 29.330060, train acc: 0.714286\n",
      "epoch 11, iter 60, loss: 30.411122, train acc: 0.693878\n",
      "epoch 11, iter 80, loss: 28.395849, train acc: 0.877551\n",
      "epoch 11, iter 100, loss: 32.410785, train acc: 0.612245\n",
      "epoch 11, iter 120, loss: 31.015617, train acc: 0.571429\n",
      "epoch 11, iter 140, loss: 29.151987, train acc: 0.673469\n",
      "epoch 11, iter 160, loss: 32.727271, train acc: 0.591837\n",
      "epoch 11, iter 180, loss: 29.380400, train acc: 0.775510\n",
      "epoch 11, iter 200, loss: 30.855048, train acc: 0.612245\n",
      "epoch 11, iter 220, loss: 32.095811, train acc: 0.632653\n",
      "epoch 11, iter 240, loss: 30.488554, train acc: 0.775510\n",
      "epoch 11, train acc: 0.681633, test acc: 0.590612\n",
      "epoch 12, iter 20, loss: 26.583623, train acc: 0.877551\n",
      "epoch 12, iter 40, loss: 28.481294, train acc: 0.714286\n",
      "epoch 12, iter 60, loss: 30.293251, train acc: 0.693878\n",
      "epoch 12, iter 80, loss: 27.691149, train acc: 0.816327\n",
      "epoch 12, iter 100, loss: 32.113528, train acc: 0.632653\n",
      "epoch 12, iter 120, loss: 30.497042, train acc: 0.571429\n",
      "epoch 12, iter 140, loss: 29.802472, train acc: 0.673469\n",
      "epoch 12, iter 160, loss: 31.891541, train acc: 0.551020\n",
      "epoch 12, iter 180, loss: 28.530268, train acc: 0.755102\n",
      "epoch 12, iter 200, loss: 30.983935, train acc: 0.632653\n",
      "epoch 12, iter 220, loss: 31.775054, train acc: 0.653061\n",
      "epoch 12, iter 240, loss: 30.399851, train acc: 0.693878\n",
      "epoch 12, train acc: 0.679020, test acc: 0.592571\n",
      "epoch 13, iter 20, loss: 26.173565, train acc: 0.857143\n",
      "epoch 13, iter 40, loss: 28.855536, train acc: 0.775510\n",
      "epoch 13, iter 60, loss: 29.874795, train acc: 0.673469\n",
      "epoch 13, iter 80, loss: 27.760892, train acc: 0.836735\n",
      "epoch 13, iter 100, loss: 31.503701, train acc: 0.612245\n",
      "epoch 13, iter 120, loss: 30.270359, train acc: 0.530612\n",
      "epoch 13, iter 140, loss: 28.974856, train acc: 0.612245\n",
      "epoch 13, iter 160, loss: 31.423442, train acc: 0.571429\n",
      "epoch 13, iter 180, loss: 28.249978, train acc: 0.755102\n",
      "epoch 13, iter 200, loss: 30.884746, train acc: 0.632653\n",
      "epoch 13, iter 220, loss: 31.352745, train acc: 0.591837\n",
      "epoch 13, iter 240, loss: 30.342239, train acc: 0.693878\n",
      "epoch 13, train acc: 0.675918, test acc: 0.600816\n",
      "epoch 14, iter 20, loss: 25.782689, train acc: 0.877551\n",
      "epoch 14, iter 40, loss: 29.138092, train acc: 0.775510\n",
      "epoch 14, iter 60, loss: 29.192446, train acc: 0.693878\n",
      "epoch 14, iter 80, loss: 27.586756, train acc: 0.795918\n",
      "epoch 14, iter 100, loss: 30.031133, train acc: 0.632653\n",
      "epoch 14, iter 120, loss: 30.389362, train acc: 0.530612\n",
      "epoch 14, iter 140, loss: 28.162897, train acc: 0.673469\n",
      "epoch 14, iter 160, loss: 31.346145, train acc: 0.571429\n",
      "epoch 14, iter 180, loss: 27.440243, train acc: 0.755102\n",
      "epoch 14, iter 200, loss: 30.821318, train acc: 0.591837\n",
      "epoch 14, iter 220, loss: 31.147279, train acc: 0.632653\n",
      "epoch 14, iter 240, loss: 30.552228, train acc: 0.693878\n",
      "epoch 14, train acc: 0.676816, test acc: 0.597796\n",
      "epoch 15, iter 20, loss: 25.656731, train acc: 0.857143\n",
      "epoch 15, iter 40, loss: 28.995693, train acc: 0.734694\n",
      "epoch 15, iter 60, loss: 29.105877, train acc: 0.693878\n",
      "epoch 15, iter 80, loss: 27.195253, train acc: 0.836735\n",
      "epoch 15, iter 100, loss: 29.784708, train acc: 0.632653\n",
      "epoch 15, iter 120, loss: 29.999659, train acc: 0.551020\n",
      "epoch 15, iter 140, loss: 28.308432, train acc: 0.612245\n",
      "epoch 15, iter 160, loss: 30.832025, train acc: 0.551020\n",
      "epoch 15, iter 180, loss: 27.178613, train acc: 0.755102\n",
      "epoch 15, iter 200, loss: 30.914348, train acc: 0.612245\n",
      "epoch 15, iter 220, loss: 30.687168, train acc: 0.591837\n",
      "epoch 15, iter 240, loss: 29.975636, train acc: 0.693878\n",
      "epoch 15, train acc: 0.674694, test acc: 0.611673\n",
      "epoch 16, iter 20, loss: 25.131185, train acc: 0.877551\n",
      "epoch 16, iter 40, loss: 28.159942, train acc: 0.714286\n",
      "epoch 16, iter 60, loss: 29.000717, train acc: 0.693878\n",
      "epoch 16, iter 80, loss: 26.968323, train acc: 0.816327\n",
      "epoch 16, iter 100, loss: 29.521122, train acc: 0.591837\n",
      "epoch 16, iter 120, loss: 29.549669, train acc: 0.591837\n",
      "epoch 16, iter 140, loss: 27.796773, train acc: 0.653061\n",
      "epoch 16, iter 160, loss: 30.848024, train acc: 0.551020\n",
      "epoch 16, iter 180, loss: 26.500347, train acc: 0.755102\n",
      "epoch 16, iter 200, loss: 30.502352, train acc: 0.591837\n",
      "epoch 16, iter 220, loss: 30.187770, train acc: 0.571429\n",
      "epoch 16, iter 240, loss: 30.053293, train acc: 0.693878\n",
      "epoch 16, train acc: 0.670122, test acc: 0.604816\n",
      "epoch 17, iter 20, loss: 25.172794, train acc: 0.877551\n",
      "epoch 17, iter 40, loss: 27.944056, train acc: 0.693878\n",
      "epoch 17, iter 60, loss: 28.576055, train acc: 0.693878\n",
      "epoch 17, iter 80, loss: 26.666153, train acc: 0.795918\n",
      "epoch 17, iter 100, loss: 29.473601, train acc: 0.591837\n",
      "epoch 17, iter 120, loss: 29.544064, train acc: 0.591837\n",
      "epoch 17, iter 140, loss: 27.072216, train acc: 0.530612\n",
      "epoch 17, iter 160, loss: 30.229884, train acc: 0.591837\n",
      "epoch 17, iter 180, loss: 30.561059, train acc: 0.755102\n",
      "epoch 17, iter 200, loss: 36.718810, train acc: 0.714286\n",
      "epoch 17, iter 220, loss: 32.835563, train acc: 0.612245\n",
      "epoch 17, iter 240, loss: 32.361808, train acc: 0.693878\n",
      "epoch 17, train acc: 0.679673, test acc: 0.594204\n",
      "epoch 18, iter 20, loss: 27.819932, train acc: 0.816327\n",
      "epoch 18, iter 40, loss: 29.422824, train acc: 0.714286\n",
      "epoch 18, iter 60, loss: 30.166179, train acc: 0.693878\n",
      "epoch 18, iter 80, loss: 26.602989, train acc: 0.816327\n",
      "epoch 18, iter 100, loss: 30.634799, train acc: 0.632653\n",
      "epoch 18, iter 120, loss: 28.342799, train acc: 0.591837\n",
      "epoch 18, iter 140, loss: 29.105591, train acc: 0.653061\n",
      "epoch 18, iter 160, loss: 30.920426, train acc: 0.530612\n",
      "epoch 18, iter 180, loss: 27.419651, train acc: 0.734694\n",
      "epoch 18, iter 200, loss: 31.234160, train acc: 0.571429\n",
      "epoch 18, iter 220, loss: 31.298843, train acc: 0.591837\n",
      "epoch 18, iter 240, loss: 30.726832, train acc: 0.755102\n",
      "epoch 18, train acc: 0.676000, test acc: 0.608082\n",
      "epoch 19, iter 20, loss: 25.250812, train acc: 0.816327\n",
      "epoch 19, iter 40, loss: 27.594574, train acc: 0.653061\n",
      "epoch 19, iter 60, loss: 29.040744, train acc: 0.755102\n",
      "epoch 19, iter 80, loss: 27.312112, train acc: 0.816327\n",
      "epoch 19, iter 100, loss: 29.118833, train acc: 0.653061\n",
      "epoch 19, iter 120, loss: 28.072494, train acc: 0.591837\n",
      "epoch 19, iter 140, loss: 27.615482, train acc: 0.612245\n",
      "epoch 19, iter 160, loss: 29.686674, train acc: 0.530612\n",
      "epoch 19, iter 180, loss: 26.422524, train acc: 0.734694\n",
      "epoch 19, iter 200, loss: 30.892579, train acc: 0.632653\n",
      "epoch 19, iter 220, loss: 31.800659, train acc: 0.551020\n",
      "epoch 19, iter 240, loss: 30.270163, train acc: 0.775510\n",
      "epoch 19, train acc: 0.666939, test acc: 0.613306\n",
      "epoch 20, iter 20, loss: 25.210187, train acc: 0.836735\n",
      "epoch 20, iter 40, loss: 27.368687, train acc: 0.653061\n",
      "epoch 20, iter 60, loss: 28.584389, train acc: 0.673469\n",
      "epoch 20, iter 80, loss: 26.988699, train acc: 0.836735\n",
      "epoch 20, iter 100, loss: 29.431073, train acc: 0.632653\n",
      "epoch 20, iter 120, loss: 28.485542, train acc: 0.571429\n",
      "epoch 20, iter 140, loss: 27.181400, train acc: 0.571429\n",
      "epoch 20, iter 160, loss: 29.757290, train acc: 0.510204\n",
      "epoch 20, iter 180, loss: 26.516067, train acc: 0.755102\n",
      "epoch 20, iter 200, loss: 30.377679, train acc: 0.653061\n",
      "epoch 20, iter 220, loss: 30.238139, train acc: 0.612245\n",
      "epoch 20, iter 240, loss: 30.164026, train acc: 0.714286\n",
      "epoch 20, train acc: 0.667673, test acc: 0.610939\n",
      "epoch 21, iter 20, loss: 24.811790, train acc: 0.816327\n",
      "epoch 21, iter 40, loss: 27.738664, train acc: 0.755102\n",
      "epoch 21, iter 60, loss: 28.858486, train acc: 0.734694\n",
      "epoch 21, iter 80, loss: 27.346500, train acc: 0.836735\n",
      "epoch 21, iter 100, loss: 29.136708, train acc: 0.591837\n",
      "epoch 21, iter 120, loss: 27.296672, train acc: 0.571429\n",
      "epoch 21, iter 140, loss: 27.095902, train acc: 0.612245\n",
      "epoch 21, iter 160, loss: 29.640800, train acc: 0.530612\n",
      "epoch 21, iter 180, loss: 26.279378, train acc: 0.734694\n",
      "epoch 21, iter 200, loss: 29.924250, train acc: 0.653061\n",
      "epoch 21, iter 220, loss: 30.274138, train acc: 0.591837\n",
      "epoch 21, iter 240, loss: 29.915653, train acc: 0.755102\n",
      "epoch 21, train acc: 0.669714, test acc: 0.606776\n",
      "epoch 22, iter 20, loss: 24.436757, train acc: 0.775510\n",
      "epoch 22, iter 40, loss: 26.367913, train acc: 0.632653\n",
      "epoch 22, iter 60, loss: 28.104288, train acc: 0.714286\n",
      "epoch 22, iter 80, loss: 26.495696, train acc: 0.775510\n",
      "epoch 22, iter 100, loss: 29.004233, train acc: 0.551020\n",
      "epoch 22, iter 120, loss: 27.098955, train acc: 0.591837\n",
      "epoch 22, iter 140, loss: 26.273415, train acc: 0.591837\n",
      "epoch 22, iter 160, loss: 29.656666, train acc: 0.510204\n",
      "epoch 22, iter 180, loss: 25.723593, train acc: 0.755102\n",
      "epoch 22, iter 200, loss: 29.258541, train acc: 0.693878\n",
      "epoch 22, iter 220, loss: 29.713408, train acc: 0.591837\n",
      "epoch 22, iter 240, loss: 29.500080, train acc: 0.755102\n",
      "epoch 22, train acc: 0.664898, test acc: 0.610776\n",
      "epoch 23, iter 20, loss: 24.350165, train acc: 0.775510\n",
      "epoch 23, iter 40, loss: 26.922401, train acc: 0.632653\n",
      "epoch 23, iter 60, loss: 27.988372, train acc: 0.714286\n",
      "epoch 23, iter 80, loss: 26.344754, train acc: 0.816327\n",
      "epoch 23, iter 100, loss: 29.090558, train acc: 0.591837\n",
      "epoch 23, iter 120, loss: 27.531290, train acc: 0.591837\n",
      "epoch 23, iter 140, loss: 26.234171, train acc: 0.591837\n",
      "epoch 23, iter 160, loss: 29.128996, train acc: 0.510204\n",
      "epoch 23, iter 180, loss: 25.989440, train acc: 0.714286\n",
      "epoch 23, iter 200, loss: 29.777275, train acc: 0.714286\n",
      "epoch 23, iter 220, loss: 29.264514, train acc: 0.591837\n",
      "epoch 23, iter 240, loss: 29.190066, train acc: 0.734694\n",
      "epoch 23, train acc: 0.660245, test acc: 0.608816\n",
      "epoch 24, iter 20, loss: 24.334122, train acc: 0.795918\n",
      "epoch 24, iter 40, loss: 25.859125, train acc: 0.612245\n",
      "epoch 24, iter 60, loss: 27.753095, train acc: 0.734694\n",
      "epoch 24, iter 80, loss: 26.927257, train acc: 0.795918\n",
      "epoch 24, iter 100, loss: 30.354858, train acc: 0.551020\n",
      "epoch 24, iter 120, loss: 27.200610, train acc: 0.632653\n",
      "epoch 24, iter 140, loss: 25.435028, train acc: 0.571429\n",
      "epoch 24, iter 160, loss: 29.343273, train acc: 0.510204\n",
      "epoch 24, iter 180, loss: 25.364675, train acc: 0.693878\n",
      "epoch 24, iter 200, loss: 28.864730, train acc: 0.612245\n",
      "epoch 24, iter 220, loss: 28.811963, train acc: 0.551020\n",
      "epoch 24, iter 240, loss: 29.071596, train acc: 0.755102\n",
      "epoch 24, train acc: 0.660163, test acc: 0.604082\n",
      "epoch 25, iter 20, loss: 28.765559, train acc: 0.795918\n",
      "epoch 25, iter 40, loss: 33.290094, train acc: 0.734694\n",
      "epoch 25, iter 60, loss: 31.010989, train acc: 0.714286\n",
      "epoch 25, iter 80, loss: 30.677162, train acc: 0.857143\n",
      "epoch 25, iter 100, loss: 34.556841, train acc: 0.653061\n",
      "epoch 25, iter 120, loss: 32.459631, train acc: 0.673469\n",
      "epoch 25, iter 140, loss: 31.276300, train acc: 0.632653\n",
      "epoch 25, iter 160, loss: 35.475066, train acc: 0.612245\n",
      "epoch 25, iter 180, loss: 31.464133, train acc: 0.795918\n",
      "epoch 25, iter 200, loss: 33.535230, train acc: 0.693878\n",
      "epoch 25, iter 220, loss: 32.092141, train acc: 0.734694\n",
      "epoch 25, iter 240, loss: 33.615747, train acc: 0.795918\n",
      "epoch 25, train acc: 0.687265, test acc: 0.606939\n",
      "epoch 26, iter 20, loss: 27.220948, train acc: 0.877551\n",
      "epoch 26, iter 40, loss: 32.065336, train acc: 0.653061\n",
      "epoch 26, iter 60, loss: 30.747811, train acc: 0.714286\n",
      "epoch 26, iter 80, loss: 29.601986, train acc: 0.836735\n",
      "epoch 26, iter 100, loss: 32.742064, train acc: 0.673469\n",
      "epoch 26, iter 120, loss: 30.860310, train acc: 0.612245\n",
      "epoch 26, iter 140, loss: 31.441021, train acc: 0.632653\n",
      "epoch 26, iter 160, loss: 33.432169, train acc: 0.530612\n",
      "epoch 26, iter 180, loss: 30.705784, train acc: 0.755102\n",
      "epoch 26, iter 200, loss: 33.378347, train acc: 0.673469\n",
      "epoch 26, iter 220, loss: 31.302156, train acc: 0.653061\n",
      "epoch 26, iter 240, loss: 32.370408, train acc: 0.775510\n",
      "epoch 26, train acc: 0.683347, test acc: 0.590286\n",
      "epoch 27, iter 20, loss: 26.845487, train acc: 0.795918\n",
      "epoch 27, iter 40, loss: 30.839001, train acc: 0.714286\n",
      "epoch 27, iter 60, loss: 30.375097, train acc: 0.714286\n",
      "epoch 27, iter 80, loss: 29.053970, train acc: 0.816327\n",
      "epoch 27, iter 100, loss: 33.183122, train acc: 0.653061\n",
      "epoch 27, iter 120, loss: 30.597765, train acc: 0.591837\n",
      "epoch 27, iter 140, loss: 30.649915, train acc: 0.653061\n",
      "epoch 27, iter 160, loss: 32.635865, train acc: 0.591837\n",
      "epoch 27, iter 180, loss: 30.223131, train acc: 0.734694\n",
      "epoch 27, iter 200, loss: 32.397134, train acc: 0.673469\n",
      "epoch 27, iter 220, loss: 29.835164, train acc: 0.653061\n",
      "epoch 27, iter 240, loss: 32.083267, train acc: 0.816327\n",
      "epoch 27, train acc: 0.683102, test acc: 0.614122\n",
      "epoch 28, iter 20, loss: 26.516477, train acc: 0.816327\n",
      "epoch 28, iter 40, loss: 30.273036, train acc: 0.775510\n",
      "epoch 28, iter 60, loss: 30.135071, train acc: 0.653061\n",
      "epoch 28, iter 80, loss: 28.706605, train acc: 0.897959\n",
      "epoch 28, iter 100, loss: 32.142653, train acc: 0.673469\n",
      "epoch 28, iter 120, loss: 29.646904, train acc: 0.714286\n",
      "epoch 28, iter 140, loss: 30.372269, train acc: 0.673469\n",
      "epoch 28, iter 160, loss: 32.597358, train acc: 0.551020\n",
      "epoch 28, iter 180, loss: 29.944798, train acc: 0.714286\n",
      "epoch 28, iter 200, loss: 32.000032, train acc: 0.632653\n",
      "epoch 28, iter 220, loss: 29.872546, train acc: 0.673469\n",
      "epoch 28, iter 240, loss: 31.276751, train acc: 0.775510\n",
      "epoch 28, train acc: 0.686041, test acc: 0.611673\n",
      "epoch 29, iter 20, loss: 26.224358, train acc: 0.816327\n",
      "epoch 29, iter 40, loss: 29.580310, train acc: 0.795918\n",
      "epoch 29, iter 60, loss: 30.338568, train acc: 0.673469\n",
      "epoch 29, iter 80, loss: 28.470596, train acc: 0.836735\n",
      "epoch 29, iter 100, loss: 31.548880, train acc: 0.653061\n",
      "epoch 29, iter 120, loss: 29.320140, train acc: 0.673469\n",
      "epoch 29, iter 140, loss: 30.415420, train acc: 0.653061\n",
      "epoch 29, iter 160, loss: 31.642030, train acc: 0.510204\n",
      "epoch 29, iter 180, loss: 29.599452, train acc: 0.734694\n",
      "epoch 29, iter 200, loss: 31.173338, train acc: 0.612245\n",
      "epoch 29, iter 220, loss: 29.484336, train acc: 0.653061\n",
      "epoch 29, iter 240, loss: 31.064133, train acc: 0.775510\n",
      "epoch 29, train acc: 0.683347, test acc: 0.620000\n",
      "epoch 30, iter 20, loss: 25.846436, train acc: 0.836735\n",
      "epoch 30, iter 40, loss: 29.502563, train acc: 0.816327\n",
      "epoch 30, iter 60, loss: 30.033950, train acc: 0.673469\n",
      "epoch 30, iter 80, loss: 28.209496, train acc: 0.836735\n",
      "epoch 30, iter 100, loss: 31.327711, train acc: 0.673469\n",
      "epoch 30, iter 120, loss: 28.884063, train acc: 0.714286\n",
      "epoch 30, iter 140, loss: 30.360042, train acc: 0.612245\n",
      "epoch 30, iter 160, loss: 31.108767, train acc: 0.530612\n",
      "epoch 30, iter 180, loss: 29.260866, train acc: 0.734694\n",
      "epoch 30, iter 200, loss: 31.099681, train acc: 0.591837\n",
      "epoch 30, iter 220, loss: 29.489297, train acc: 0.653061\n",
      "epoch 30, iter 240, loss: 30.929515, train acc: 0.795918\n",
      "epoch 30, train acc: 0.681306, test acc: 0.623429\n",
      "epoch 31, iter 20, loss: 25.663086, train acc: 0.836735\n",
      "epoch 31, iter 40, loss: 29.036120, train acc: 0.795918\n",
      "epoch 31, iter 60, loss: 30.080252, train acc: 0.693878\n",
      "epoch 31, iter 80, loss: 27.833359, train acc: 0.816327\n",
      "epoch 31, iter 100, loss: 31.110115, train acc: 0.714286\n",
      "epoch 31, iter 120, loss: 29.083586, train acc: 0.734694\n",
      "epoch 31, iter 140, loss: 29.970979, train acc: 0.591837\n",
      "epoch 31, iter 160, loss: 30.889588, train acc: 0.571429\n",
      "epoch 31, iter 180, loss: 29.223933, train acc: 0.734694\n",
      "epoch 31, iter 200, loss: 30.646985, train acc: 0.693878\n",
      "epoch 31, iter 220, loss: 29.150707, train acc: 0.673469\n",
      "epoch 31, iter 240, loss: 30.320064, train acc: 0.816327\n",
      "epoch 31, train acc: 0.680571, test acc: 0.616245\n",
      "epoch 32, iter 20, loss: 25.265779, train acc: 0.816327\n",
      "epoch 32, iter 40, loss: 28.376353, train acc: 0.755102\n",
      "epoch 32, iter 60, loss: 29.864303, train acc: 0.693878\n",
      "epoch 32, iter 80, loss: 27.840091, train acc: 0.836735\n",
      "epoch 32, iter 100, loss: 30.527482, train acc: 0.714286\n",
      "epoch 32, iter 120, loss: 28.984769, train acc: 0.714286\n",
      "epoch 32, iter 140, loss: 29.587093, train acc: 0.612245\n",
      "epoch 32, iter 160, loss: 31.028004, train acc: 0.551020\n",
      "epoch 32, iter 180, loss: 29.037013, train acc: 0.734694\n",
      "epoch 32, iter 200, loss: 30.502487, train acc: 0.632653\n",
      "epoch 32, iter 220, loss: 28.965642, train acc: 0.693878\n",
      "epoch 32, iter 240, loss: 29.684855, train acc: 0.795918\n",
      "epoch 32, train acc: 0.679347, test acc: 0.622694\n",
      "epoch 33, iter 20, loss: 25.174758, train acc: 0.836735\n",
      "epoch 33, iter 40, loss: 28.357602, train acc: 0.816327\n",
      "epoch 33, iter 60, loss: 29.596444, train acc: 0.714286\n",
      "epoch 33, iter 80, loss: 28.035107, train acc: 0.816327\n",
      "epoch 33, iter 100, loss: 30.013480, train acc: 0.673469\n",
      "epoch 33, iter 120, loss: 28.810349, train acc: 0.714286\n",
      "epoch 33, iter 140, loss: 29.524121, train acc: 0.632653\n",
      "epoch 33, iter 160, loss: 31.128686, train acc: 0.571429\n",
      "epoch 33, iter 180, loss: 28.920392, train acc: 0.755102\n",
      "epoch 33, iter 200, loss: 30.592528, train acc: 0.653061\n",
      "epoch 33, iter 220, loss: 29.119248, train acc: 0.673469\n",
      "epoch 33, iter 240, loss: 29.511488, train acc: 0.795918\n",
      "epoch 33, train acc: 0.677796, test acc: 0.624163\n",
      "epoch 34, iter 20, loss: 25.115159, train acc: 0.836735\n",
      "epoch 34, iter 40, loss: 28.219950, train acc: 0.816327\n",
      "epoch 34, iter 60, loss: 29.663429, train acc: 0.673469\n",
      "epoch 34, iter 80, loss: 28.087078, train acc: 0.857143\n",
      "epoch 34, iter 100, loss: 29.803238, train acc: 0.673469\n",
      "epoch 34, iter 120, loss: 28.628950, train acc: 0.673469\n",
      "epoch 34, iter 140, loss: 29.165794, train acc: 0.612245\n",
      "epoch 34, iter 160, loss: 31.373094, train acc: 0.632653\n",
      "epoch 34, iter 180, loss: 28.660083, train acc: 0.734694\n",
      "epoch 34, iter 200, loss: 30.175701, train acc: 0.632653\n",
      "epoch 34, iter 220, loss: 28.441037, train acc: 0.673469\n",
      "epoch 34, iter 240, loss: 29.367116, train acc: 0.775510\n",
      "epoch 34, train acc: 0.676571, test acc: 0.617633\n",
      "epoch 35, iter 20, loss: 25.015319, train acc: 0.836735\n",
      "epoch 35, iter 40, loss: 28.045136, train acc: 0.795918\n",
      "epoch 35, iter 60, loss: 29.143936, train acc: 0.693878\n",
      "epoch 35, iter 80, loss: 28.080539, train acc: 0.877551\n",
      "epoch 35, iter 100, loss: 29.713022, train acc: 0.673469\n",
      "epoch 35, iter 120, loss: 28.437260, train acc: 0.612245\n",
      "epoch 35, iter 140, loss: 28.757775, train acc: 0.632653\n",
      "epoch 35, iter 160, loss: 31.030251, train acc: 0.612245\n",
      "epoch 35, iter 180, loss: 28.345030, train acc: 0.734694\n",
      "epoch 35, iter 200, loss: 29.917558, train acc: 0.591837\n",
      "epoch 35, iter 220, loss: 28.332851, train acc: 0.632653\n",
      "epoch 35, iter 240, loss: 28.914456, train acc: 0.775510\n",
      "epoch 35, train acc: 0.675102, test acc: 0.620408\n",
      "epoch 36, iter 20, loss: 24.735999, train acc: 0.836735\n",
      "epoch 36, iter 40, loss: 27.988392, train acc: 0.775510\n",
      "epoch 36, iter 60, loss: 28.635217, train acc: 0.693878\n",
      "epoch 36, iter 80, loss: 27.989081, train acc: 0.897959\n",
      "epoch 36, iter 100, loss: 29.390478, train acc: 0.653061\n",
      "epoch 36, iter 120, loss: 28.113615, train acc: 0.673469\n",
      "epoch 36, iter 140, loss: 28.661084, train acc: 0.612245\n",
      "epoch 36, iter 160, loss: 30.498169, train acc: 0.632653\n",
      "epoch 36, iter 180, loss: 28.067194, train acc: 0.755102\n",
      "epoch 36, iter 200, loss: 30.176479, train acc: 0.612245\n",
      "epoch 36, iter 220, loss: 28.481210, train acc: 0.673469\n",
      "epoch 36, iter 240, loss: 28.760510, train acc: 0.775510\n",
      "epoch 36, train acc: 0.677959, test acc: 0.616082\n",
      "epoch 37, iter 20, loss: 26.272541, train acc: 0.816327\n",
      "epoch 37, iter 40, loss: 31.326720, train acc: 0.693878\n",
      "epoch 37, iter 60, loss: 30.135258, train acc: 0.612245\n",
      "epoch 37, iter 80, loss: 29.324738, train acc: 0.795918\n",
      "epoch 37, iter 100, loss: 32.863967, train acc: 0.591837\n",
      "epoch 37, iter 120, loss: 28.737226, train acc: 0.714286\n",
      "epoch 37, iter 140, loss: 31.629268, train acc: 0.530612\n",
      "epoch 37, iter 160, loss: 33.616676, train acc: 0.673469\n",
      "epoch 37, iter 180, loss: 29.778819, train acc: 0.734694\n",
      "epoch 37, iter 200, loss: 33.196346, train acc: 0.632653\n",
      "epoch 37, iter 220, loss: 32.047290, train acc: 0.653061\n",
      "epoch 37, iter 240, loss: 32.133274, train acc: 0.673469\n",
      "epoch 37, train acc: 0.703837, test acc: 0.621959\n",
      "epoch 38, iter 20, loss: 25.510172, train acc: 0.836735\n",
      "epoch 38, iter 40, loss: 30.082825, train acc: 0.734694\n",
      "epoch 38, iter 60, loss: 29.435100, train acc: 0.591837\n",
      "epoch 38, iter 80, loss: 28.258517, train acc: 0.836735\n",
      "epoch 38, iter 100, loss: 29.643819, train acc: 0.653061\n",
      "epoch 38, iter 120, loss: 28.470325, train acc: 0.591837\n",
      "epoch 38, iter 140, loss: 30.069586, train acc: 0.530612\n",
      "epoch 38, iter 160, loss: 31.882268, train acc: 0.653061\n",
      "epoch 38, iter 180, loss: 28.493624, train acc: 0.775510\n",
      "epoch 38, iter 200, loss: 31.876309, train acc: 0.693878\n",
      "epoch 38, iter 220, loss: 30.721732, train acc: 0.612245\n",
      "epoch 38, iter 240, loss: 31.161206, train acc: 0.714286\n",
      "epoch 38, train acc: 0.689878, test acc: 0.609959\n",
      "epoch 39, iter 20, loss: 25.916884, train acc: 0.795918\n",
      "epoch 39, iter 40, loss: 29.421925, train acc: 0.714286\n",
      "epoch 39, iter 60, loss: 29.663231, train acc: 0.693878\n",
      "epoch 39, iter 80, loss: 27.651897, train acc: 0.836735\n",
      "epoch 39, iter 100, loss: 30.694439, train acc: 0.653061\n",
      "epoch 39, iter 120, loss: 28.029155, train acc: 0.591837\n",
      "epoch 39, iter 140, loss: 29.505013, train acc: 0.530612\n",
      "epoch 39, iter 160, loss: 30.709293, train acc: 0.653061\n",
      "epoch 39, iter 180, loss: 28.849975, train acc: 0.775510\n",
      "epoch 39, iter 200, loss: 29.996318, train acc: 0.530612\n",
      "epoch 39, iter 220, loss: 29.758060, train acc: 0.673469\n",
      "epoch 39, iter 240, loss: 30.935915, train acc: 0.775510\n",
      "epoch 39, train acc: 0.685061, test acc: 0.612490\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/num_problems)*num_timesteps # loss at iteration 0\n",
    "\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "print_batch_sz = 20\n",
    "for e in xrange(epochs):\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    total_acc_train = 0.0\n",
    "    total_acc_test = 0.0\n",
    "    for i in xrange(num_train):\n",
    "        inputs_train, targets_train, correctness_train = extract_x_y_corr_for_sample(X_train, y_train, corr_train)\n",
    "        # forward num_timesteps characters through the net and fetch gradient\n",
    "        loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs_train, targets_train, correctness_train, hprev)\n",
    "        smooth_loss = smooth_loss * 0.7 + loss * 0.3\n",
    "        losses.append(smooth_loss)\n",
    "\n",
    "        # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                      [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                      [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "        ps_train = forward_pass(inputs_train)\n",
    "        acc_train = accuracy(ps_train, targets_train, correctness_train)\n",
    "        train_accuracies.append(acc_train)\n",
    "        \n",
    "        if i%print_batch_sz == 0 and i != 0:\n",
    "            print ('epoch %d, iter %d, loss: %f, train acc: %f' % (e, i, smooth_loss, acc_train)) \n",
    "            \n",
    "        total_acc_train += acc_train\n",
    "    total_acc_train /= num_train\n",
    "        \n",
    "    for i in xrange(num_test):\n",
    "        inputs_test, targets_test, correctness_test = extract_x_y_corr_for_sample(X_test, y_test, corr_test)\n",
    "        ps_test = forward_pass(inputs_test)\n",
    "        acc_test = accuracy(ps_test, targets_test, correctness_test) \n",
    "        test_accuracies.append(acc_test)\n",
    "        total_acc_test += acc_test\n",
    "    \n",
    "    total_acc_test /= num_test\n",
    "    print ('epoch %d, train acc: %f, test acc: %f' % (e, total_acc_train, total_acc_test)) # print progress\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-87bb1e8e7e8c>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-87bb1e8e7e8c>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    ax1.set_xlabel('iterations')\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def smoothen_data(data, smooth_window=100):\n",
    "    smooth = []\n",
    "    for i in xrange(len(data)-smooth_window):\n",
    "        smooth.append(np.mean(data[i:i+smooth_window]))\n",
    "\n",
    "    for i in xrange(len(data)-smooth_window, len(data)):\n",
    "        smooth.append(np.mean(data[i:len(data)]))\n",
    "    return smooth\n",
    "   \n",
    "smooth_accs_train = smoothen_data(train_accuracies)\n",
    "smooth_accs_test = smoothen_data(test_accuracies)\n",
    "                    \n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "train_accs_line, = ax1.plot(xrange(len(smooth_accs_train)), smooth_accs_train, 'b-', label='train accuracies')\n",
    "test_accs_line, = ax1.plot(xrange(len(smooth_accs_test), smooth_accs_test, 'g-', label='test accuracies')\n",
    "# ax1.set_ylabel('accuracies', color='b')\n",
    "# ax1.set_xlabel('iterations')\n",
    "for tl in ax1.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "losses_line, = ax2.plot(xrange(len(losses)), losses, 'r-', label='losses')\n",
    "ax2.set_ylabel('losses', color='r')\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('r')\n",
    "\n",
    "plt.legend(handles=[losses_line, train_accs_line, test_accs_line],bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=2, mode=\"expand\")\n",
    "# plt.legend([losses_line, train_accs_line, test_accs_line], ['losses', 'train accuracies', 'test accuracies'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
